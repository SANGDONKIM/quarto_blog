{"title":"complete seperation in logistic regression","markdown":{"yaml":{"title":"complete seperation in logistic regression","description":"glm.fit: fitted probabilities numerically 0 or 1 occurred\n","author":"Don Don","date":"2022-06-22","categories":["R","glm","caret"],"image":"box_im.png"},"headingText":"complete seperation","containsRefs":false,"markdown":"\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE,\n                      message = FALSE, \n                      warning = FALSE,\n                      fig.align = \"center\")\n```\n\n```{r}\nlibrary(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())\n\n```\n\n\n\n-   설명변수를 이용해서 반응변수를 완벽하게 예측할 수 있는 경우를 의미함 \n\n-   설명변수를 이용해서 반응변수를 거의 완벽하게 예측할 수 있는 경우 quasi-complete seperation 이라고 함 \n\n## complete seperation 예제\n\n| Y   | X1  | X2  |\n|-----|-----|-----|\n| 0   | 1   | 3   |\n| 0   | 2   | 2   |\n| 0   | 3   | -1  |\n| 0   | 3   | -1  |\n| 1   | 5   | 2   |\n| 1   | 6   | 4   |\n| 1   | 10  | 1   |\n| 1   | 11  | 0   |\n\n-   $X_1\\le 3$일 경우 $Y=0$이고, $X_1>3$일 경우 $Y=1$로 완벽하게 분리됨\n\n\n\n## 왜 complete seperation이 문제인가? \n\n-   Logistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음 \n\n```{r}\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n```\n\n```{r}\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n```\n\n-   `glm.fit: fitted probabilities numerically 0 or 1 occurred` warning message 출력 \n\n    -   예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함 \n    \n    -   즉, 너무 완벽하게 예측했을 경우 발생함 \n\n-   `glm.fit: algorithm did not converge` warning message 출력 \n\n    -   MLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함  \n\n    \n\n\n\n# Complete seperation이 발생할 경우 해결책 \n\n**1. Complete seperation을 발생하게 하는 설명변수 제거**\n\ncomplete seperation이 일어나는 경우는 $Y$와 너무 밀접하게 연관된 $X$ 변수를 수집했을 때 발생합니다. 혹은, 데이터가 매우 작은 경우 데이터 수집과정에서 우연히 발생할 수도 있을 것입니다. 먼저, complete seperation이 일어나게 하는 설명변수 $X$가 타당한 설명변수인지 생각해봐야 합니다. 아래 예제 데이터를 보겠습니다. \n\n**Data description**\n\n-   temperature : 온도(섭씨)\n\n-   Humidity : 습도(%)\n\n-   Light : 밝기(lux)\n\n-   Co2 : 이산화탄소 농도(ppm)\n\n-   Occupancy : 방 이용 유무(1 : 이용 x, 0 : 이용 o)\n\n<https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+>\n\n온도, 밝기, 습도, 이산화탄소 농도 등의 변수를 활용해서 방 이용 유무를 예측하는 이진 분류 문제입니다. \n\n```{r}\n\ndat <- read.csv('/Users/sangdon/myblog/posts/com_sep/dat9.csv')\n```\n\n먼저, train/test를 나누고 EDA를 진행해보겠습니다. \n\n```{r}\ndat %>% summary()\n```\n\n\n```{r}\ndat <- dat %>% \n    mutate(occupancy = factor(occupancy, labels = c(\"No\", \"Yes\")))\n\nlibrary(rsample)\n\nset.seed(1231)\nsplits <- initial_split(dat, prop =  0.7, strata=occupancy)\n\ntrain<-training(splits)\ntest<-testing(splits)\n```\n\n\n```{r}\nrec <- train %>%\n  recipe(occupancy~.) %>%\n  step_bagimpute(co2, impute_with=imp_vars(all_predictors()))\n\ntrain<- rec %>% prep() %>% juice()\ntest <- rec %>% prep() %>% bake(new_data=test)\ntrain %>% is.na() %>% colSums()\n```\n\n다른 전처리는 하지 않고, 결측치 대치만 진행한 후, logistic regression 모형을 적합해보겠습니다. \n\n\n```{r}\nset.seed(1)\ncontrol <- trainControl(method='cv', \n                        number=3, \n                        classProbs = T,\n                        summaryFunction = twoClassSummary, \n                        savePredictions = \"all\"\n                        )\n\n\ntic()\nglm_gridsearch <- train(occupancy ~ .,             \n                       data = train,               \n                       method = 'glm',\n                       family = \"binomial\",\n                       trControl = control,\n                       metric = \"ROC\") \ntoc()\n\n```\n\n\n다음과 같은 warning message가 출력됩니다. \n\n-   glm.fit: algorithm did not converge \n\n-   glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n위에서 설명했던 것처럼 complete seperation이 있을 때에 해당합니다. 따라서 데이터를 보고 어떤 설명변수 $X$가 이러한 문제를 발생시켰는지 확인이 필요합니다. \n\n\n```{r}\ntrain %>% \n    ggplot(aes(x = occupancy, y = light, fill = occupancy)) + \n    geom_boxplot()\n```\n\n\n```{r}\ntrain %>% \n    group_by(occupancy) %>% \n    summarise(min = min(light), \n              mean = mean(light), \n              max = max(light))\n\n```\n\noccupancy ~ light의 boxplot과 통계량을 보면 occupancy가 $No$일 때, light는 대부분 $0$의 값을 갖고, 최대값은 $153$입니다. 반면, occupancy가 $Yes$일 때, light는 $393 \\sim 1380$까지의 값을 갖는 것을 알 수 있습니다. \n\n변수의 의미를 생각해보면 방을 이용하지 않을 경우 불은 꺼져있을 것이고, 방을 이용할 경우 불이 켜져있는 것은 당연하다고 볼 수 있습니다. 따라서 이상치가 없다는 가정 하에 light 변수가 설명변수에 있을 경우 occupancy를 정확히 예측할 수 있습니다. \n\n이러한 예제의 경우 light 변수를 제거하지 않고, 모형에 포함시키는 것이 합리적입니다. 다만, 데이터가 작을 경우, 연관성이 없는 변수에 대해 이러한 현상이 발생할 수 있습니다. 혹은 반응변수의 의미와 완벽하게 중복되는 설명변수가 포함되는 경우도 있을 수 있습니다. 이러한 경우 변수를 제거하는 것이 합리적일 것입니다. \n\n이렇듯, 분석가의 판단 하에 변수를 제거할지 여부를 판단하는 것이 필요합니다. \n\n\n**2. Logistic regression 모형 말고 다른 모형 적합** \n\nLogistic regression 모형이 아닌 다른 ML 모형의 경우 (ex. tree based model) complete seperation 문제 때문에 계산이 안되는 문제는 없기 때문에 다른 모형을 적합하면 됩니다. \n\n\n**3. Penalized logistic regression** \n\nLogistic regression 모형에 penalty term을 추가할 경우 logistic regression의 알고리즘이 수렴하지 문제를 해결할 수 있습니다. \n\n\n```{r}\n#devtools::install_version(\"LiblineaR\", version = \"2.10-8\")\nset.seed(1231)\ncontrol<-trainControl(method=\"cv\",\n                      number=5,\n                      classProbs=T,\n                      summaryFunction=twoClassSummary,\n                      savePredictions = T)\n\nlogistic_gridsearch<-train(occupancy~.,\n                           data=train,\n                           method=\"regLogistic\",\n                           trControl=control,\n                           tuneLength=3,\n                           metric=\"ROC\")\n\nlogistic_gridsearch\n```\n\n해당 데이터의 경우 ADP 23회 기출문제입니다. 시험 답안을 작성하실 때, 어떻게 작성하면 좋을지 한번 생각해보세요! \n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"com_sep.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.426","theme":"../../ember.scss","title-block-banner":true,"toc-title":"Table of contents","toc-location":"left","citation":true,"title":"complete seperation in logistic regression","description":"glm.fit: fitted probabilities numerically 0 or 1 occurred\n","author":"Don Don","date":"2022-06-22","categories":["R","glm","caret"],"image":"box_im.png"},"extensions":{"book":{"multiFile":true}}}}}