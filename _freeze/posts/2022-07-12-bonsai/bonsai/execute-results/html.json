{
  "hash": "09962cf29e5738d646141b7dde0e1247",
  "result": {
    "markdown": "---\ntitle: \"bonsai and treesnip\"\ndescription: |\n  R lightgbm, xgboost, catboost 관련 패키지\nauthor: \"Don Don\"\ndate: \"2022-07-13\"\ncategories: [R, ligthtgbm, xgboost, catboost]\nimage: \"https://bonsai.tidymodels.org/logo.png\"\n---\n\n\n## Library\n\ntidymodels에서 xgboost의 경우 공식 지원한다. 다만 많이 사용되는 lightgbm, catboost의 경우 공식 지원하지 않는다. 따라서 CRAN에 등록되어있지 않은 treesnip 패키지를 설치해서 사용했어야 했다. 최근에 treesnip 패키지 개발을 멈추고 bonsai 패키지를 새롭게 만들어서 개발 중인 것 같다. bonsai 패키지 사용법에 대해 한번 정리를 해본다. bonsai 패키지의 경우 lightgbm은 지원하지만 catboost는 지원하지 않기 때문에, treesnip 패키지를 활용해서 catboost 모형을 적합하는 방법에 대해 정리해본다. bonsai, treesnip 패키지의 경우 tidymodels 워크플로우에 해당 모델이 추가된 형태이므로 tidymodels의 워크플로우를 그대로 따라간다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"pak\")\n#install.packages(\"lightgbm\")\n#pak::pak(\"tidymodels/bonsai\")\n#remotes::install_github(\"curso-r/treesnip@catboost\")\n#remotes::install_github(\"curso-r/treesnip\")\n\n\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 0.2.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.0     ✔ recipes      1.0.1\n✔ dials        1.0.0     ✔ rsample      1.0.0\n✔ dplyr        1.0.9     ✔ tibble       3.1.7\n✔ ggplot2      3.3.6     ✔ tidyr        1.2.0\n✔ infer        1.0.2     ✔ tune         0.2.0\n✔ modeldata    1.0.0     ✔ workflows    1.0.0\n✔ parsnip      1.0.0     ✔ workflowsets 0.2.1\n✔ purrr        0.3.4     ✔ yardstick    1.0.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(bonsai)\nlibrary(skimr)\nlibrary(treesnip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'treesnip':\n  method                     from  \n  multi_predict._lgb.Booster bonsai\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'treesnip'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:bonsai':\n\n    predict_lightgbm_classification_class,\n    predict_lightgbm_classification_prob,\n    predict_lightgbm_classification_raw,\n    predict_lightgbm_regression_numeric, train_lightgbm\n```\n:::\n\n```{.r .cell-code}\nlibrary(catboost)\n```\n:::\n\n\n## Data description\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- MASS::Boston\ndat %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 506\nColumns: 14\n$ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…\n$ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…\n$ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…\n$ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…\n$ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…\n$ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…\n$ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…\n$ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…\n$ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…\n$ black   <dbl> 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…\n$ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…\n$ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(dat)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |dat  |\n|Number of rows           |506  |\n|Number of columns        |14   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |14   |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|     p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|------:|------:|------:|------:|------:|:-----|\n|crim          |         0|             1|   3.61|   8.60|   0.01|   0.08|   0.26|   3.68|  88.98|▇▁▁▁▁ |\n|zn            |         0|             1|  11.36|  23.32|   0.00|   0.00|   0.00|  12.50| 100.00|▇▁▁▁▁ |\n|indus         |         0|             1|  11.14|   6.86|   0.46|   5.19|   9.69|  18.10|  27.74|▇▆▁▇▁ |\n|chas          |         0|             1|   0.07|   0.25|   0.00|   0.00|   0.00|   0.00|   1.00|▇▁▁▁▁ |\n|nox           |         0|             1|   0.55|   0.12|   0.38|   0.45|   0.54|   0.62|   0.87|▇▇▆▅▁ |\n|rm            |         0|             1|   6.28|   0.70|   3.56|   5.89|   6.21|   6.62|   8.78|▁▂▇▂▁ |\n|age           |         0|             1|  68.57|  28.15|   2.90|  45.02|  77.50|  94.07| 100.00|▂▂▂▃▇ |\n|dis           |         0|             1|   3.80|   2.11|   1.13|   2.10|   3.21|   5.19|  12.13|▇▅▂▁▁ |\n|rad           |         0|             1|   9.55|   8.71|   1.00|   4.00|   5.00|  24.00|  24.00|▇▂▁▁▃ |\n|tax           |         0|             1| 408.24| 168.54| 187.00| 279.00| 330.00| 666.00| 711.00|▇▇▃▁▇ |\n|ptratio       |         0|             1|  18.46|   2.16|  12.60|  17.40|  19.05|  20.20|  22.00|▁▃▅▅▇ |\n|black         |         0|             1| 356.67|  91.29|   0.32| 375.38| 391.44| 396.22| 396.90|▁▁▁▁▇ |\n|lstat         |         0|             1|  12.65|   7.14|   1.73|   6.95|  11.36|  16.96|  37.97|▇▇▅▂▁ |\n|medv          |         0|             1|  22.53|   9.20|   5.00|  17.02|  21.20|  25.00|  50.00|▂▇▅▁▁ |\n:::\n:::\n\n\n## train/test split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplits <- initial_split(dat, prop = 0.7)\n```\n:::\n\n\n## data preprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(medv ~ ., training(splits))\n```\n:::\n\n\n## Catboost\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_mod <- boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune()) %>%\n  set_engine(engine = \"catboost\" \n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(cat_mod)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(),  size = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- tune_grid(\n    cat_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  min_n tree_depth learn_rate .metric .estimator  mean     n std_err .config    \n  <int>      <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      \n1    27          8 0.0703     rmse    standard    3.25     5   0.231 Preprocess…\n2    31         11 0.00798    rmse    standard    4.21     5   0.403 Preprocess…\n3    22          5 0.00125    rmse    standard    7.19     5   0.512 Preprocess…\n4     7          9 0.0000988  rmse    standard    8.85     5   0.483 Preprocess…\n5    35         12 0.00000379 rmse    standard    9.03     5   0.479 Preprocess…\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(cat_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 27\n  tree_depth = 8\n  learn_rate = 0.0702985501331971\n\nComputational engine: catboost \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_cat <- \n    predict(cat_fit, testing(splits)) %>% \n    mutate(modelo = \"cat\")\n\n\nresult3 <- data.frame(pred = pred_cat$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result3, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.06 \n2 rsq     standard       0.816\n3 mae     standard       2.36 \n```\n:::\n:::\n\n\n## lightgbm\n\ntidymodels에서 lightgbm을 사용할 경우 `set_engine()`에 ligthtgbm을 명시해주면 된다. cuda gpu를 사용할 경우 `params=list(tree_method = 'gpu_hist')`를 명시해줘야 한다. 또한 CRAN에 등록되어있는 lightgbm 패키지가 아닌 gpu 옵션을 지원하는 lightgbm을 따로 설치해줘야한다(참고 : <https://cran.r-project.org/web/packages/lightgbm/readme/README.html#installing-a-gpu-enabled-build>). 이 경우 Docker file을 다운받아서 설치하는 것이 오류가 날 가능성이 적다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_mod <- \n  boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune(),\n        loss_reduction = tune(),\n        stop_iter = tune()) %>%\n  set_engine(engine = \"lightgbm\" \n             #params=list(tree_method = 'gpu_hist')\n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n\nR lightgbm 공식 문서를 보면 추가적인 parameter가 많은데, 추가하고 싶은 parameter는 set_engine에 해당 parameter를 추가하면 된다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlight_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(bt_mod)\n```\n:::\n\n\n워크플로우에 전처리 recipe와 정의한 모델을 업데이트한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n\nvalidation set을 지정한다. fold=5인 cross validation 방식으로 설정한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)\n```\n:::\n\n\ntuning 방식을 정의하는데 latine hypercube 방식을 이용했다. tuning 방식은 tidymodels 공식문서를 참고하면 다양한 방법을 제공한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doFuture)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: future\n```\n:::\n\n```{.r .cell-code}\nregisterDoFuture()\ncl <- parallel::makeCluster(6)\nplan(cluster, workers = cl)\n```\n:::\n\n\n병렬 처리 옵션의 경우 doFuture 패키지를 이용했다. 해당 컴퓨터의 코어 수를 확인하고 적절하게 세팅해주면 된다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- tune_grid(\n    light_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE, allow_par = T))\n```\n:::\n\n\n`tune_grid()`를 이용해서, workflow, validation set, metric, 예측값 저장 옵션 등을 설정하고, 파라미터 튜닝을 진행한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 11\n  min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean\n  <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl>\n1    34          5  0.0158     0.000183              5 rmse    standard    3.55\n2    27          7  0.00210    0.00000126           16 rmse    standard    4.16\n3    18          3  0.000239   0.00000000667        10 rmse    standard    7.69\n4    17         14  0.0000300  0.0000274             7 rmse    standard    8.86\n5     6         13  0.0000199  1.05                  4 rmse    standard    8.91\n# … with 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n\n`show_best()`를 통해 tuning 후 최적의 파라미터를 확인할 수 있다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(light_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 34\n  tree_depth = 5\n  learn_rate = 0.0158299029849725\n  loss_reduction = 0.000183367712483135\n  stop_iter = 5\n\nComputational engine: lightgbm \n```\n:::\n:::\n\n\n최적의 파라미터를 선택한 후 `finalize_workflow()`를 선택된 최적의 파라미터를 workflow에 업데이트 시켜준다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlight_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n\n최적의 파라미터를 업데이트한 모델을 훈련 데이터를 이용해서 학습을 진행해준다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_light <- \n    predict(light_fit, testing(splits)) %>% \n    mutate(modelo = \"lightgbm\")\n\npred_light %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  .pred modelo  \n  <dbl> <chr>   \n1  30.8 lightgbm\n2  31.8 lightgbm\n3  19.5 lightgbm\n4  20.2 lightgbm\n5  18.2 lightgbm\n6  18.6 lightgbm\n```\n:::\n\n```{.r .cell-code}\nresult1 <- data.frame(pred = pred_light$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n\n마지막으로 test 데이터셋에 학습을 해서 예측값을 산출한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result1, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.51 \n2 rsq     standard       0.774\n3 mae     standard       2.82 \n```\n:::\n:::\n\n\nyardstick 패키지의 `metrics()`의 경우 회귀, 분류 세팅에 맞춰서 적절한 평가지표를 보여준다. `metrics()`의 경우 예측값과 실제값이 포함된 데이터셋을 넣어줘야 작동한다.\n\n## XGBOOST\n\nxgboost의 경우 tidymodels에서 공식 지원하기 때문에, tidymodels 워크플로우에 맞춰서 그대로 사용하면 된다. 대신 gpu를 사용할 경우 lightgbm과 마찬가지로 gpu가 지원되는 xgboost 버전을 따로 설치해줘야 한다(참고 : <https://github.com/dmlc/xgboost/issues/6654#issuecomment-813828671>). lightgbm과 동일하게 gpu가 지원되는 xgboost를 설치했을 경우 `params=list(tree_method = 'gpu_hist')` 옵션을 넣어주면 된다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_mod <- boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune(),\n        loss_reduction = tune(),\n        stop_iter = tune()) %>%\n  set_engine(engine = \"xgboost\" \n             #params=list(tree_method = 'gpu_hist')\n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(xgb_mod)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doFuture)\nregisterDoFuture()\ncl <- parallel::makeCluster(6)\nplan(cluster, workers = cl)\n\n\nres <- tune_grid(\n    xgb_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE, allow_par = T))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 11\n  min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean\n  <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl>\n1    12          6  0.0894           6.42e-2        17 rmse    standard    3.21\n2    23         11  0.00647          2.09e-3        10 rmse    standard    3.75\n3     8          1  0.000643         4.19e-9         9 rmse    standard   13.7 \n4    28          4  0.0000392        6.95e-6         8 rmse    standard   22.9 \n5    35         11  0.0000159        1.25e+1         4 rmse    standard   23.4 \n# … with 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(xgb_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 12\n  tree_depth = 6\n  learn_rate = 0.0894462983351361\n  loss_reduction = 0.0642365279282407\n  stop_iter = 17\n\nComputational engine: xgboost \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_xgb <- \n    predict(xgb_fit, testing(splits)) %>% \n    mutate(modelo = \"xgb\")\n\npred_xgb %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  .pred modelo\n  <dbl> <chr> \n1  28.5 xgb   \n2  35.5 xgb   \n3  22.0 xgb   \n4  21.7 xgb   \n5  19.3 xgb   \n6  18.9 xgb   \n```\n:::\n\n```{.r .cell-code}\nresult2 <- data.frame(pred = pred_xgb$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result2, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.37 \n2 rsq     standard       0.795\n3 mae     standard       2.57 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}