[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "데이터분석전문가(ADP) 실기 머신러닝 강의 소개",
    "section": "",
    "text": "강의 홈페이지 : https://statisticsplaybook.com/p/adp-r\n\nADP 23회 합격자 인터뷰\nhttps://www.youtube.com/watch?v=eSRZrkDpAow&t=3970s\n\n\nADP 24회 합격자 인터뷰\nhttps://youtu.be/-NfIcBSxU8w\n수업은 ADP 시험 일정에 맞춰서 7월 말부터 다시 시작 예정입니다. 이전 수업 영상과 자료는 결제 후에 보실 수 있습니다.\n\n\n\n\nCitationBibTeX citation:@online{don,\n  author = {Don Don and Don Don},\n  title = {데이터분석전문가(ADP) {실기} {머신러닝} {강의} {소개}},\n  date = {},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDon Don, and Don Don. n.d. “데이터분석전문가(ADP) 실기 머신러닝\n강의 소개.”"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I specialize in using R, python for data analysis, data visualizations. I am also passionate about making bike-sharing systems more efficient."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Don Don",
    "section": "",
    "text": "news\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nRMySQL\n\n\n\n\nRMySQL, DBI 패키지를 활용하여 R과 MySQL 연동하기\n\n\n\n\n\n\nJul 10, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\npython\n\n\n\n\nreticulate vscode 세팅\n\n\n\n\n\n\nJul 1, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMysql\n\n\n\n\nmac m1 pro에 MySQL 설치하기\n\n\n\n\n\n\nJun 28, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhexSticker\n\n\nR\n\n\n\n\nR 패키지 logo 만들기\n\n\n\n\n\n\nJun 24, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\nsensemakr 패키지 소개 및 간단한 논문 리뷰\n\n\n\n\n\n\nJun 24, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nglm\n\n\ncaret\n\n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nglm\n\n\n\n\nGLM에서 다중공선성 체크 해야 하는지\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nvscode에서 quarto와 파이썬 사용하기\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\nQuarto로 블로그 만들기 튜토리얼\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndocker\n\n\nrocker\n\n\n\n\nIntroduction to rocker\n\n\n\n\n\n\nOct 2, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npackrat\n\n\nrenv\n\n\n\n\nIntroduction to packrat, renv\n\n\n\n\n\n\nOct 2, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nfeature importance, partial dependence plot, shap value 소개\n\n\n\n\n\n\nJul 29, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nkernel density estimation 소개\n\n\n\n\n\n\nJun 22, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\nMultiple test에 대한 소개\n\n\n\n\n\n\nJun 18, 2021\n\n\nDon Don\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\noptimization\n\n\n\n\nADMM example code\n\n\n\n\n\n\nMay 30, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntimeseries\n\n\ntidymodels\n\n\n\n\ntidymodels를 이용한 시계열 모델링\n\n\n\n\n\n\nMay 8, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nrecipes\n\n\ntidymodels\n\n\n\n\ntidymodels에 대한 간단한 소개\n\n\n\n\n\n\nMar 6, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ngbm\n\n\n\n\ngradient boosting machine 소개\n\n\n\n\n\n\nJan 6, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\nIntroduction to MCMC\n\n\n\n\n\n\nJan 6, 2021\n\n\nDon Don\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/logistic_reg/index.html",
    "href": "posts/logistic_reg/index.html",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(recipes)\nlibrary(broom)"
  },
  {
    "objectID": "posts/logistic_reg/index.html#example",
    "href": "posts/logistic_reg/index.html#example",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "Example",
    "text": "Example\n먼저 다중회귀모형에서 다중공선성이 있을 때, 회귀계수의 추정량의 분산이 어떻게 변화하는지를 알아보자.\n간단하게, 설명변수 \\(X_1, X_2\\), 절편이 존재하는 다중회귀모형의 \\(\\beta\\)의 공분산은 다음과 같이 구할 수 있다.\n\\[\n\\begin{aligned}\ncov(\\hat{\\beta}) = \\begin{bmatrix}\nvar(\\hat{\\beta_0}) & cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & cov(\\hat{\\beta}_0, \\hat{\\beta}_2) \\\\\ncov(\\hat{\\beta}_1, \\hat{\\beta}_0) & var(\\hat{\\beta}_1) & cov(\\hat{\\beta}_1, \\hat{\\beta}_2) \\\\\ncov(\\hat{\\beta}_2, \\hat{\\beta}_0) & cov(\\hat{\\beta}_2, \\hat{\\beta}_1) & var(\\hat{\\beta}_2)\n\\end{bmatrix}\n=\\sigma^2(X^tX)^{-1}\n\\end{aligned}\n\\]\n표준화된 설명변수 \\(X_1, X_2\\)에 대한 상관계수가 \\(0.99, 0.95, 0.85\\)로 높은 경우를 가정해보자.\n\nxtx <- matrix(c(1, 0.99, 0.85, 0.99, 1, 0.95, 0.85, 0.95, 1), ncol = 3)\nxtx\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.99 0.85\n[2,] 0.99 1.00 0.95\n[3,] 0.85 0.95 1.00\n\n\n\nsolve(xtx)\n\n       [,1]   [,2]    [,3]\n[1,] -15.60  29.20 -14.480\n[2,]  29.20 -44.40  17.360\n[3,] -14.48  17.36  -3.184\n\n\n\\(X\\)는 표준화된 설명변수라고 할 때, 상관계수가 \\(0.99, 0.95, 0.85\\)로 높을 경우 역행렬을 구해보면 역행렬의 각 element는 모두 큰 값으로 커지게 된다(\\(\\sigma = 1\\)). 즉, 위의 식에 대입해서 보면 \\(\\hat{\\beta}\\)의 분산이 매우 커지는 것을 볼 수 있다."
  },
  {
    "objectID": "posts/logistic_reg/index.html#example-2",
    "href": "posts/logistic_reg/index.html#example-2",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "Example",
    "text": "Example\n\nset.seed(1)\nX1 <- rnorm(10)\nX2 <- X1*runif(10)\ny <- rbinom(10, 1, 0.3)\ndat <- data.frame(X1, X2, y)\n\nfit <- glm(y~., family = binomial, dat)\n\n\\((X^tWX)^{-1}\\)는 R base 함수인 vcov()를 통해 구할 수 있다.\n\nvcov(fit)\n\n            (Intercept)         X1         X2\n(Intercept)   0.9009862 -0.7664066  0.6210139\nX1           -0.7664066  3.5509802 -7.4328529\nX2            0.6210139 -7.4328529 28.2736799\n\n\n\\((X^tWX)^{-1}\\)를 직접 구해보면 다음과 같다. \\(w_{ii} = \\hat{\\pi}_i(1-\\hat{\\pi}_i)\\)\n\np <- fit$fitted.values\nw <- p*(1-p)\nX <- model.matrix(fit)\n\nsolve(t(X)%*%diag(w)%*%X)\n\n            (Intercept)         X1         X2\n(Intercept)   0.9009921 -0.7664113  0.6210083\nX1           -0.7664113  3.5509949 -7.4329015\nX2            0.6210083 -7.4329015 28.2739839\n\n\nvcov()의 \\((1, 1)\\) element와 \\(var(\\hat{\\beta_0})\\)의 결과가 같은 것을 볼 수 있다.\n\ntidy(fit)$std.error[1]^2\n\n[1] 0.9009862\n\n\n따라서 GLM에서도 다중회귀모형과 마찬가지로, 모형 적합 시에 다중공선성도 체크해줘야 한다."
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html",
    "href": "posts/ com_seperation/com_sep.html",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "",
    "text": "library(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html#complete-seperation-예제",
    "href": "posts/ com_seperation/com_sep.html#complete-seperation-예제",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "complete seperation 예제",
    "text": "complete seperation 예제\n\n\n\nY\nX1\nX2\n\n\n\n\n0\n1\n3\n\n\n0\n2\n2\n\n\n0\n3\n-1\n\n\n0\n3\n-1\n\n\n1\n5\n2\n\n\n1\n6\n4\n\n\n1\n10\n1\n\n\n1\n11\n0\n\n\n\n\n\\(X_1\\le 3\\)일 경우 \\(Y=0\\)이고, \\(X_1>3\\)일 경우 \\(Y=1\\)로 완벽하게 분리됨"
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html#왜-complete-seperation이-문제인가",
    "href": "posts/ com_seperation/com_sep.html#왜-complete-seperation이-문제인가",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "왜 complete seperation이 문제인가?",
    "text": "왜 complete seperation이 문제인가?\n\nLogistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음\n\n\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n\n\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n\n           1            2            3            4            5            6 \n2.220446e-16 9.861322e-11 3.179312e-12 3.179312e-12 1.000000e+00 1.000000e+00 \n           7            8 \n1.000000e+00 1.000000e+00 \n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred warning message 출력\n\n예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함\n즉, 너무 완벽하게 예측했을 경우 발생함\n\nglm.fit: algorithm did not converge warning message 출력\n\nMLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함"
  },
  {
    "objectID": "posts/com_sep/com_sep.html",
    "href": "posts/com_sep/com_sep.html",
    "title": "complete seperation in logistic regression",
    "section": "",
    "text": "library(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/com_sep/com_sep.html#complete-seperation-예제",
    "href": "posts/com_sep/com_sep.html#complete-seperation-예제",
    "title": "complete seperation in logistic regression",
    "section": "complete seperation 예제",
    "text": "complete seperation 예제\n\n\n\nY\nX1\nX2\n\n\n\n\n0\n1\n3\n\n\n0\n2\n2\n\n\n0\n3\n-1\n\n\n0\n3\n-1\n\n\n1\n5\n2\n\n\n1\n6\n4\n\n\n1\n10\n1\n\n\n1\n11\n0\n\n\n\n\n\\(X_1\\le 3\\)일 경우 \\(Y=0\\)이고, \\(X_1>3\\)일 경우 \\(Y=1\\)로 완벽하게 분리됨"
  },
  {
    "objectID": "posts/com_sep/com_sep.html#왜-complete-seperation이-문제인가",
    "href": "posts/com_sep/com_sep.html#왜-complete-seperation이-문제인가",
    "title": "complete seperation in logistic regression",
    "section": "왜 complete seperation이 문제인가?",
    "text": "왜 complete seperation이 문제인가?\n\nLogistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음\n\n\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n\n\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n\n           1            2            3            4            5            6 \n2.220446e-16 9.861322e-11 3.179312e-12 3.179312e-12 1.000000e+00 1.000000e+00 \n           7            8 \n1.000000e+00 1.000000e+00 \n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred warning message 출력\n\n예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함\n즉, 너무 완벽하게 예측했을 경우 발생함\n\nglm.fit: algorithm did not converge warning message 출력\n\nMLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함"
  },
  {
    "objectID": "posts/py_example/py_ex.html",
    "href": "posts/py_example/py_ex.html",
    "title": "quarto vscode and python",
    "section": "",
    "text": "참고\nhttps://quarto.org/docs/get-started/hello/vscode.html"
  },
  {
    "objectID": "posts/data/blog_building/build_blog.html",
    "href": "posts/data/blog_building/build_blog.html",
    "title": "Quarto로 블로그 만들기",
    "section": "",
    "text": "https://quarto.org/docs/websites/website-blog.html\nhttps://quarto.org/docs/output-formats/html-themes.html\nhttps://www.youtube.com/watch?v=DMgEGpqXEM4"
  },
  {
    "objectID": "posts/2021-06-12-admm/admm.html",
    "href": "posts/2021-06-12-admm/admm.html",
    "title": "ADMM example",
    "section": "",
    "text": "Given x, z and , to some initial value.\nRepeat:\n\n\\(x:= \\arg\\max_{x}(f(x) + \\frac{\\rho}{2}\\|Ax+Bz-c+\\mu\\|_2^2)\\)\n\\(x:= \\arg\\max_{x}(g(z) + \\frac{\\rho}{2}\\|Ax+Bz-c+\\mu\\|_2^2)\\)\n\\(\\mu:= \\mu + (Ax + Bz - c)\\)\nStopping criterion : quit \\(\\|r\\|_2<\\epsilon\\) and \\(\\|s\\|_2<\\epsilon\\) \n\n\n\n\nWe can define the primal and dual residuals in ADMM at step k+1.\n* Primal residuals : \\(r^{k+1} = Ax^{k+1} + Bz^{k+1} - c\\)\n* Dual residuals : \\(s^{k+1} = \\rho A^TB(z^{k+1} - z^k)\\)\n\nTherefore stopping criterion satisfies that \\(\\|r\\|_2\\) and \\(\\|s\\|_2\\) are smaller than any \\(\\epsilon\\)\n\n\n\n\n\\[\n\\begin{equation*}\n\\begin{aligned}\n& \\underset{\\beta}{\\text{minimize}}\n& & \\sum_{i=1}^n (y_i - \\beta_0 - x_i^t\\beta)^2 + \\lambda \\sum_{j = 1}^p |\\beta_j| \\\\\n\\end{aligned}\n\\end{equation*}\n\\]\n\\(\\Leftrightarrow\\)\n\\[\n\\begin{equation*}\n\\begin{aligned}\n& \\underset{\\beta}{\\text{minimize}}\n& & f(\\beta) + f(z) \\\\\n& \\text{subject to}\n& & I\\beta - IZ = 0\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\n\\[\n\\begin{align}\nr = I\\beta - IZ \\newline\nL_\\rho(\\beta, z, v) &= f(\\beta) + g(z) + v^tr + \\frac{\\rho}{2}||r||_2^2 \\newline\n                    &= f(\\beta) + g(z) + \\frac{\\rho}{2}||r+\\frac{1}{\\rho}v||_2^2 - \\frac{\\rho}{2}||v||_2^2 \\newline\n                    &= f(\\beta) + g(z) + \\frac{\\rho}{2}||r+\\mu||_2^2 - constant_v, \\quad \\mu = \\frac{1}{\\rho}v\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n\\beta^{k+1} &:= \\operatorname*{argmin}_\\beta (f(\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_\\beta (y-X\\beta)^t(y-X\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &\\Rightarrow -2X^ty + 2X^tX\\beta + \\rho\\beta - \\rho Z^k +\\rho\\mu^k = 0 \\newline\n            &\\Leftrightarrow (2X^tX + \\rho I)\\beta = 2X^ty + \\rho(Z^k - \\mu^k) \\newline\n            &\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n\\beta^{k+1} &:= \\operatorname*{argmin}_\\beta (f(\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_\\beta (y-X\\beta)^t(y-X\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &\\Rightarrow -2X^ty + 2X^tX\\beta + \\rho\\beta - \\rho Z^k +\\rho\\mu^k = 0 \\newline\n            &\\Leftrightarrow (2X^tX + \\rho I)\\beta = 2X^ty + \\rho(Z^k - \\mu^k) \\newline\n            &\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\n\\end{align}\n\\]\n\n\nThe prox operatior for \\(g(z) = \\lambda||z||_1\\)\n\\[\n\\begin{align}\nprox_{\\lambda, g}(z) &= \\operatorname*{argmin}_v (\\lambda||z||_1 + \\frac{1}{2}||z-v||_2^2) \\newline\n                     &= \\operatorname*{argmin}_v (||v||_1 + \\frac{1}{2\\cdot \\lambda}||z-v||_2^2) \\newline\n                     &\\therefore \\operatorname*{argmin}_{v_i} (\\frac{1}{2}(v_i - z_i)^2 + \\lambda|v_i|)\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nZ^{k+1} &:= \\operatorname*{argmin}_Z (g(Z) + \\frac{\\rho}{2}||I\\beta^{k+1} - IZ + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_Z (g(Z) + \\frac{\\rho}{2}||\\beta^{k+1} + \\mu^k - Z) ||_2^2 \\newline\n            &= \\operatorname*{argmin}_Z (g(Z) + \\frac{1}{2\\cdot \\frac{1}{\\rho}}||\\beta^{k+1} + \\mu^k - Z) ||_2^2 \\newline\n            &\\therefore prox_{\\frac{1}{\\rho}, g}(\\beta^{k+1} + \\mu^k)\n\\end{align}\n\\]\n\n\n\nGiven \\(\\beta\\), \\(z\\), \\(\\mu\\), \\(\\rho\\) to some initial value\nRepeat:\n\n\\(\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\\)\n\\(\\therefore Z^{k+1} = prox_{\\frac{1}{\\rho}, g}(\\beta^{k+1} + \\mu^k)\\)\n\\(\\mu^{k+1}:= \\mu^k + (\\beta^{k+1} - Z^{k+1})\\)\nStopping criterion : quit \\(\\|r\\|_2<\\epsilon\\) and \\(\\|s\\|_2<\\epsilon\\)\n\n\n\n\nprime resdual : \\(r^{k+1} = \\beta^{k+1} - z^{k+1}\\)\ndual resdual : $s^{k+1} = -(z^{k+1} - z^k) $"
  },
  {
    "objectID": "posts/2021-06-12-admm/admm.html#r-code",
    "href": "posts/2021-06-12-admm/admm.html#r-code",
    "title": "ADMM example",
    "section": "R code",
    "text": "R code\n\nnll <- function(X, Y, beta) {\n                A <- Y - X %*% beta\n                loglike <- crossprod(A)\n                return(loglike)\n}\n\n# Proximal operator\nprox.l1 <- function(u, lambda) {\n                uhat <- abs(u) - lambda\n                prox <- sign(u) * pmax(rep(0, length(u)), uhat)\n                return(prox)\n}\n\nl2norm <- function(x) sqrt(sum(x^2))\n\n\nADMM <- function(X,Y,rho=5,lambda=.1,iter=100, eps = 0.0001){\n                \n                n <- nrow(X)\n                p <- ncol(X)\n                \n                beta <- matrix(0, nrow=iter, ncol=p) \n                beta[1,] <- rep(0, p)\n                \n                obj <- rep(0, iter)\n                obj[1] <- nll(X, Y, beta[1,]) + lambda * sum(abs(beta[1,]))\n                \n                z <- matrix(0, nrow=iter, ncol=p)\n                v <- rep(0, p)     \n                \n                invmat <- solve(2*crossprod(X) + diag(rho, p))\n                \n                s <- 0    \n                r <- 0    \n                t <- 0\n                \n                for (t in 2:iter){\n                                \n                                beta[t,] <- invmat %*% (2*crossprod(X, Y) + rho * (z[t-1,]-v))\n                                z[t,] <- prox.l1(beta[t,] + v, lambda/rho)\n                                v <- v + beta[t,] - z[t,]\n                                obj[t] <- nll(X, Y, beta[t,]) + lambda * sum(abs(beta[t,]))\n                                \n                                r <- beta[t,] - z[t,]\n                                s <- -rho * (z[t,] - z[t-1,])\n                                \n                                r.norm <- l2norm(r)\n                                s.norm <- l2norm(s)\n                                \n                                if (r.norm < eps & s.norm < eps) {\n                                                break\n                                }\n                }\n                beta <- beta[-c(t+1:iter),]\n                obj <- obj[-c(t+1:iter)]\n                result <- list(\"beta.hat\" = beta[nrow(beta),], \"beta\"=beta, \"objective\"=obj, \"iter\"=t)\n                return(result)\n}\n\nx <- cbind(1, matrix(rnorm(1000*4), ncol = 4))\nbeta <- c(1.4, -2, -3, 4, 5)\n\neps <- rnorm(1000*1)　\ny <- x%*%beta + eps \n\nADMM(X = x, Y = y)\n\n$beta.hat\n[1]  1.379323 -1.990279 -2.998234  4.018089  5.044086\n\n$beta\n         [,1]      [,2]      [,3]     [,4]     [,5]\n[1,] 0.000000  0.000000  0.000000 0.000000 0.000000\n[2,] 1.376414 -1.985059 -2.991330 4.008202 5.031837\n[3,] 1.379269 -1.990212 -2.998172 4.018014 5.044009\n[4,] 1.379323 -1.990278 -2.998234 4.018089 5.044086\n[5,] 1.379323 -1.990279 -2.998234 4.018089 5.044086\n\n$objective\n[1] 58940.123  1022.952  1022.613  1022.613  1022.613\n\n$iter\n[1] 5"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html",
    "href": "posts/2021-08-24-packrat/packrat_renv.html",
    "title": "packrat and renv",
    "section": "",
    "text": "데이터과학 전 분야에서 reproducible research가 중요해지고 있다. reproducible research를 위해서는 어떤 개발환경에서든 원 개발 버전과 동일한 환경을 보존해야하는데, 이 때 장애물이 되는 것이 운영체제, r 버전, r 패키지 버전 별로 동일한 환경을 구축하는 일이다. reproducible research를 쉽게 할 수 있게 해주는 대표적인 패키지인 renv, packrat에 대해 알아보자."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#docker-소개",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#docker-소개",
    "title": "packrat and renv",
    "section": "docker 소개",
    "text": "docker 소개\ndocker 설치 방법 및 구동 방법은 https://www.youtube.com/watch?v=VVxvL4xRPjU 에 잘 정리되어있다.\nR은 도커 허브에 rocker라는 이름으로 등록된 많은 image 파일이 있다. 필요한 상황에 맞는 image를 찾아서 다운을 받으면 된다.\nrocker image 설치 방법\nwindow powershell을 키고 아래 문법을 입력하면 설치가 된다.\n$ docker pull rocker/rstudio:3.6.0\n컴퓨터에 설치된 docker image 목록을 확인할 수 있다.\n$ docker image list\nrocekr image 실행 방법\n\n프로젝트 파일을 생성한다.\nr server에서 실행 후 저장한 R script, csv 등의 모든 파일을 local 컴퓨터에 저장하는 폴더를 만드는 것이다.\n\n\n아래 코드를 실행하면 다운받은 image가 실행된다.\n\n$ docker run -d -e USERID=$UID -e PASSWORD=1111 -v ${pwd}:/work -p 7009:8787 rocker/rstudio:3.6.0\n\n\n참고 : https://cultivo-hy.github.io/docker/image/usage/2019/03/14/Docker%EC%A0%95%EB%A6%AC/\n\n\n옵션\n설명\n\n\n\n\n-d\ndetach mode(백그라운드 모드)\n\n\n-p\n호스트와 컨테이너의 포트를 연결\n\n\n-e\n컨테이너 내에서 사용할 환경변수 설정\n\n\n-v\n호스트와 컨테이너의 디렉토리를 연결\n\n\n\n\n구글 크롬에 접속해서 localhost:7009를 주소창에 입력한다.\n\n아이디 : rstudio\n비밀번호 : -e PASSWORD에 입력한 1111\n아래와 같은 rstudio 창이 나오면 완료"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#packrat-사용법",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#packrat-사용법",
    "title": "packrat and renv",
    "section": "packrat 사용법",
    "text": "packrat 사용법\nrstudio 서버에서 아래 코드를 순차적으로 실행한다.\n먼저 docker run에서 마운드한 위치는 /work이므로 setwd로 위치를 변경해준다.\n\nsetwd('/work')\n\n새로운 프로젝트를 생성하는 것이므로 R script, csv 데이터 등을 저장할 새 디렉토리를 생성한다. packrat::init을 실행하게 되면 일반 프로젝트에서 packrat 프로젝트로 전환된다. 즉. 자체 개인 패키지 라이브러리가 있는 프로젝트로 전환되며, 설치된 패키지는 해당 프로젝트에서만 사용할 수 있다.\n\ninstall.packages(\"packrat\")\n\npackrat::init(\"/work\")\n\n\n\n.libPaths()\npackrat::on(\"/work\")\n\n.libPaths()\n\n\npackrat::on으로 packrat 모드를 키게 되면 work 디렉토리에 packrat 폴더로 패키지 저장 위치가 업데이트된다.\n\ninstall.packages(\"tidyverse\")\n\n\n패키지 snapshot을 보면 R 3.6 기준 2019-07-05에 업데이트된 tidyverse 패키지를 설치한다. 패키지 설치는 local에 설치하는 속도보다 몇 배 이상 오래걸릴 수 있다.\nR script를 새로 생성하고 적당한 예제 코드를 작성 후에 저장을 하게되면 work 디렉토리에 R script 파일이 저장되는 것을 볼 수 있다. server 상에서 저장한 이 파일은 local 컴퓨터에 생성했던 프로젝트 파일 test_project2에도 동일하게 저장된다.\n\n\npackrat::snapshot()\n\npackrat::snapshot()을 이용하면 라이브러리의 현재 상태(현재 설치된 패키지 버전)를 저장할 수 있다.\n이것도 저장하는 속도가 생각보다 많이 느리다. 한번 저장해놓으면 계속 쓸 수 있지만 속도가 느린 것은 치명적인 단점인 것 같다. binary package를 이용하면 속도 문제는 개선할 수 있다. (docker post 참고)\n\npackrat::restore()\n\npackrat::restore()을 이용하면 최근 스냅샷에 저장된 라이브러리 상태를 복원할 수 있다."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#init",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#init",
    "title": "packrat and renv",
    "section": "init",
    "text": "init\n다음의 코드를 실행하면, 프로젝트 환경을 초기화한다. 먼저 renv 파일이 생성되고, r version, package version이 json 파일로 저장된 renv.lock 파일이 생성된다. 또 .Rprofile 파일이 생성된다. renv 파일에는 프로젝트에 설치된 패키지가 저장되는 library 파일이 함께 생성된다.\n\nrenv::init()"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#install-package",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#install-package",
    "title": "packrat and renv",
    "section": "Install package",
    "text": "Install package\n프로젝트 내에 특정 패키지를 설치하고 싶을 때는 renv 내에 특정 함수를 이용하면 된다. devtools를 따로 설치하지 않아도 특정 버전만 지정해주면 old version 패키지를 설치할 수 있다. CRAN에 등록되지 않은 github 소스에서도 다이렉트로 패키지를 설치할 수 있다.\n\n# install the latest version of 'digest'\nrenv::install(\"digest\")\n\n# install an old version of 'digest' (using archives)\nrenv::install(\"digest@0.6.18\")\n\n# install 'digest' from GitHub (latest dev. version)\nrenv::install(\"eddelbuettel/digest\")\n\n# install a package from GitHub, using specific commit\nrenv::install(\"eddelbuettel/digest@df55b00bff33e945246eff2586717452e635032f\")"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#snapshot",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#snapshot",
    "title": "packrat and renv",
    "section": "snapshot",
    "text": "snapshot\nsnapshot은 프로젝트 환경의 현재 상태를 renv.lock 파일에 저장한다. 사용한 패키지와 버전에 대한 세부 정보가 기록된다. 만약 패키지를 추가 설치했을 경우, 다시 snapshot을 실행하면 최신상태로 업데이트 된다.\n\nrenv::snapshot()"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#restore",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#restore",
    "title": "packrat and renv",
    "section": "restore",
    "text": "restore\nrestore는 renv::snapshot()을 실행했던 상태로 복구하는 함수이다. 즉 renv.lock 파일에 업데이트된 최신 버전 패키지를 로드하게 된다. 다른 사람들이 renv.lock 파일을 가져와서 개발환경을 재현하려고 하면 github에서 파일을 다운받고, renv::init() 실행 후 restore 함수를 실행하면 프로젝트 환경이 재현된다.\n\n#renv::init() \nrenv::restore()\n\nWINDOW에서 renv 패키지를 이용해서 프로젝트 환경을 세팅하고, MAC에서 실험했을 때 프로젝트 환경이 동일하게 세팅되는 것을 확인했다."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#renv-and-docker",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#renv-and-docker",
    "title": "packrat and renv",
    "section": "renv and docker",
    "text": "renv and docker\nDocker 이미지를 구울 때, renv를 함께 사용하기도 하는 것 같다. 굳이 docker를 쓰는 이유는?? 모르겠다. docker 이미지를 구울 때 renv::restore()를 이용하면 패키지 설치를 새로 안해도 되니까 이미지 굽는 속도가 많이 개선될 것 같다. 참고 링크\n\nFROM rocker/r-base:4.0.2\n# install renv package\nRUN Rscript -e \"install.packages('renv')\"\n# copy everything to docker, including renv.lock file\nCOPY . /app\n# set working directory\nWORKDIR /app\n# restore all the packages\nRUN Rscript -e \"renv::restore()\"\n# run our R code\nCMD [\"Rscript\", \"main.R\"]\n\n참고 자료\nhttps://www.youtube.com/watch?v=Z0Tm-Y7vzNQ\nhttps://www.youtube.com/watch?v=VVxvL4xRPjU\nhttps://rstudio.github.io/packrat/\nhttps://6chaoran.wordpress.com/2020/07/20/introduction-of-renv-package/\nhttps://www.seanwarlick.com/post/setting-up-renv/\nhttps://rstudio.github.io/renv/reference/install.html"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html",
    "title": "Modeltime with tidymodels",
    "section": "",
    "text": "modeltime 패키지는 tidymodels와 연동이 가능한 시계열 모델링 관련 패키지이다. tidymodels에도 auto arima, ma 모형 같은 간단한 시계열 모델이 있지만 다른 머신러닝 모델에 비해서 쓸 수 있는 모델이 제한적이다. 이러한 단점을 modeltime 패키지가 해결해준다.\nmodeltime 시계열 모델링에 특화된 패키지로 시계열에 특화된 모델링과 전처리 관련 함수가 내장되어 있고, tidymodels의 워크플로우를 거의 그대로 이용할 수 있어서 향후 tidymodels가 R의 대표적인 머신러닝 패키지가 된다면 시계열 파트에서는 modeltime 패키지가 주축이 될 것 같다.\nmodeltime 패키지는 하나의 단일 패키지가 아니라 다양한 머신러닝 패키지와 연동해서 하나의 시계열 생태계를 구축하고 있다. 대표적인 패키지는 다음과 같다.\n\nModeltime : 시계열 머신러닝 관련 패키지\nModeltime H2O : H2O의 autoML과 연동이 가능함\nModeltime GluonTS : 시계열 관련 딥러닝 패키지\nModeltime Ensemble : Modeltime 관련 앙상블 패키지\nModeltime Resample : Backtesting 관련 패키지\nTimetk : feature engineering, Data wrangling, time series visualization"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#preparations-준비작업",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#preparations-준비작업",
    "title": "Modeltime with tidymodels",
    "section": "Preparations (준비작업)",
    "text": "Preparations (준비작업)\n\nLibraries\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(skimr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(gt)\nlibrary(timetk)\nlibrary(tidyquant)\nlibrary(sknifedatar)\n\n# Visualization\nlibrary(ggthemes)\nlibrary(ggsci)\nlibrary(viridis)\nlibrary(ggExtra)\n\n\ntheme_set(theme_bw())\n\n\n\nData load\n\nfile_path <- getwd()\nfiles <- list.files(file_path)\nfiles\n\n[1] \"modeltime.qmd\"       \"modeltime.rmarkdown\" \"rdata.csv\"          \n\nrdata <- read.csv(file.path(file_path, \"rdata.csv\"), fileEncoding = \"CP949\", encoding = \"UTF-8\")"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-overview-데이터-기본정보",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-overview-데이터-기본정보",
    "title": "Modeltime with tidymodels",
    "section": "Data overview (데이터 기본정보)",
    "text": "Data overview (데이터 기본정보)\n\nData\n\nglimpse(rdata)\n\nRows: 55,392\nColumns: 20\n$ time      <chr> \"2015-01-01 01:00:00\", \"2015-01-01 02:00:00\", \"2015-01-01 03…\n$ 기온      <dbl> -4.4, -4.6, -4.7, -5.0, -5.0, -5.3, -5.7, -5.7, -5.4, -5.0, …\n$ 강수      <dbl> 0.00, 0.00, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ 풍속      <dbl> 5.4, 4.9, 6.2, 5.0, 5.5, 4.0, 5.0, 4.6, 6.7, 6.5, 6.4, 6.0, …\n$ 풍향      <dbl> 340.0000, 340.0000, 320.0000, 320.0000, 320.0000, 340.0000, …\n$ 습도      <dbl> 47, 50, 50, 56, 52, 58, 58, 56, 57, 59, 60, 62, 58, 62, 61, …\n$ 증기압    <dbl> 2.1, 2.2, 2.2, 2.4, 2.2, 2.4, 2.3, 2.3, 2.3, 2.5, 2.6, 2.6, …\n$ 이슬점    <dbl> -14.0, -13.4, -13.5, -12.4, -13.3, -12.2, -12.6, -13.0, -12.…\n$ 기압      <dbl> 1020.3, 1020.3, 1020.7, 1020.6, 1020.4, 1020.7, 1021.3, 1021…\n$ 해면기압  <dbl> 1024.0, 1024.0, 1024.5, 1024.4, 1024.2, 1024.5, 1025.1, 1025…\n$ 일조      <dbl> NA, NA, NA, NA, NA, NA, NA, 0.0, 0.0, 0.6, 0.9, 0.5, 0.8, 0.…\n$ 일사      <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.025, 0.14…\n$ 전운량    <int> 6, NA, 6, 6, 6, 6, 6, 3, 6, 6, 7, 8, 8, 8, 8, 6, 6, 8, NA, N…\n$ 시정      <dbl> 1500.000, 1750.000, 2000.000, 2000.000, 2000.000, 2000.000, …\n$ 지면온도  <dbl> -4.4, -4.6, -4.7, -5.0, -5.0, -5.3, -5.7, -5.7, -5.4, -5.0, …\n$ floating  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ warehouse <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 12, 150, 195, 270, 254, 184, 131, 18…\n$ dangjin   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 8, 227, 263, 356, 333, 225, 199, 218…\n$ ulsan     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\nskim(rdata)\n\n\nData summary\n\n\nName\nrdata\n\n\nNumber of rows\n55392\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntime\n0\n1\n19\n19\n0\n55392\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n기온\n0\n1.00\n12.21\n10.36\n-19.3\n3.60\n12.60\n21.10\n36.80\n▁▅▇▇▂\n\n\n강수\n0\n1.00\n0.12\n1.05\n0.0\n0.00\n0.00\n0.00\n102.70\n▇▁▁▁▁\n\n\n풍속\n0\n1.00\n1.96\n1.56\n0.0\n0.70\n1.60\n2.90\n11.70\n▇▃▁▁▁\n\n\n풍향\n0\n1.00\n164.29\n129.50\n0.0\n20.00\n180.00\n290.00\n360.00\n▇▁▃▂▆\n\n\n습도\n0\n1.00\n75.34\n20.14\n10.0\n61.00\n79.00\n94.00\n100.00\n▁▂▃▅▇\n\n\n증기압\n82\n1.00\n12.93\n8.92\n1.0\n5.40\n10.20\n19.20\n42.90\n▇▅▃▂▁\n\n\n이슬점\n85\n1.00\n7.37\n11.04\n-22.4\n-1.70\n7.30\n16.90\n30.20\n▁▆▇▇▅\n\n\n기압\n79\n1.00\n1014.05\n8.36\n983.6\n1007.40\n1014.30\n1020.60\n1036.30\n▁▃▇▇▂\n\n\n해면기압\n78\n1.00\n1017.37\n8.48\n986.6\n1010.60\n1017.60\n1024.00\n1039.70\n▁▃▇▇▂\n\n\n일조\n25453\n0.54\n0.52\n0.45\n0.0\n0.00\n0.60\n1.00\n1.00\n▇▁▁▁▇\n\n\n일사\n0\n1.00\n0.54\n0.83\n0.0\n0.00\n0.01\n0.89\n4.85\n▇▂▁▁▁\n\n\n전운량\n12427\n0.78\n5.23\n3.84\n0.0\n1.00\n6.00\n9.00\n10.00\n▇▂▃▅▇\n\n\n시정\n0\n1.00\n1754.20\n1024.17\n3.0\n1100.00\n1800.00\n2029.00\n6454.00\n▅▇▁▁▁\n\n\n지면온도\n0\n1.00\n12.21\n10.36\n-19.3\n3.60\n12.60\n21.10\n36.80\n▁▅▇▇▂\n\n\nfloating\n28344\n0.49\n122.42\n192.34\n0.0\n0.00\n0.00\n192.25\n753.00\n▇▁▁▁▁\n\n\nwarehouse\n2040\n0.96\n95.81\n150.72\n0.0\n0.00\n0.00\n152.00\n595.00\n▇▁▁▁▁\n\n\ndangjin\n2040\n0.96\n140.11\n221.67\n0.0\n0.00\n0.00\n227.00\n881.00\n▇▁▁▁▁\n\n\nulsan\n3528\n0.94\n66.28\n104.17\n0.0\n0.00\n0.00\n104.00\n448.00\n▇▁▁▁▁\n\n\nhour\n0\n1.00\n11.50\n6.92\n0.0\n5.75\n11.50\n17.25\n23.00\n▇▇▆▇▇"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-preprocessing",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-preprocessing",
    "title": "Modeltime with tidymodels",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\nrdata %>% \n    select(-hour) %>% \n    mutate(time = ymd_hms(time)) %>% \n    filter(between(time, ymd('2018-03-01'), ymd('2021-01-31'))) -> rdata\nrdata %>% glimpse()  \n\nRows: 25,609\nColumns: 19\n$ time      <dttm> 2018-03-01 00:00:00, 2018-03-01 01:00:00, 2018-03-01 02:00:…\n$ 기온      <dbl> 3.1, 2.8, 2.6, 2.0, 2.2, 4.1, 3.5, 2.2, 1.0, 0.3, 0.6, 0.5, …\n$ 강수      <dbl> 0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 1.80, 0.00, 0.00, 0.05, …\n$ 풍속      <dbl> 3.6, 0.7, 3.2, 1.9, 2.1, 4.4, 7.9, 6.4, 7.7, 8.9, 7.9, 9.1, …\n$ 풍향      <dbl> 340, 140, 320, 230, 180, 270, 320, 290, 320, 320, 320, 320, …\n$ 습도      <dbl> 96, 97, 95, 97, 97, 97, 93, 86, 82, 71, 63, 58, 60, 60, 56, …\n$ 증기압    <dbl> 7.3, 7.2, 7.0, 6.8, 6.9, 7.9, 7.3, 6.1, 5.4, 4.4, 4.0, 3.7, …\n$ 이슬점    <dbl> 2.5, 2.3, 1.8, 1.5, 1.7, 3.6, 2.4, 0.0, -1.7, -4.3, -5.6, -6…\n$ 기압      <dbl> 1001.3, 1001.9, 1002.6, 1002.8, 1003.0, 1001.8, 1002.7, 1004…\n$ 해면기압  <dbl> 1004.9, 1005.5, 1006.2, 1006.4, 1006.6, 1005.4, 1006.3, 1007…\n$ 일조      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0…\n$ 일사      <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.69, …\n$ 전운량    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ 시정      <dbl> 922, 4315, 2601, 1717, 1957, 571, 57, 436, 593, 593, 711, 82…\n$ 지면온도  <dbl> 3.1, 2.8, 2.6, 2.0, 2.2, 4.1, 3.5, 2.2, 1.0, 0.3, 0.6, 0.5, …\n$ floating  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 313, 532, 607, 614, 608, 641,…\n$ warehouse <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 209, 296, 315, 474, 544, 496,…\n$ dangjin   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 318, 490, 550, 727, 733, 672,…\n$ ulsan     <int> 0, 0, 0, 0, 0, 0, 0, 0, 4, 35, 71, 82, 334, 372, 346, 318, 2…\n\nrdata %>% \n  summarise(across(.fns = ~sum(is.na(.))/length(.)))\n\n  time 기온 강수 풍속 풍향 습도      증기압     이슬점        기압    해면기압\n1    0    0    0    0    0    0 0.001835292 0.00191339 0.001718146 0.001679097\n       일조 일사    전운량 시정 지면온도 floating warehouse dangjin ulsan\n1 0.4545668    0 0.1552579    0        0        0         0       0     0"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#univariate-timeseries-analysis",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#univariate-timeseries-analysis",
    "title": "Modeltime with tidymodels",
    "section": "Univariate timeseries analysis",
    "text": "Univariate timeseries analysis\n울산 지역의 전력 발전량 데이터만 활용할 것이기 때문에 날짜 변수를 제외한 나머지 변수는 제거했다.\n\nulsan <- rdata %>% \n  select(-c(dangjin, warehouse, floating)) %>% \n  select(time, ulsan) %>% \n  rename(date = time, value = ulsan)\n\n\nTime series visualization\ntidymodels의 경우 train/test 분리를 위해서 initial_split()을 활용했는데 시계열 데이터의 경우 특정 날짜를 기준으로 잘라야하기 때문에 Modeltime 패키지에 내장되어있는 initial_time_split()을 이용한다. 특정 날짜를 기준으로 자르고 싶을 경우 timeseries_split()을 이용할 수도 있다.\n\ntk_time_series_cv_plan() : split된 object를 데이터프레임으로 전환\nplot_time_series_cv_plan() : sampling된 데이터를 이용해서 시계열 그래프 생성\n\n\ninitial_time_split(data = ulsan, prop = 0.9) %>% \n  tk_time_series_cv_plan() %>% \n  plot_time_series_cv_plan(date, value,\n                           .interact = FALSE, \n                           .title = \"Partition Train / Test\")\n\n\n\n\n\n\nSplit train/test\n\nmonths <- 1\n\ntotal_months <- lubridate::interval(base::min(ulsan$date),\n                                    base::max(ulsan$date)) %/%  \n                                    base::months(1)\n\n\nprop <- (total_months - months) / total_months\n\nsplits <- rsample::initial_time_split(ulsan, prop = prop)\n\n\nsplits %>%\n  timetk::tk_time_series_cv_plan() %>%  \n  timetk::plot_time_series_cv_plan(date, value) \n\n\n\n\n\n\n\nModel fitting\n모델 fitting은 tidymodels 패키지의 방식과 동일하다. 현재는 default 세팅으로 모델을 fitting했지만 튜닝 파라미터가 있을 경우 이전에 tidymodels에서 했던 방식 그대로 grid_latin_hypercube(), grid_random, bayes tuning 등을 이용해서 최적의 파라미터를 찾고 모델을 fitting 해야한다.\n\n# Exponential smoothing \nmodel_fit_ets <- modeltime::exp_smoothing() %>%\n    parsnip::set_engine(engine = \"ets\") %>%\n    parsnip::fit(value ~ date, data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# ARIMA \nmodel_fit_arima <- modeltime::arima_reg() %>%\n    parsnip::set_engine(\"auto_arima\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# ARIMA Boost\nmodel_fit_arima_boost <- modeltime::arima_boost() %>%\n    parsnip::set_engine(\"auto_arima_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date), \n        data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# Prophet\nmodel_fit_prophet <- modeltime::prophet_reg() %>%\n    parsnip::set_engine(\"prophet\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# Prophet Boost\nmodel_fit_prophet_boost <- modeltime::prophet_boost() %>%\n    parsnip::set_engine(\"prophet_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date), \n        data = training(splits))\n\n\n\nModel time table\nmodeltime_table에 fitting한 모델을 추가한다. modeltime_table은 이전에 서술했다시피 각 모델이 재대로 적합되었는지 확인하고, 이후 예측 워크플로우를 위해서 modeltime_table 구조를 이용하므로 model table을 에러 없이 세팅하는 것이 중요하다.\n\nmodel_tbl <- modeltime::modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet,\n    model_fit_prophet_boost)\n\nmodel_tbl\n\n# Modeltime Table\n# A tibble: 5 × 3\n  .model_id .model   .model_desc                              \n      <int> <list>   <chr>                                    \n1         1 <fit[+]> ETS(A,N,A)                               \n2         2 <fit[+]> ARIMA(5,0,0)(2,1,0)[24]                  \n3         3 <fit[+]> ARIMA(2,0,1)(2,1,0)[24] W/ XGBOOST ERRORS\n4         4 <fit[+]> PROPHET                                  \n5         5 <fit[+]> PROPHET W/ XGBOOST ERRORS                \n\n\n\n\ncalibration\n이전에 만든 modeltime_table을 test 데이터에 적합시켜서 보정을 하는 단계이다.\n\ncalibration_tbl <- model_tbl %>%\n    modeltime::modeltime_calibrate(testing(splits))  \n\n\ncalibration_tbl %>%\n    modeltime::modeltime_accuracy() %>%   \n    flextable::flextable() %>% \n    flextable::bold(part = \"header\") %>% \n    flextable::bg(bg = \"#D3D3D3\", part = \"header\") %>% \n    flextable::autofit()\n\nℹ We have detected a possible intermittent series, you can change the default metric set to the extended_forecast_accuracy_metric_set() containing the MAAPE metric, which is more appropriate for this type of series.\n\n\n\n\n\n\n\n\n\n\n.model_id\n\n\n\n\n.model_desc\n\n\n\n\n.type\n\n\n\n\nmae\n\n\n\n\nmape\n\n\n\n\nmase\n\n\n\n\nsmape\n\n\n\n\nrmse\n\n\n\n\nrsq\n\n\n\n\n\n\n\n\n1\n\n\n\n\nETS(A,N,A)\n\n\n\n\nTest\n\n\n\n\n47.69905\n\n\n\n\nInf\n\n\n\n\n2.191688\n\n\n\n\n146.0680\n\n\n\n\n73.01855\n\n\n\n\n0.7655620\n\n\n\n\n\n\n2\n\n\n\n\nARIMA(5,0,0)(2,1,0)[24]\n\n\n\n\nTest\n\n\n\n\n22.46559\n\n\n\n\nInf\n\n\n\n\n1.032255\n\n\n\n\n140.5838\n\n\n\n\n46.02106\n\n\n\n\n0.7889317\n\n\n\n\n\n\n3\n\n\n\n\nARIMA(2,0,1)(2,1,0)[24] W/ XGBOOST ERRORS\n\n\n\n\nTest\n\n\n\n\n42.10938\n\n\n\n\nInf\n\n\n\n\n1.934853\n\n\n\n\n142.5776\n\n\n\n\n58.27198\n\n\n\n\n0.7888361\n\n\n\n\n\n\n4\n\n\n\n\nPROPHET\n\n\n\n\nTest\n\n\n\n\n30.97759\n\n\n\n\nInf\n\n\n\n\n1.423366\n\n\n\n\n144.1818\n\n\n\n\n48.23531\n\n\n\n\n0.7779553\n\n\n\n\n\n\n5\n\n\n\n\nPROPHET W/ XGBOOST ERRORS\n\n\n\n\nTest\n\n\n\n\n58.43433\n\n\n\n\nInf\n\n\n\n\n2.684955\n\n\n\n\n146.4095\n\n\n\n\n70.19536\n\n\n\n\n0.7788829\n\n\n\n\n\n\n\n\n\n\n\nvisualization the forecast test\n\ncalibration_tbl %>%\n    modeltime::modeltime_forecast(new_data = testing(splits), \n                                  actual_data = ulsan,\n                                  conf_interval = 0.90) %>%\n    modeltime::plot_modeltime_forecast(.legend_show = TRUE, \n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\nRefit\ntrain/test를 합한 original 데이터를 이용해서 refitting을 진행하는 단계이다.\n\nrefit_tbl <- calibration_tbl %>%\n    modeltime::modeltime_refit(data = ulsan)\n\nfrequency = 24 observations per 1 day\nfrequency = 24 observations per 1 day\nfrequency = 24 observations per 1 day\n\n\n\n\nforecast\nrefitting된 모델을 이용해서 지정한 time interval에 대해 forecast를 수행하는 단계이다.\n\nforecast_tbl <- refit_tbl %>%\n    modeltime::modeltime_forecast(\n        h = \"1 month\",\n        actual_data = ulsan,\n        conf_interval = 0.90\n    ) \n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nforecast_tbl %>%\n    modeltime::plot_modeltime_forecast(.interactive = TRUE,\n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\nAggregate model\nAccuracy 향상을 위해서 적합시킨 5 가지 모델을 평균내서 최종 모델을 산출한다. Modeltime.ensemble을 이용하면 더 세련된 앙상블 기법을 이용할 수 있다.\n\nmean_forecast_tbl <- forecast_tbl %>%\n    dplyr::filter(.key != \"actual\") %>%\n    dplyr::group_by(.key, .index) %>%\n    dplyr::summarise(across(.value:.conf_hi, mean)) %>%\n    dplyr::mutate(\n        .model_id   = 6,\n        .model_desc = \"AVERAGE OF MODELS\"\n    )\n\n`summarise()` has grouped output by '.key'. You can override using the\n`.groups` argument.\n\n# Visualize aggregate model \nforecast_tbl %>%\n    dplyr::filter(.key == \"actual\") %>%\n    dplyr::bind_rows(mean_forecast_tbl) %>%\n    modeltime::plot_modeltime_forecast()  \n\n\n\n\n\n\n\n참고 자료\nH2O 관련\nhttps://www.r-bloggers.com/2021/03/introducing-modeltime-h2o-automatic-forecasting-with-h2o-automl/\ngluonTS 관련\nhttps://cran.r-project.org/web/packages/modeltime.gluonts/vignettes/getting-started.html#references\nModeltime 관련\nhttps://cran.r-project.org/web/packages/modeltime/vignettes/getting-started-with-modeltime.html\nhttps://www.adam-d-mckinnon.com/posts/2020-10-10-forecastpeopleanalytics/"
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html",
    "title": "multiple test",
    "section": "",
    "text": "다중검정은 multiple test, simultaneous tes, joint test 등으로 불리우는데, 보통 ANOVA 이후 집단 간의 세부적인 차이를 알아보기 위한 사후 검정으로 활용된다. 이번에 다중 검정 관련 논문을 읽을 일이 있어서 기본 개념에 대해 정리를 해보려고 한다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#one-way-anova",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#one-way-anova",
    "title": "multiple test",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n세 집단에 약물을 복용했을 때 치료 효과가 있는지 ANOVA를 실시한다고 해보자. ANOVA를 수식으로 표현하면 다음과 같다.\n\\[\nY_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}, \\quad i = 1,2, 3 \\quad j = 1, \\cdots ,n \\\\\n\\alpha_i : \\text{i - th treatment effect}\n\\]\n수식을 직관적으로 표로 표현하면 다음과 같다.\n\n\n\n\n\n\n\n\n\n\nTreat \\ Rep\n\\[\n1\n\\]\n\\[\n2\n\\]\n\\[\n\\cdots\n\\]\n\\[\nn\n\\]\n\n\n\n\n\\[\n1\n\\]\n\\[\n                Y_{11}\n                \\]\n\\[\n                         Y_{12}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{1n}\n                                           \\]\n\n\n\\[\n2\n\\]\n\\[\n                Y_{21}\n                \\]\n\\[\n                         Y_{22}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{2n}\n                                           \\]\n\n\n\\[\n3\n\\]\n\\[\n                Y_{31}\n                \\]\n\\[\n                         Y_{32}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{3n}\n                                           \\]\n\n\n\nANOVA에 대한 가설은 다음과 같이 도출될 수 있다.\n\\[\nH_0 : \\alpha_1 = \\alpha_2 = \\alpha_3 \\quad \\text{(no treatment effect)} \\\\\nH_1: \\text{Not }H_0\n\\]\n다음과 같은 가설 설정에 대해서 등분산 가정 하에 ANOVA를 수행했을 때 항상 두 가지 결론이 도출될 수 있다.\n\n귀무가설을 기각하지 못할 경우\n\n즉 세 집단 간의 평균 차이가 없다(혹은 약물의 치료 효과가 없다)\n\n귀무가설을 기각할 경우\n\n즉, 세 집단 간의 평균 차이가 존재한다.\n\n\n여기서 귀무가설을 기각할 경우를 주목해보자. 귀무가설을 기각할 경우 내릴 수 있는 결론은 “세 집단 간의 평균 차이가 존재한다”이다. 다시 풀어서 써보면 “세 집단 간 중에 적어도 하나는 평균 차이가 존재한다”이며, 이는 어느 집단 간에 차이가 나는지 모른다는 의미와 같다. 즉, 다음과 같은 경우의 수가 도출될 수 있다.\n\\[H_1 : \\\\\n\\alpha_1 = \\alpha_2 > \\alpha3 \\\\\n\\alpha_1 = \\alpha_2 < \\alpha3 \\\\\n\\alpha_1 < \\alpha_2 = \\alpha3 \\\\\n\\vdots \\\\\n\\alpha_1 \\neq \\alpha_2 \\neq \\alpha3\n\\]\nANOVA test로는 어느 집단 간에 평균 차이가 존재하는지 모르기 때문에 사후검정 을 추가적으로 실시해야만 이를 알 수 있다. 이 때 수행하는 것이 다중 검정(multiple test)이다.\n\\[\nH_{01}: \\alpha_1 = \\alpha_2, \\quad H_{02}: \\alpha_1 = \\alpha_3, \\quad H_{03}: \\alpha_2 = \\alpha_3,\n\\]\n가설의 개수는 treatment 3일 때, 2개의 쌍을 뽑는 것이므로 \\(3 \\choose 2\\)가 된다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#multiple-test",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#multiple-test",
    "title": "multiple test",
    "section": "Multiple test",
    "text": "Multiple test\n다중 검정이 왜 필요한지에 대해 이해했으니, 이제 다중 검정을 실시할 때의 문제점에 대해 알아보자.\n\\(m\\)개의 가설이 있다고 해보자.\n\\[\nH_{0i} : \\mu_{1i} = \\mu_{2i}, \\quad i = 1, \\cdots, m\n\\]\n각 가설에 대한 제 1종 오류를 수식으로 표현하면 다음과 같다.\n\\[\nE_i = \\{\\text{reject }H_{0i} \\text{ when }H_{0i}\\text{ is true}\\}, \\quad i = 1, \\cdots, m\n\\]\n\\(m\\) 개의 가설에 대한 제 1종 오류의 upper bound를 표현하면\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\bigcup_{i=1}^{m} E_{i}) &= 1-P(\\bigcap_{i=1}^{m} E_{i}^c) \\\\\n&= 1 -  \\prod_{i=1}^{m} P(E_i^c) \\\\\n&\\le 1 - (1-\\alpha)^m, \\quad P(E_i) \\le \\alpha\n\\end{aligned}\n\\end{equation*}\n\\]\n여기서 \\(\\bigcap\\)이 \\(\\prod\\)로 표현되는 이유는 가설 설정 시에 모든 가설(\\(E_i\\))은 서로 독립이기 때문이다. \\(m\\) 에 값을 대입하면서 1종 오류의 upper bound에 대해 알아보자.\n\\[\n\\begin{equation*}\n\\begin{aligned}\n&\\alpha = 0.05 \\text{일 때}, \\\\\n&m = 2,\\quad 1 - (1-\\alpha)^m=0.0975 \\\\\n&m = 3,\\quad 1 - (1-\\alpha)^m=0.1426 \\\\\n&m = 100,\\quad 1 - (1-\\alpha)^m \\backsimeq 0.9941 \\backsimeq 1 \\\\\n\\end{aligned}\n\\end{equation*}\n\\]\n\\(m\\) 이 증가할 때, 즉 가설의 개수가 많아질 때 제 1종 오류가 발생할 upper bound는 거의 1에 가깝게 되며, 이는 거의 항상 error가 발생한다고 할 수 있다. 따라서 이러한 문제를 해결하기 위해서 제 1종 오류에 대한 일종의 보정을 수행해야 하는데 대표적인 방법이 Bonferroni correction이다. Bonferroni correction을 설명하기 이전에 광의적인 개념인 FWER에 대해 집고 넘어간다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#family-wise-error-ratefwer",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#family-wise-error-ratefwer",
    "title": "multiple test",
    "section": "Family-wise error rate(FWER)",
    "text": "Family-wise error rate(FWER)\n\\(m\\) 개의 가설검정(\\(H^1, H^2,\\cdots, H^m\\))을 수행한다고 가정했을 때 결과를 table로 정리해보면 다음과 같다.\n\nResult of m multiple hypothesis\n\n\n\n\n\n\n\n\ntrue state \\ decision\n\\[\n\\text{accept }H_0\n\\]\n\\[\n\\text{reject }H_0\n\\]\n\n\n\n\n\n\\[\nH_0: \\text{true}\n\\]\n\\[\n                          U\\text{(unknown)}\n                          \\]\n\\[\n                                              V\\text{(unknown)}\n                                              \\]\n\\[\n                                                                  m_0\\text{(unknown)}\n                                                                  \\]\n\n\n\\[\nH_1: \\text{true}\n\\]\n\\[\n                          T\\text{(unknown)}\n                          \\]\n\\[\n                                              S\\text{(unknown)}\n                                              \\]\n\\[\n                                                                  m_1\\text{(unknown)}\n                                                                  \\]\n\n\n\\[\n\\text{Total}\n\\]\n\\[\n                          m-R\\text{(known)}\n                          \\]\n\\[\n                                              R\\text{(known)}\n                                              \\]\n\\[\n                                                                  m\\text{(known)}\n                                                                  \\]\n\n\n\nFWER의 의미는 the probability of at least one type 1 error, 즉 1번이라도 1종 오류가 발생할 확률을 의미한다. 이를 수식으로 표현하면 \\(P(V \\ge1)\\) 이다. FWER을 통제한다는 의미는 \\(P(V \\ge1) \\le \\alpha\\) , 즉 최소 1번이라도 제 1종 오류를 범할 확률을 \\(\\alpha\\) 이하로 조절한다는 의미이고, 이를 위해서는 개별 검정 결과에 대한 제 1종 오류를 범할 확률을 조절해야한다.\n\nBonferroni Correction\nFWER을 조절하는 대표적인 방법이 본페로니 보정방법이다. FWER은 Bonferroni inequality에 의해 다음과 같은 upper bound를 얻을 수 있다.\n\\[\nP(V \\ge 1)=P(\\bigcup_{i=1}^{m} E_{i}) \\le \\sum_{i=1}^m P(E_i) \\le \\alpha\n\\]\nBonferroni Correction은 위의 upper bound를 이용해서 개별 가설에 대한 \\(\\alpha^*\\) 값을 전체 \\(\\alpha\\) 를 개별 가설의 개수 \\(m\\)으로 나눠준다.\n\\[\\alpha^* = \\frac{\\alpha}{m}\\]\nBonferroni Correction은 \\(m\\) 이 작을 때, 비교적 잘 작동한다. \\(m\\) 이 클 경우 Bonferroni Correction은 잘 작동하지 않는다. 수식을 보면 직관적으로 알 수 있는데 \\(m\\)이 커질 경우 \\(\\alpha^*\\) 값이 매우 작아지기 때문에 개별 가설에 대해서 거의 기각하지 못하는 문제가 발생한다. 즉 Bonferroni Correction은 매우 보수적인 방법이라고 할 수 있다.\n\n\nSidak Correction\n시닥 보정 방법은 본페로니 보정 방법에서 조금 개선된 버전이다. 하지만 본페로니 보정의 문제점을 그대로 가지고 있다.\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\bigcup_{i=1}^{m} E_{i}) &= 1-P(\\bigcap_{i=1}^{m} E_{i}^c) \\\\\n&= 1 - \\prod_{i=1}^{m} P(E_i^c) \\\\\n&= 1 - \\{1-P(E_i)\\}^m \\\\\n&= 1 - (1-\\alpha^*)^m \\le \\alpha \\\\\n&\\therefore \\alpha^* = 1-(1-\\alpha)^\\frac{1}{m}\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nExample\n\nNo correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-0.05)^{20} \\\\\n&\\simeq 0.64\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nBonferroni correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-\\frac{0.05}{20})^{20} \\\\\n&\\simeq 0.0488\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nSidak correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-\\alpha^*)^{20} \\\\\n&\\simeq 0.1854\n\\end{aligned}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#false-discovery-ratefdr",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#false-discovery-ratefdr",
    "title": "multiple test",
    "section": "False Discovery Rate(FDR)",
    "text": "False Discovery Rate(FDR)\nFDR은 FWER의 단점인 가설의 수가 많아질 때 개별 가설에 대해서 거의 기각하지 못하는 단점을 보완한 방법이다.\nBenjamini and Hochberg(1995)\n\\(E[\\frac{V}{R}] \\le q, \\quad q \\text{ : correspond to level of significance}\\)\n식을 해석해보면 잘못기각되는 가설의 비율의 기댓값을 \\(q\\) 수준으로 조절하는 것이라고 할 수 있다.\nBenjamini and hochberg FDR procedure\nTo control FDR at level \\(\\delta\\)\n\nOrder the unadjusted p-values: \\(p_1 \\le p_2 \\le \\cdots \\le p_m\\)\nThen find the test with the highest rank, \\(j\\), for which the p-value, \\(p_j\\), is less than or equal to \\(\\frac{j}{m}\\cdot \\delta\\)\nDeclare the tests of rank \\(1, 2, \\cdots, j\\) as significant \\(p(j) \\le \\delta \\cdot \\frac{j}{m}\\)\n\n\nCode\n\nNo corrections\n\nset.seed(311)\nx <- c(rnorm(900), rnorm(100, mean = 3))\nplot(density(x))\n\n\n\np <- pnorm(x, lower.tail = F)\nlength(p)\n\n  [1] 1000\n\ntest <- p > 0.05\n\nsummary(test[1:900])\n\n     Mode   FALSE    TRUE \n  logical      45     855\n\nsummary(test[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      95       5\n\n\n\n\nBonferroni correction\n\nbonftest <- p > 0.00005\nsummary(bonftest[1:900])\n\n     Mode    TRUE \n  logical     900\n\nsummary(bonftest[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      20      80\n\n\n\n\nFDR\n\npsort <- sort(p)\nfdrtest <- NULL\n\nfor (i in 1:1000){\n  fdrtest <- c(fdrtest, p[i] > match(p[i],psort) * .05/1000)\n\n}\nsummary(fdrtest[1:900])\n\n     Mode   FALSE    TRUE \n  logical       2     898\n\nsummary(fdrtest[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      62      38\n\n\n\n\nFDR using R stat package\n\np.adj <- stats::p.adjust(p, method = 'BH', n = length(p))\nfdrtest2 <- p.adj > 0.05\nsummary(fdrtest2[1:900])\n\n     Mode   FALSE    TRUE \n  logical       2     898\n\nsummary(fdrtest2[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      62      38\n\nmatplot(p, p.adj)\n\n\n\n\n\n\n\n참고 자료\nhttps://www.stat.berkeley.edu/~mgoldman/Section0402.pdf\nhttps://be-favorite.tistory.com/26\nhttps://www.tandfonline.com/doi/suppl/10.1080/01621459.2020.1859379?scroll=top\nhttps://ndownloader.figstatic.com/files/25717051\nhttps://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture10.pdf"
  },
  {
    "objectID": "posts/2021-06-12-kernel-density-estimation/KDE.html",
    "href": "posts/2021-06-12-kernel-density-estimation/KDE.html",
    "title": "kernel density estimation",
    "section": "",
    "text": "확률밀도함수(pdf)는 확률변수의 분포를 나타내는 함수로 보통 확률변수가 연속형일 때를 지칭한다. 확률 밀도 함수는 두 가지 조건을 만족해야 한다.\n\n모든 실수값 x에 대해 \\(f(x)\\ge 0\\)\n\\(\\int_{-\\infty}^{\\infty} f(x) dx\\)=1\n\npdf 조건에서 알 수 있듯이 확률밀도함수는 확률이 아니며, 확률밀도함수를 적분해야만 확률이 나온다."
  },
  {
    "objectID": "posts/2021-06-12-kernel-density-estimation/KDE.html#gaussian-kernel-example",
    "href": "posts/2021-06-12-kernel-density-estimation/KDE.html#gaussian-kernel-example",
    "title": "kernel density estimation",
    "section": "Gaussian kernel example",
    "text": "Gaussian kernel example\n\nx <- c(65, 75, 67, 79, 81, 91) # observed data \ny <- 50:99  \nh <- 5.5\nn <- length(y)\n\nB <- numeric(n)\nK <- numeric(n)\n\n\n\\(x_i\\) = 65일 때\n\nfor (j in 1:n) {\n        A <- 1/(h*sqrt(2*pi))\n        B[j] <- (-0.5)*((y[j] - 65)/h)^2\n        K[j] <- A*exp(B[j])\n}\n\nplot(y, K, type = 'l', main = 'kernel at xi = 65')\n\n\n\n\n\n\n각 \\(x_i\\) 별 kernel plot\n\nm <- length(x)\n\nB <- matrix(0, nrow = n, ncol = m)\nK <- matrix(0, nrow = n, ncol = m)\n\nfor (i in 1:m) {\n        for (j in 1:n) {\n                A <- 1/(h*sqrt(2*pi)*m)\n                B[j, i] <- (-0.5)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\n\n\nplot(y, K[,1], type = 'l', main = '', xlim = c(45, 110), ylim = c(0, 0.04))\nfor (i in 2:6) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\n# 최종 kernel\nK <- round(K, digit = 7)\nd <- rowSums(K)\n\nlines(y, d, type = 'l', main = 'Kernel density')\n\n\n\n\n\n\n\\(h\\) 값에 따른 kernel의 형태 변화 (Gaussian kernel 일 때)\n\\(h\\) 값을 작게 하면 undersmooth되고, 반면에 h값을 크게 하면 oversmooth된다. 따라서 적절한 \\(h\\)를 찾는 것이 중요하다. \\(h\\) 값은 MLCV(Maximum likelihood cross validation)에 의해 추정할 수 있다. \\(MLCV_{max} = \\frac{1}{n}\\sum_{i=1}^nlog[\\sum_{j}w(\\frac{x_j-X_i}{h})]-log[(n-1)h]\\)\n\nx <- c(-0.77, -0.6, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)\ny <- seq(-4, 4, 0.1)\nn <- NROW(x)\nm <- NROW(y)\n\nB <- matrix(0, nrow = m, ncol = n)\nK <- matrix(0, nrow = m, ncol = n)\n\nh <- 0.25\n\n\nfor (i in 1:n) {\n        for (j in 1:m) {\n                A <- 1/(h*sqrt(2*pi)*n)\n                B[j, i] <- (-1/2)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\nplot(y, K[,1], type = 'l', main = 'h = 0.25', ylim = c(0, 0.55), ylab = '', xlab = '')\nfor (i in 2:10) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\nK <- round(K, digit = 7)\nd <- rowSums(K)\nlines(y, d, type = 'l', main = '')\n\n\n\nx <- c(-0.77, -0.6, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)\ny <- seq(-4, 4, 0.1)\nn <- NROW(x)\nm <- NROW(y)\n\nB <- matrix(0, nrow = m, ncol = n)\nK <- matrix(0, nrow = m, ncol = n)\n\nh <- 1\n\n\nfor (i in 1:n) {\n        for (j in 1:m) {\n                A <- 1/(h*sqrt(2*pi)*n)\n                B[j, i] <- (-1/2)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\nplot(y, K[,1], type = 'l', main = 'h = 1', ylim = c(0, 0.55), ylab = '', xlab = '')\nfor (i in 2:10) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\nK <- round(K, digit = 7)\nd <- rowSums(K)\nlines(y, d, type = 'l', main = '')"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html",
    "title": "tidymodels tutorial",
    "section": "",
    "text": "tidymodels는 R 유저라면 한번쯤 써봤을 패키지인 tidyverse와 결이 같은 패키지이다. tidyverse가 데이터 전처리, 시각화를 간결한 파이프라인으로 만들 수 있는 것과 마찬가지로, tidymodels는 데이터 모델링 관점에서의 데이터 전처리, 모델링, 시각화 등을 쉽게 할 수 있게 고안된 패키지이다. tidyverse 수준으로 굉장히 완성도가 있고, R을 좋아하는 진성 유저들이 기존에 존재하는 패키지를 tidymodels와 결합하거나 새로운 함수를 끊임없이 만들고 있는 중이다."
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#recipe",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#recipe",
    "title": "tidymodels tutorial",
    "section": "Recipe",
    "text": "Recipe\n데이터 전처리를 위한 단계를 정의하는 object이다. 특이한 점은 즉시 실행되지 않고 단계를 정의만한다는 것이다. 왜 번거롭게 recipe를 사용해야 하는지 의문이 생길 수 있다.\nRecipe의 장점은 다음과 같다.\n\nrecipe object를 여러가지 모델에 재사용 가능\nrecipe 내에 사전 정의된 함수를 이용하면 코드의 간결성 확보 가능\n\n즉, recipe를 이용해서 사전 정의된 object는 linear regression, random forest, xgboost 등등 tidymodel과 연동된 여러가지 모델에 대해 동일하게 적용할 수 있다. 또 recipe에는 생각보다 다양한 데이터 전처리 관련 함수가 있는데 이를 이용하면 기존에 각 변수별로 정의를 해주어야했던 데이터 전처리 과정을 간결하고 가독성 있는 코드로 구현이 가능하다.\n\names_rec <- \n  recipe(sale_price ~ ., data = ames_train) %>%\n  step_string2factor(all_nominal()) %>% \n  step_other(all_nominal(), threshold = 0.01) %>%\n  step_nzv(all_nominal())\n\nrecipe 내에 step_function은 다음과 같다. 이외에도 imputation 관련 다양한 함수가 존재하며, 데이터 전처리에 필요한 거의 대부분의 함수가 내장되어있다.\n\nNormalization\n\n\nstep_center(var) - 평균을 빼서 중심 이동\nstep_normalize(var) - 평균 빼고, 분산으로 나눠서 표준화\n\n\nFilters\n\n\nstep_corr(threshold = 0.9) - 상관계수 절대값이 큰 변수 제거\nstep_rm(var) - 변수 제거\nstep_zv() - 분산이 0인 변수 제거\nstep_nzv() - 분산이 거의 0인 변수 제거\n\n\nTransformations\n\n\nstep_log(var, base = exp(1) ) - 로그 변환\nstep_logit(var) - 로짓 변환\nstep_poly(var, degree = 2) - 변수에 polynomial term 추가(glm에서 poly() 와 동일, 즉 orthogonal polynomial 이용)\nstep_BoxCox() - Boxcox 변환\nstep_YeoJohnson - YeoJohnson 변환\n\n\nDiscretization\n\n\nstep_discretize(var, num_breaks = 4) - 연속형 변수 이산형으로 변환\nstep_cut() - 연속형 변수를 지정한 값을 기준으로 이산형으로 변환\n\n\ninclude_outside_range - 지정한 범위를 넘어선 값을 양끝 break에 포함시킬지 여부. default = FALSE이며 결측치 처리됨\nbreaks - 절단 기준이 되는 값\n\n\n\n\nDummy variables and encodings\n\n\nstep_date() - date 변수에서 year, month, day of week 변수를 새롭게 생성\n\nfeature = c(‘dow’, ‘month’, ‘year’) - 요일, 달, 연도 변수 추가\nabbr = T - Sunday or Sun\nlabel = Sunday or number\n\nstep_holiday() - date 변수에서 공휴일에 관한 이진변수 새롭게 생성\n\n\nholidays = c(‘LaborDay’, ‘NewYearDay’, ‘ChristmasDay’)\nholidays = timeDate::listHolidays(‘US’)\n\n\nstep_dummy() - character or factor 변수를 더미변수로 변환\n\n\none_hot = TRUE - C +1개의 더미변수 생성(one_hot = F: C-1개 더미변수 생성\n\n\nstep_other() - 범주형 변수의 level이 여러개일 때, 하위 범주를 기타로 묶음\n\n\nthreshold = 0.05 - 하위 5% 범주는 기타로 묶임\nother : 기타로 지정할 level 이름 지정\n\n\nstep_interact() - 상호작용 항 추가"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#prep",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#prep",
    "title": "tidymodels tutorial",
    "section": "Prep",
    "text": "Prep\nrecipe object를 설정한 후에 prep을 이용해서 계산을 한다.\n\names_rec_prepped <- prep(ames_rec)"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#bake",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#bake",
    "title": "tidymodels tutorial",
    "section": "Bake",
    "text": "Bake\nrecipe, prep을 거쳐서 전처리된 데이터를 output으로 내보내는 단계이다.\n데이터 전처리를 완료한 결과를 보고 싶으면 bake 함수를 이용하면 되는데 training data를 기준으로 이전에 데이터 전처리를 했기 때문에 new_data = training set을 넣고 중복 계산할 필요가 없다.\n\names_train_prepped <- bake(ames_rec_prepped, new_data = NULL)\n\ntest 데이터를 기준으로 전처리를 진행할 때 new_data = test set을 넣어주기만 하면 recipe, prep을 재지정해줄 필요 없이 곧바로 데이터 전처리가 가능하다.\n\names_test_prepped <- bake(ames_rec_prepped, ames_test)"
  },
  {
    "objectID": "posts/2021-06-12-gradient-boosting-regression/gbm.html#pseudo-code",
    "href": "posts/2021-06-12-gradient-boosting-regression/gbm.html#pseudo-code",
    "title": "gradient boosting machine tutorial",
    "section": "Pseudo code",
    "text": "Pseudo code\nInput : Data \\(\\{x_i, y_i\\}^n_{i=1}\\), and a differentiable Loss function \\(L(y_i, F(x))\\)\nStep 1 : Initialized model with a constant value: \\(F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)\\)\nStep 2 : for \\(m = 1\\) to M:\n(A) Compute \\(r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1,...n\\)\n(B) Fit a regression tree to the \\(r_{im}\\) values and create terminal regions \\(R_{im}\\), for \\(j = 1,...,J_m\\)\n(C) For \\(j = 1,...,j_m\\) compute \\(r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)\\)\n(D) Update \\(F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\)\nStep 3 : Output \\(F_M(x)\\)"
  },
  {
    "objectID": "posts/2021-06-12-gradient-boosting-regression/gbm.html#details",
    "href": "posts/2021-06-12-gradient-boosting-regression/gbm.html#details",
    "title": "gradient boosting machine tutorial",
    "section": "Details",
    "text": "Details\n\nInput : Data \\(\\{x_i, y_i\\}^n_{i=1}\\), and a differentiable Loss function \\(L(y_i, F(x))\\)\n\n미분 가능한 loss function으로 GBM에서는 L2 norm을 선택한다. 이 때 \\(\\frac{1}{2}\\)는 계산상의 편의를 위해서 scaling constant이다.\nLoss function : \\(L(y_i, F(x)) = \\frac{1}{2} \\sum_{i=1}^n (y_i -F(x))^2\\)\n\nloss <- function(y, yhat){\n                return(mean(1/2*(y-yhat)^2))\n}\n\n\nStep 1 : Initialized model with a constant value: \\(F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\\[\n\\begin{align}\nF_0(x) &= \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma) \\newline\n&={d \\over d\\gamma} \\frac{1}{2} \\sum_{i=1}^n (y_i -\\gamma^2)\\newline\n&= -\\sum_{i=1}^n(y_i - \\gamma) \\newline\n&= 0 \\newline\n&\\Leftrightarrow \\hat{\\gamma} = \\bar{y}\n\\end{align}\n\\]\n초기값은 y의 평균으로 계산한다.\n\n\nCompute \\(r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1,...n\\)\n\n\\[\n\\begin{align}\nr_{im} &= -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}, \\quad i = 1,...n, \\quad m = \\#tree \\newline\n&\\Leftrightarrow r_{im} = y_i - F_{m-1}(x_i)\n\\end{align}\n\\]\n\\(r_{im}\\)은 negative gradient or pseudo residual이라고 한다. GBM은 residual을 기반으로 regression tree를 생성하는데 이 때 이용되는 residual 값이 \\(r_{im}\\)으로 계산된 pseudo residual이다.\n\n\nnegative_residual <- function(y, yhat) {\n                return(y - yhat)\n}\n\n\n\nFit a regression tree to the \\(r_{im}\\) values and create terminal regions \\(R_{im}\\), for \\(j = 1,...,J_m\\)\n\ntree의 깊이는 보통 8~32 정도로 구성된다. full tree가 아닌 weak learner or weak tree를 만들기 때문에 tree의 terminal regions \\(R_{im}\\) 에는 여러 개의 값이 존재할 수 있다.\n\nFor \\(j = 1,...,j_m\\) compute \\(r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)\\)\n\n이 때 tree의 terminal regions \\(R_{im}\\) 에 존재하는 여러 개의 값은 \\(r_{jm}\\) : terminal region의 평균으로 계산된다.\n\n\\[\n\\begin{align}\nr_{jm} &= \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma) \\newline\n       &=\\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}}\\frac{1}{2}(y_i - (F_{m-1}(x_i)+\\gamma)) \\newline\n       &\\Leftrightarrow -\\sum_{x_i \\in R_{ij}}(y_i - F_{m-1}(x_i)-\\gamma)) = 0 \\newline\n       &\\Leftrightarrow \\hat{\\gamma} := terminal\\; region의\\;평균\n\\end{align}\n\\]\n\n\nUpdate \\(F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\)\n\n\n완성된 tree는 \\(\\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\) 로 표현되며 learning rate \\(\\nu\\)를 이용해서 예측값에 대한 개별 tree의 영향력을 조절한다. \\(\\nu\\)가 작으면 개별 tree의 영향력이 줄어들고, 계산량이 많아지지만 accuracy는 향상된다. \\(\\nu\\)가 크면 개별 tree의 영향력이 커지고, 계산량이 상대적으로 적으며, accuracy가 상대적으로 줄어든다.\n\nFull code\n구글 서치 중에 OLS 기반으로 gradient boosting 수행하는 코드를 발견했는데 gradient boosting 알고리즘을 이해하는데 많은 도움이 되었다(실제 패키지에서는 regression tree 기반으로 계산되기 때문에 theta 값은 계산되지 않는다)\n\ngrad_boost <- function(formula, data, nu = 0.01, stop, \n                       grad.fun, loss.fun, yhat.init = 0) {\n  \n  data <- as.data.frame(data)\n  formula <- terms.formula(formula)\n  X <- model.matrix(formula, data)\n  \n  y <- data[, as.character(formula)[2]] # as.character(formula)[2] : formula y~.에서 y에 해당하는 명칭\n\n  fit <- yhat.init\n  \n  u <- grad.fun(y = y, yhat = fit) # pseudo residual 계산 \n  \n  theta <- rep(0, ncol(X))\n  \n  loss <- c()\n \n  for (i in 1:stop) {\n    \n    # Design matrix를 이용한 regression, OLS 기반, Tree X \n    base_prod <- lm.fit(x = X, y = u) \n    theta_i <- coef(base_prod)\n    \n    # theta 값 업데이트 \n    theta <- theta + nu*as.vector(theta_i)\n    \n    # yhat 값 업데이트\n    fit <- fit + nu * fitted(base_prod)\n    \n    # pseudo residual 계산\n    u <- grad.fun(y = y, yhat = fit)\n    \n    # loss 값 업데이트 \n    loss <- append(loss, loss.fun(y = y, yhat = fit))\n  }  \n  names(theta) <- colnames(X)\n  return(list(theta = theta, u = u, fit = fit, loss = loss, \n              formula = formula, data = data))\n}\n\nfit <- grad_boost(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, data = iris, stop = 1000, grad.fun = negative_residual, loss.fun = loss)\nfit$theta\n\n      (Intercept)       Sepal.Width      Petal.Length       Petal.Width \n        2.1711726         0.4958675         0.8292081        -0.3151416 \nSpeciesversicolor  Speciesvirginica \n       -0.7235307        -1.0234536 \n\n\n\n\nUsing GBM package\n패키지는 Tree 기반으로 계산되므로 고정된 coefficient 결과를 산출하지 않는다.\n대신에 feature importance 값으로 변수별 상대적인 영향력을 볼 수 있다.\n\n\nn.trees : tree의 갯수(the number of gradient boosting iteration), pseudo code에서 \\(M\\)에 해당\ninteraction.depth : tree당 최대 노드의 개수, 보통 8~32\nshringkage : learning rate(\\(\\nu\\))\nn.minobsinnode : terminal nodes의 최소 관찰값의 수\nbag.fraction (Subsampling fraction) : training set을 나눌 비율. 기본적으로 stochastic gradient boosting 전략 채택. default : 0.5\ntrain.fraction : 첫 train.fraction * nrows(data) 관찰값은 gbm fitting에 사용되고 나머지는 loss function에서의 out-of-sample 추정량을 계산하는데 사용됨. default = 1\ncv.folds : cross validation fold의 개수\nverbose : 모델 진행 상황을 모니터링할건지 유무\ndistribution : 분류 문제일 경우 - bernoulli, multinomial, regression 문제일 경우 - gaussian or tdist\n\n보통 bag.fraction, train.fraction은 따로 지정하지 않음.\n\nR code\n\nlibrary(gbm)\nfit_pack <- gbm(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, \n                data = iris, \n                verbose = T, \n                shrinkage = 0.01, \n                distribution = 'gaussian')\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.6726             nan     0.0100    0.0076\n     2        0.6650             nan     0.0100    0.0081\n     3        0.6573             nan     0.0100    0.0080\n     4        0.6493             nan     0.0100    0.0075\n     5        0.6414             nan     0.0100    0.0073\n     6        0.6326             nan     0.0100    0.0068\n     7        0.6248             nan     0.0100    0.0072\n     8        0.6171             nan     0.0100    0.0067\n     9        0.6102             nan     0.0100    0.0068\n    10        0.6028             nan     0.0100    0.0070\n    20        0.5403             nan     0.0100    0.0058\n    40        0.4424             nan     0.0100    0.0039\n    60        0.3720             nan     0.0100    0.0030\n    80        0.3182             nan     0.0100    0.0016\n   100        0.2757             nan     0.0100    0.0014\n\nsummary(fit_pack)\n\n\n\n\n                      var rel.inf\nPetal.Length Petal.Length 96.0517\nPetal.Width   Petal.Width  3.9483\nSepal.Width   Sepal.Width  0.0000\nSpecies           Species  0.0000\n\n\n\npretty.gbm.tree(fit_pack, i = 1)\n\n  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n0        1  4.5500000000        1         2           3       35.36091     75\n1       -1 -0.0054100775       -1        -1          -1        0.00000     43\n2       -1  0.0084729167       -1        -1          -1        0.00000     32\n3       -1  0.0005133333       -1        -1          -1        0.00000     75\n     Prediction\n0  0.0005133333\n1 -0.0054100775\n2  0.0084729167\n3  0.0005133333\n\n\npretty.gbm.tree()를 이용하면 개별 tree를 적합할 때 진행상황을 모니터링할 수 있다. 여기서의 predict 값은 개별 tree에 대한 값이므로 pseudo residual 값에 해당한다(참고 3).\n\n\n\n참고 자료\n참고 1 : https://www.youtube.com/watch?v=2xudPOBz-vs&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6&index=2\n참고 2 : https://medium.com/@statworx_blog/coding-gradient-boosted-machines-in-100-lines-of-code-d06b1d7bc084\n참고 3 : https://stats.stackexchange.com/questions/237582/interpretation-of-gbm-single-tree-prediction-in-pretty-gbm-tree"
  },
  {
    "objectID": "posts/2021-10-02-docker/docker.html",
    "href": "posts/2021-10-02-docker/docker.html",
    "title": "Docker 소개",
    "section": "",
    "text": "docker 파일을 구울 때 사전에 패키지를 설치하고 싶으면 install.packages를 이용하면 되는데, 이 경우 너무 많은 시간이 걸린다. docker 파일을 굽고 서버에서 install.packages를 해도 마찬가지다. 해결책을 찾아보니 pre-compiled binary package를 이용하면 설치 시간을 10배 이상 단축할 수 있는 것 같다.\n잠시 운영체제에 대한 설명으로 넘어가면 linux에서는 package 관리 방법이 두가지가 있는데 source package와 binary package다. source package는 소스 코드가 있는 패키지로 컴파일 과정을 통해 binary package로 만드는 과정을 거쳐야만 시행될 수 있다. 설치할 때 컴파일 과정도 함께 진행되어야 하므로 설치 시간이 길고, 컴파일 작업 과정에서 오류가 생길 수 있다. 반면 binary package는 컴파일이 완료된 바이너리 파일이 들어있는 패키지이다. 사전에 컴파일이 되어 있으므로, 소스 패키지에 비해 설치 시간이 짧고, 오류가 발생될 가능성이 적다.\n대부분 바이너리 패키지를 이용하지만 소프트웨어를 원하는데로 수정하고 싶을 때는 소스 패키지를 이용한다. 바이너리 패키지는 설치시간이 비교적 짧지만 바이너리 패키지를 실행하기 위해서는 다른 특정 패키지가 필요할 수 있는데, 이를 패키지 의존성이라고 한다.\n패키지 의존성을 해결해주는, 즉 패키지 간의 연결관계를 파악하고 자동으로 필요한 패키지를 설치해주는 도구가 존재하는데 apt-get, apt 등이 있다.\n다시 R로 넘어가서 linux에 대한 이해를 바탕으로 유추를 해보면 docker 파일을 구울 때 install.packages는 source package에 해당하고, pre-compiled binary package는 binary package를 의미하는 것 같다. 즉 binary package의 장점을 이용해서 속도가 빨라지는 것이라고 이해하면 될 것 같다."
  },
  {
    "objectID": "posts/2021-10-02-docker/docker.html#reference",
    "href": "posts/2021-10-02-docker/docker.html#reference",
    "title": "Docker 소개",
    "section": "Reference",
    "text": "Reference\nlinux package 설명\nhttps://bradbury.tistory.com/227?category=768468\ndocker tutorial\nhttps://jsta.github.io/r-docker-tutorial/\ndocker build 명령어 요약\nhttps://blog.d0ngd0nge.xyz/docker-dockerfile-write/\nrocker에서 package 빠르게 설치해서 굽는 법\nhttps://stackoverflow.com/questions/51500385/how-to-speed-up-r-packages-installation-in-docke\nr-base vs r-apt 속도 비교\nhttps://datawookie.dev/blog/2019/01/docker-images-for-r-r-base-versus-r-apt/"
  },
  {
    "objectID": "posts/network/network.html",
    "href": "posts/network/network.html",
    "title": "networkx",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#example",
    "href": "posts/sensmakr/sensmakr.html#example",
    "title": "sensemakr",
    "section": "Example",
    "text": "Example\n\n\\(D\\) : treatment variable\n\\(X\\) : observed covariates\n\\(Z\\) : unobserved covariates\n\n\\[\n\\begin{align*}\nY = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}\n\\end{align*}\n\\]\nFrisch-waugh-Lovell theorem을 적용하면 \\(Y = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}\\)에 FWL을 적용하면\n\n\\(Y ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Y^{\\perp X}\\)\n\\(D ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(D^{\\perp X}\\)\n\\(Z ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Z^{\\perp X}\\)\n\\(Y^{\\perp X} \\sim D^{\\perp X} + Z^{\\perp X}\\) \\(\\longrightarrow\\) \\(\\hat{\\tau}, \\, \\hat{\\gamma}\\)\n\n\\(Y^{\\perp X} = \\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X}\\)\n\n\n\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    Z = rnorm(N, 1),\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\nZperpX <- lm(Z ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX, ZperpX)\n\nprint(coef(lm(YperpX ~ DperpX+ZperpX, resid_df))[c(2, 3)], digits = 2)\n\n  DperpX   ZperpX \n-0.00072  0.01351 \n\nprint(coef(lm(Y~D+X+Z, df))[c(2, 4)], digits = 2)\n\n       D        Z \n-0.00072  0.01351 \n\n\n\\[\n\\begin{align*}\nY = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}\n\\end{align*}\n\\]\n\\(Y = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}\\)에 FWL을 적용하면\n\n\\(y ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Y^{\\perp X}\\)\n\\(D ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(D^{\\perp X}\\)\n\\(Y^{\\perp X} \\sim D^{\\perp X}\\) \\(\\longrightarrow\\) \\(\\hat{\\tau}_{res}\\)\n\n\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX)\n\nprint(coef(lm(YperpX ~ DperpX, resid_df))[2], digits = 2)\n\nDperpX \n0.0078 \n\nprint(coef(lm(Y~D+X, df))[2], digits = 2)\n\n     D \n0.0078"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#the-traditional-omitted-variable-bias",
    "href": "posts/sensmakr/sensmakr.html#the-traditional-omitted-variable-bias",
    "title": "sensemakr",
    "section": "3.1 The traditional omitted variable bias",
    "text": "3.1 The traditional omitted variable bias\n\\[\n\\begin{align*}\n\\hat{\\tau}_{res} &= \\frac{cov(D^{\\perp X}, Y^{\\perp X})}{var(D^{\\perp X})} \\\\\n&= \\frac{cov(D^{\\perp X},\\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X})}{var(D^{\\perp X})} \\\\\n&= \\frac{\\hat{\\tau}cov(D^{\\perp X},D^{\\perp X}) + \\hat{\\gamma}cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})} \\\\\n&=\\hat{\\tau} + \\hat{\\gamma} \\cdot \\hat{\\delta}, \\quad \\hat{\\delta} = \\frac{cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})}, \\quad \\hat{\\gamma} = \\frac{cov(Y^{\\perp X, D}, Z^{\\perp X, D})}{var(Z^{\\perp X, D})}\n\\end{align*}\n\\]\n따라서 unobserved confounder에 의한 추정량의 bias는 다음과 같다.\n\\[\n\\begin{align*}\n\\hat{bias} = \\hat{\\tau}_{res} - \\hat{\\tau} =  \\hat{\\gamma} \\cdot \\hat{\\delta}\n\\end{align*}\n\\]\n\\(Z\\)는 unobserved confounder이므로 \\(\\hat{\\gamma}, \\, \\hat{\\delta}\\)의 부호를 알 수 없다. 따라서 unobserved confounder의 추정량에 영향을 미치는 크기를 고려해야 한다. 즉, 연구의 주요 결론에 영향을 줄 정도로 추정량을 변경하려면 unobserved confounder \\(Z\\)의 효과는 어느 정도 크기여야 하는가?\n이는 sensitivity analysis를 통해 파악할 수 있다."
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#contour-plot",
    "href": "posts/sensmakr/sensmakr.html#contour-plot",
    "title": "sensemakr",
    "section": "contour plot",
    "text": "contour plot"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#ovb-with-the-partial-r2-parameterization",
    "href": "posts/sensmakr/sensmakr.html#ovb-with-the-partial-r2-parameterization",
    "title": "sensemakr",
    "section": "OVB with the partial \\(R^2\\) parameterization",
    "text": "OVB with the partial \\(R^2\\) parameterization\n\\[\n\\begin{align*}\n\\hat{bias} &= \\hat{\\gamma} \\cdot \\hat{\\delta} \\\\\n&=\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}} \\cdot \\frac{sd(Y^{\\perp X,D})}{sd(D^{\\perp X})} \\\\\n&=\\hat{se}(\\hat{\\tau}_{res})\\cdot\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}}(df)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#robustness-value",
    "href": "posts/sensmakr/sensmakr.html#robustness-value",
    "title": "sensemakr",
    "section": "Robustness value",
    "text": "Robustness value\n\\[\n\\begin{align*}\nRV_q = \\frac{1}{2}(\\sqrt{f^4_q + 4f^2_q - f^2_q})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2021-06-26-create-logo/hexsticker.html",
    "href": "posts/2021-06-26-create-logo/hexsticker.html",
    "title": "hexSticker",
    "section": "",
    "text": "CitationBibTeX citation:@online{don2022,\n  author = {Don Don and Don Don},\n  title = {hexSticker},\n  date = {2022-06-24},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDon Don, and Don Don. 2022. “hexSticker.” June 24, 2022."
  },
  {
    "objectID": "posts/2021-06-28-install-sql-for-mac/sql.html",
    "href": "posts/2021-06-28-install-sql-for-mac/sql.html",
    "title": "install MySQL for mac",
    "section": "",
    "text": "homebrew 홈페이지 : https://brew.sh/\nterminal 창에 아래 코드를 입력하면 homebrew가 설치된다. homebrew 홈페이지에 있는 코드와 동일하다.\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n설치가 완료된 후에는 terminal 창을 끄지말고, Next step 문구에 있는 코드를 복사해서 넣어줘야 한다.\nNext step 하단에 echo 'eval $(블라블라) eval $(/opt/homebrew/bin/brew shellenv)'를 복사해서 terminal에 붙여넣기를 해준다."
  },
  {
    "objectID": "posts/2021-06-28-install-sql-for-mac/sql.html#mysql-설치",
    "href": "posts/2021-06-28-install-sql-for-mac/sql.html#mysql-설치",
    "title": "install MySQL for mac",
    "section": "mysql 설치",
    "text": "mysql 설치\n\nmysql 설치 : brew install mysql\n설치 버전 확인 : mysql -V\n서버 켜기 : mysql.server start\n초기 설정 : mysql_secure_installation (재설정하고 싶을 때도 동일하게 입력)\n\n복잡한 비밀번호 설정 : Yes or No\n익명 유저 설정 여부 : Yes or No\nlocalhost에서만 root 접속 여부 : Yes or No\ntest 데이터베이스 삭제 여부 : Yes or No\n\nmysql 접속 : mysql -u root -p\nsql 종료 : exit\n서버 종료 : mysql.server stop"
  },
  {
    "objectID": "posts/2021-06-28-install-sql-for-mac/sql.html#workbench-설치",
    "href": "posts/2021-06-28-install-sql-for-mac/sql.html#workbench-설치",
    "title": "install MySQL for mac",
    "section": "workbench 설치",
    "text": "workbench 설치\n\nbrew install --cask mysqlworkbench\nmysqlworkbench 실행\nworkbench 세팅 방법 참고 : https://dearmycode.tistory.com/15\n\nmysql.server start 하고 연결해야 잘 작동함"
  },
  {
    "objectID": "posts/2021-07-01-data-preprocessing/dp.html",
    "href": "posts/2021-07-01-data-preprocessing/dp.html",
    "title": "data preprocessing",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/2021-07-01-quarto-python-setting/dp.html",
    "href": "posts/2021-07-01-quarto-python-setting/dp.html",
    "title": "quarto vscode and python",
    "section": "",
    "text": "참고\nhttps://quarto.org/docs/get-started/hello/vscode.html\n\n\n\n\nCitationBibTeX citation:@online{don2022,\n  author = {Don Don and Don Don},\n  title = {Quarto Vscode and Python},\n  date = {2022-06-22},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDon Don, and Don Don. 2022. “Quarto Vscode and Python.”\nJune 22, 2022."
  },
  {
    "objectID": "posts/2022-07-01-python-test/python.html",
    "href": "posts/2022-07-01-python-test/python.html",
    "title": "reticulate",
    "section": "",
    "text": "reticulate 패키지는 R에서 python을 사용하고 싶을 때 이용할 수 있는 패키지이다. vscode에서 quarto를 이용할 때, 가상환경을 적절하게 세팅하면 각 qmd 파일 별로 R, python을 적절하게 사용할 수 있지만 R, python을 한 qmd에서 동시에 사용할 수는 없다. 동시에 사용하고 싶을 경우 reticulate 패키지를 이용하면 된다."
  },
  {
    "objectID": "posts/2022-07-01-python-test/python.html#reticulate",
    "href": "posts/2022-07-01-python-test/python.html#reticulate",
    "title": "reticulate",
    "section": "reticulate",
    "text": "reticulate\nreticulate를 이용할 때는 가상환경을 만들고, 해당 가상환경을 연결해서 사용하는 것이 바람직하다. 나는 pytorch-test 폴더에 가상환경 env를 만들고, reticulate 패키지와 연결할 가상환경의 path를 맞춰주었다. 이 작업을 안할 경우 error가 난다;;\n\nSys.setenv(RETICULATE_PYTHON=\"/Users/sangdon/pytorch-test/env/bin/python\")\nreticulate::use_condaenv(condaenv = 'env')"
  },
  {
    "objectID": "posts/2022-07-01-python-test/python.html#참고",
    "href": "posts/2022-07-01-python-test/python.html#참고",
    "title": "reticulate",
    "section": "참고",
    "text": "참고\nhttps://rstudio.github.io/reticulate/articles/versions.html#providing-hints-1\nhttps://stackoverflow.com/questions/59842256/specify-reticulate-python-path-in-reticulate-python-environment-variable"
  },
  {
    "objectID": "posts/2022-07-01-python-test/python.html#python-example",
    "href": "posts/2022-07-01-python-test/python.html#python-example",
    "title": "reticulate",
    "section": "python example",
    "text": "python example\n\nimport pandas as pd \nimport numpy as np \nimport torch\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom torch_geometric.data import Data\nimport torch_geometric\nfrom sklearn.datasets import load_iris\n\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\n\n\n\nG = nx.star_graph(20)\npos = nx.spring_layout(G, seed=63)  # Seed layout for reproducibility\ncolors = range(20)\noptions = {\n    \"node_color\": \"#A0CBE2\",\n    \"edge_color\": colors,\n    \"width\": 4,\n    \"edge_cmap\": plt.cm.Blues,\n    \"with_labels\": False,\n}\nnx.draw(G, pos, **options)\nplt.show()"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html",
    "title": "machine learning explainability",
    "section": "",
    "text": "머신러닝 모델을 해석하는 방법은 크게 두 가지 범주로 나뉜다. 특정 모델에 한정된 방법 (Model-specific method)과 모델에 상관없이 사용할 수 있는 방법(Model-agnostic method)이다. 각 방법론별로 대표적인 방법을 kaggle study 발표 자료를 준비하면서 정리해본다."
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#random-forest",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#random-forest",
    "title": "machine learning explainability",
    "section": "Random forest",
    "text": "Random forest\n\n\n\nhttps://towardsdatascience.com/random-forest-learning-essential-understanding-1ca856a963cb\n\n\n그림을 통해 random forest의 아이디어를 간단하게 설명하면 각 boostrap set(복원추출된 데이터) 별로 개별 tree를 하나씩 생성해서 나온 예측 결과를 aggregate해서 최종 예측 모델을 만드는 방법이다. 보통 random forest로 예측을 한 후에 feature importance plot을 통해 변수 중요도를 파악하게 된다. 변수 중요도가 어떻게 계산되는지 몰랐는데 이번 기회에 정리해본다.\n변수 중요도를 계산하는 방법은 크게 gini impurity를 이용한 방법과 permutation을 이용한 방법 2가지가 있다. 먼저 gini impurity를 이용한 방법부터 살펴보자."
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#gini-importance-in-random-forest",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#gini-importance-in-random-forest",
    "title": "machine learning explainability",
    "section": "Gini importance in random forest",
    "text": "Gini importance in random forest\n먼저 single tree에 대해 gini importance가 어떻게 계산되는지 보자.\nSingle tree 일 때 $$ \\[\\begin{align*}\n&VI_i = \\sum_{n \\in t, i_n = i} p(n) \\boldsymbol{\\Delta}_{gini}(n), \\\\\n&i_n : \\text{Sum over all nodes of the tree that use feature i} \\\\\n&p(n) : \\text{Probability of using that node for a sample data point} \\\\\n&\\boldsymbol{\\Delta}_{gini}(n) : \\text{Change in gini impurity at that node}\n\\end{align*}\\]\n$$\n각 feature별 gini importance 값은 해당 feature 노드의 gini impurity 값의 변화율을 이용해서 계산된다.\n밑에 예시를 통해 \\(X_1\\), \\(X_2\\) 변수의 gini importance가 어떻게 계산되는지 알아보자.\n\n\n\nhttps://www.youtube.com/watch?v=qC3PRqHqnfE\n\n\n각 노드별로 gini impurity 값이 산출되었다고 했을 때, feature importance를 구하기 위해서는 \\(\\boldsymbol{\\Delta_{gini}}\\)의 값을 구해야한다.\n\\(\\boldsymbol{\\Delta_{gini}}\\)를 어떻게 구하는지를 \\(X_1\\) 변수를 기준으로 보면 기준노드의 gini impurity값과 split된 노드의 gini impurity값의 차이를 통해 계산된다. 정확히는 split된 노드에 해당하는 데이터의 비율을 gini impurity에 곱한 가중합과 기준 노드의 gini impurity 값의 차이를 통해 계산된다. \\(X_1\\)의 \\(\\boldsymbol{\\Delta_{gini}}\\)는 0.26으로 계산됨을 아래 이미지에서 확인할 수 있다.\n이렇게 기준 변수에 해당하는 노드의 \\(\\boldsymbol{\\Delta_{gini}}\\)를 구했으면 마지막으로 \\(p(n)\\) 즉, 해당 노드의 데이터의 비율을 가중합하면 개별 변수의 변수 중요도가 산출된다.\n정리하면 tree에서 가지를 뻗어나갈 때 해당 변수의 gini impurity가 얼마나 개선되었는지를 이용해서 변수 중요도를 산출한다고 볼 수 있다.\n이제 Random forest일 때로 돌아가보자.\nRandom forest일 때\nRandom forest일 때는 모델 특성상 각 부스트랩 샘플당 tree가 하나씩 나오게 되는데 각 tree에서 구한 \\(VI_i\\) 값을 평균내주면 된다.\n$$\n\\[\\begin{align*}\nVI_i=\\frac{1}{|B|} \\sum_{t \\in B} VI_i(t)\n\\end{align*}\\]\n$$"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#purmutation-importance-in-random-forest",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#purmutation-importance-in-random-forest",
    "title": "machine learning explainability",
    "section": "Purmutation importance in random forest",
    "text": "Purmutation importance in random forest\n위에서 설명한 gini importance는 gini impurity의 변화율을 활용해서 feature importance를 산출하는 방법이다. purmutation importance도 위의 gini importance와 방식은 동일하지만 gini 값이 아닌 다른 지표를 활용한다는 것에서만 차이가 있다.\n먼저 알고리즘에 대해 살펴보자(분류 문제일 때)\n$$\n\\[\\begin{align*}\n\\text{within each tree } t \\\\\n&VI^{(t)}(x_j) = \\frac{\\sum_{i=\\bar B^{(t)}} I(y_i = \\hat{y}_i^{(t)})}{|\\bar B^{(t)}|} - \\frac{\\sum_{i=\\bar B^{(t)}} I(y_i = \\hat{y}_{i, \\pi_j}^{(t)})}{|\\bar B^{(t)}|}  \\\\\n&\\hat{y}_i^{(t)} = f^{(t)}(x_i) : \\text{predicted class before permuting} \\\\\n&\\hat{y}_{i, \\pi_j}^{(t)} = f^{(t)}(x_{i, \\pi_j}) : \\text{predicted class after permuting }X_j \\\\\n&X_{i, \\pi_j} = (x_{i,1}, \\cdots, x_{i, j-1}, x_{\\pi_j(i), j}, x_{i, j+1}, \\cdots , x_{i, p}) \\\\\n& \\bar B^{(t)} = \\text{out of bag sample} \\\\\n\\text{over all trees:} \\\\\n&\\text{1. row importance : } VI(x_j) = \\frac{\\sum_{t = 1}^{ntree}VI^{(t)}(x_j)}{ntree} \\\\\n&\\text{2. scale importance : } sVI(x_j) = \\frac{VI(x_j)}{\\frac{\\hat{\\sigma}}{\\sqrt ntree}} \\\\\n\\end{align*}\\]\n$$ 수식을 보면 개별 tree에 대해서 \\(t=1\\)일 때 \\(\\sum_{i = B} I(y_i = \\hat{y_i})\\)와 \\(\\sum_{i = B} I(y_i = \\hat{y_i, \\pi_j})\\) 의 차이를 통해 feature importance 값이 계산되고, 각 부스트랩 샘플에 해당하는 tree당 계산된 feature importance 값을 평균내주면 random forest에서의 feature importance 값이 계산된다. 여기서 purmutation 개념이 나오는데 아이디어는 간단하다. feature importance를 구하고자 하는 target variable만 다른 변수의 값은 고정한 채로 행 단위로 shuffling을 진행한다. 이렇게 무작위로 shuffling 했을 때의 예측값과 shuffling하지 않았을 때의 예측값의 차이를 이용해서 변수 중요도를 산출한다(permutation의 아이디어는 밑에 더 설명한다).\n참고\nhttps://www.zeileis.org/papers/Lifestat-2008.pdf\nR code\nset_engine에 옵션으로 permutation or impurity을 지정해주면 된다.\n\ndata(\"ames\")\names_train <- ames %>%\n    transmute(Sale_Price = log10(Sale_Price),\n              Gr_Liv_Area = as.numeric(Gr_Liv_Area),\n              Year_Built, Bldg_Type)\n\nrf_model <- \n    rand_forest(trees = 1000) %>% \n    set_engine(\"ranger\", importance = 'impurity') %>% \n    set_mode(\"regression\")\n\nrf_wflow <- \n    workflow() %>% \n    add_formula(\n        Sale_Price ~ Gr_Liv_Area + Year_Built + Bldg_Type) %>% \n    add_model(rf_model) \n\nrf_fit <- rf_wflow %>% fit(data = ames_train)\n\nrf_fit %>% \n  extract_fit_parsnip()\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      2930 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       0.008627292 \nR squared (OOB):                  0.7246625 \n\n\nGini importance plot\n\nrf_fit %>% \n  extract_fit_parsnip() %>% \n  vip(geom = 'point')"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#permutation-importance",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#permutation-importance",
    "title": "machine learning explainability",
    "section": "Permutation importance",
    "text": "Permutation importance\npermutation importance는 모델을 학습시킨 이후(post-hoc) 특정 변수의 관측치를 shuffle했을 때의 예측력을 비교해서 feature importance를 계산하는 방법이다(random forest의 permutation importance도 aggregation이 추가된 것 외에 동일하다).\n알고리즘 특성상 특정 모델에 국한된 방법이 아니라 어떤 모델이든 적용할 수 있는 방법이다. permutation importance의 기본 아이디어에 대해 살펴보자.\nExample\n\n\n\nhttps://www.kaggle.com/dansbecker/permutation-importance\n\n\n사람의 10살 때의 정보를 이용해서 10년 후 20살 때의 키를 예측하려고 한다.\n직관적으로 변수 중 10살 때의 키에 관한 변수는 20살 때의 키를 예측하는데 중요한 변수이고, 10살 때 갖고 있는 양말의 수는 20살 때의 키를 예측하는데 중요한 변수가 아니다.\n이러한 직관에서 출발하면 다음과 같은 질문을 해볼 수 있다.\nvalidation set에서 특정 변수의 관측치를 shuffle하고, 나머지 변수를 고정시키면 예측 정확도에 어떤 영향을 미칠까?\n특정 한 변수의 관측치만 행방향으로 무작위로 섞기 때문에 당연히 모델의 예측력은 감소하게 될 것이다. 다만 변수별로 정도의 차이가 있을 수 있다. 즉, 위의 예시로 보면 10살 때의 키에 관한 변수를 shuffle 했을 때 모델의 예측력은 많이 떨어지지만, 10살 때 갖고 있는 양말의 수에 관한 변수를 shuffle 했을 때는 모델의 예측력에 큰 차이가 없을 수 있다. 이러한 직관이 purmutaion importance의 아이디어이다.\nProcess\n\n학습이 끝난 모델 세팅\n한 변수의 관측치를 shuffling한 데이터를 이용해서 동일하게 예측을 진행\n예측치와 실제값의 차이인 손실함수를 이용해서 shuffling 후에 얼마나 loss가 커졌는지 계산\nloss의 증감을 이용해서 feature importance를 계산\n모든 변수에 대해 반복\n\n장점\n\n계산 속도가 빠름\n\n특정 변수를 제거하고 재학습을 시키는 것이 아니라 특정 변수 하나를 permutation하는 것이므로 상대적으로 계산량 감소\n\n사용 범위가 넓고, 직관적임\n상대적으로 일관된 feature importance를 측정할 수 있음\n\n단점\n\n무작위로 shuffling 하다보면 비현실적인 값이 나올 수도 있음\nweight의 구간을 이용한 해석 필요\n\nR code\n\nsvm_spec <- svm_poly(degree = 1, cost = 1/4) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"regression\")\n\nsvm_wflow <- \n    workflow() %>% \n    add_formula(\n        Sale_Price ~ Gr_Liv_Area + Year_Built + Bldg_Type) %>% \n    add_model(svm_spec)\n\nsvm_fit <- svm_wflow %>% fit(data = ames_train)\n\nsvm_fit %>% \n  extract_fit_parsnip()\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: eps-svr  (regression) \n parameter : epsilon = 0.1  cost C = 0.25 \n\nPolynomial kernel function. \n Hyperparameters : degree =  1  scale =  1  offset =  1 \n\nNumber of Support Vectors : 2344 \n\nObjective Function Value : -204.8838 \nTraining error : 0.278962 \n\n\npermutation importance plot\ntidymodels는 vip 패키지와 연동해서 feature importance plot을 그릴 수 있다. method = “permute”를 지정해주는 것이 중요하다.\n\nsvm_fit %>%\n  extract_fit_parsnip() %>%\n  vip(method = \"permute\", \n      target = \"Sale_Price\", metric = \"rsquared\",\n      pred_wrapper = kernlab::predict, train = ames_train)\n\n\n\n\npermutation을 여러번 반복해서 평균을 낼 수도 있다.\n\nsvm_fit %>%\n  extract_fit_parsnip() %>%\n  vip(method = \"permute\", \n      target = \"Sale_Price\", metric = \"rsquared\", nsim = 20, all_permutations = TRUE, \n      geom = \"boxplot\",\n      mapping = aes_string(fill = \"Variable\"),\n      pred_wrapper = kernlab::predict, train = ames_train)\n\n\n\n\n참고 https://koalaverse.github.io/vip/articles/vip.html"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#partial-dependence-plot",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#partial-dependence-plot",
    "title": "machine learning explainability",
    "section": "Partial dependence plot",
    "text": "Partial dependence plot\nfeature importance는 어떤 변수가 예측에 큰 영향을 미쳤는지를 보여준다. 반면에 partial dependence plot은 변수가 target variable에 어떤 영향을 미쳤는지를 보여준다.\npartial dependence plot은 pumutation importance와 마찬가지로 모델을 학습한 후(post-hoc) 계산된다.\n먼저 예제를 살펴보자.\nExample\n\n모든 주택에 관한 변수를 통제할 때, 위·경도는 주택가격에 어떤 영향을 미치는가?\n두 그룹의 건강에 대한 예측 결과의 차이는 식단 때문인가? 아니면 다른 요인 때문인가?\n\n이런 질문에 간접적인 답을 할 수 있는 방법이 partial dependence plot이다. 즉 개별 변수별로 그래프를 그려서 target variable에 어떤 영향을 미쳤는지 설명할 수 있다.\nProcess\n$$\n\\[\\begin{align*}\n\\hat{f}_{x_s}(x_s) = E_{x_c}[\\hat{f}(x_s, x_c)] &= \\int \\hat{f}(x_s, x_c)dP(x_c) \\\\\n&\\approx \\frac{1}{n} \\sum_{i=1}^n \\hat{f}(x_s, x_c^{(i)})\\quad  \\text{using Monte Carlo}\\\\\n&\\hat{f} : \\text{학습이 완료된 임의의 모델} \\\\\n& x_s : \\text{plot하고자 하는 변수} \\\\\n& x_c : x_s\\text{ 외 나머지 변수}\n\\end{align*}\\]\n$$\n\\(\\hat{f}\\)를 안다고 할 때, \\(x_{c}^{(i)}\\) 는 데이터셋으로 부터 주어진 값이므로 \\(x_s\\)에 값을 넣어서 값을 얻을 수 있다. \\(\\hat{f}\\)는 학습이 완료된 모델이지만 적분이 불가능하므로 monte carlo integration을 이용해서 근사적으로 계산한다.\n즉, 관심변수 외에 다른 변수들의 값이 고정되어 있을 때 관심 변수 값에 따라 모델의 예측값이 어떻게 변화하는지를 보는 것이다.\npartial dependence plot은 회귀분석에서 회귀계수를 해석하는 방식과 동일하며, 관심 변수와 관심변수 외의 변수들 간의 독립을 가정하는 것 또한 동일하다.\n장점\n\n해석이 직관적임\n상대적으로 구현하기 쉬움\n\n단점\n\n2차원으로 표현되기 때문에 변수 2개에 대해서만 해석 가능\n계산량이 많음\n독립성 가정\n\nR code\n\ndata(ames)\names_train <- ames %>%\n    transmute(Sale_Price = log10(Sale_Price),\n              Gr_Liv_Area = as.numeric(Gr_Liv_Area), \n              Year_Built, Bldg_Type)\n\nrf_model <- \n    rand_forest(trees = 1000) %>% \n    set_engine(\"ranger\") %>% \n    set_mode(\"regression\")\n\nrf_wflow <- \n    workflow() %>% \n    add_formula(\n        Sale_Price ~ Gr_Liv_Area + Year_Built + Bldg_Type) %>% \n    add_model(rf_model) \n\nrf_fit <- rf_wflow %>% fit(data = ames_train)\n\nPartial Dependence Plot DALEXtra 패키지를 이용해서 partial dependence plot을 그릴 수 있다. model_profile 함수 안에 variable = ““에 그리고 싶은 변수를 지정하면 된다. group별 plot을 보고 싶으면 group 옵션을 선택할 수 있다.\n\nlibrary(DALEXtra)\n\nexplainer_rf <- explain_tidymodels(\n    rf_fit, \n    data = dplyr::select(ames_train, -Sale_Price), \n    y = ames_train$Sale_Price,\n    label = \"random forest\"\n)\n\nPreparation of a new explainer is initiated\n  -> model label       :  random forest \n  -> data              :  2930  rows  3  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  2930  values \n  -> predict function  :  yhat.workflow  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package tidymodels , ver. 0.2.0 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  4.902419 , mean =  5.220532 , max =  5.508208  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.7984955 , mean =  0.0001081782 , max =  0.373146  \n  A new explainer has been created!  \n\npdp_rf <- model_profile(explainer_rf, N = NULL, \n                        variables = \"Gr_Liv_Area\", groups = \"Bldg_Type\")\n\nplot(pdp_rf)"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#shapley-value",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#shapley-value",
    "title": "machine learning explainability",
    "section": "Shapley value",
    "text": "Shapley value\nShapley value는 Lloyd Shapley가 정의한 협동적 게임이론을 근거로 한 방법론이다. 협동적 게임이론은 몰라도 되는 것 같다…\n\n\n\nhttps://www.youtube.com/watch?v=9haIOplEIGM&t=152s\n\n\n그림을 통해 보면 4명이 팀을 이뤄서 kaggle competition에 우승을 했다고 해보자. 우승 상금을 분배해야하는데 어떻게 하면 공정하게 분배할 수 있을까?\n가장 공평한 방법은 개별 팀원의 기여도를 통해서 우승에 크게 기여한 사람에게 큰 금액을, 우승에 작게 기여한 사람에게 적은 금액을 부여하는 것이다. 여기서 한 가지 문제는 기여도를 어떻게 측정할 것인지에 대한 측도가 필요하다. 가장 쉬운 방법은 한명이 빠졌을 때의 상금과 4명이 우승했을 때의 상금을 비교하는 것이다.\n그림으로 예를 들면 파란색 팀원은 도메인 전문가로 팀에 상당한 기여를 했다고 가정해보자. 파란색 팀원이 빠졌을 경우 팀은 3등으로 순위가 떨어져 상금을 3000 달러 밖에 수령하지 못한다고 가정해보면 파란색 팀원은 대략 7000달러의 기여도를 갖고 있다고 볼 수 있다.\n이렇게 개별 팀원이 빠졌을 때와 빠지지 않았을 때를 비교해서 쉽게 기여도를 계산할 수 있지만, 문제점은 개별 팀원 사이에 관계를 고려하지 못하는 것이다.\nshapley value는 이러한 문제를 극복하기 위해서 개별 팀원이 빠졌을 때와 빠지지 않았을 때의 모든 경우의 수를 고려해서 개별 팀원의 기여도를 계산하는 방식을 채택한다.\n\n\n\nhttps://www.youtube.com/watch?v=9haIOplEIGM&t=152s\n\n\n머신러닝 모델로 일반화를 하면 팀 내의 개별 팀원은 feature로 볼 수 있고, Game은 Black box 모델, Payout은 prediction 값으로 볼 수 있다. 정리하면 shapley value는 각 feature가 예측에 얼마나 기여를 했는지 측정하는 측도로 볼 수 있고, 이 측도를 구하기 위해서 모든 하위 부분집합의 값을 계산하는 방법을 채택한다. 이를 marginal contribution 이라고 하며, 각 feature 별로 계산된 marginal contribution을 평균내서 최종 shapley value 값을 산출한다.\nprocess\n$$\n\\[\\begin{align*}\n\\phi_j(val) &= \\sum_{S \\subseteq\\{x_1, \\cdots, x_p\\}\\setminus \\{x_j\\}} \\frac{|S|!(P-|S|-1)!}{p!}(val(S \\cup\\{x_j\\})-val(S)) \\\\\n&S : \\text{subset of the features} \\\\\n&p : \\text{the number of features} \\\\\n&x : \\text{the vector of feature values of instance to be explained} \\\\\n&val : \\text{prediction for feature values}\n\\end{align*}\\]\n$$\nshapley value를 구하는 순서는 간략하게 다음과 같다.\n\n기준변수를 제외했을 때와 제외하지 않았을 때 예측값의 차이를 계산\n가중치 계산\n가중합 계산\n\nExample\n위의 kaggle competition 예제로 다시 돌아가서 kaggle competition에 참여한 3명의 player가 있다고 가정해보자. 여기서 각 player에 대한 상금의 기여분에 대한 경우의 수는 다음과 같다.\n$$\n\\[\\begin{align*}\n&val(1) = 100, \\, val(2) = 125, \\, val(3) = 50, \\\\\n&val(1, 2) = 270, \\, val(1, 3) = 375, \\, val(2, 3) = 350, \\\\\n&\\, val(1, 2, 3) = 500\n\\end{align*}\\]\n$$\n즉, 3명의 player가 모두 참여했을 때는 500$를 수령할 수 있지만, 1번 player만 참여했을 때는 100$, 2번 player만 참여했을 때는 125$, 1번과 2번 player가 동시에 참여했을 때는 270$ .. etc 으로 해석할 수 있다(실제 ML model에서 val은 prediction value이므로 주어진 값이다).\n이제 각 player별 marginal contribution을 계산할 수 있는데 아래 table과 같이 계산된다.\n\nhttp://faculty.econ.ucdavis.edu/faculty/bonanno/teaching/122/Shapley.pdf\n\n\nProbability\ncombination\n1’s marginal contribution\n2’s marginal contribution\n3’s marginal contribution\n\n\n\n\n\\[\n\\frac{1}{6}\n\\]\nfirst 1, then2, then3:\n\\[\n               123\n               \\]\n\\[\n                             val(1) = 100\n                             \\]\n\\[\n                                                         val(1,2)-val(1) = 170\n                                                         \\]\n\\[\n                                                                                     val(1,2,3)-val(1,2) = 230\n                                                                                     \\]\n\n\n\\[\n\\frac{1}{6}\n\\]\n\\[\n               132\n               \\]\n\\[\n                             val(1)=100\n                             \\]\n\\[\n                                                         val(1,2,3)=val(1,3)=125\n                                                         \\]\n\\[\n                                                                                     val(1,3)-val(1)=275\n                                                                                     \\]\n\n\n\\[\n\\frac{1}{6}\n\\]\n\\[\n               213\n               \\]\n\\[\nval(1,2)-val(2)=145\n\\]\n\\[\nval(2)=125\n\\]\n\\[\nval(1,2,3)-val(1,2)=230\n\\]\n\n\n\\[\n\\frac{1}{6}\n\\]\n\\[\n               231\n               \\]\n\\[\nval(1,2,3)-val(2,3)=150\n\\]\n\\[\nval(2)=125\n\\]\n\\[\nval(2,3)-val(2)=225\n\\]\n\n\n\\[\n\\frac{1}{6}\n\\]\n\\[\n               312\n               \\]\n\\[\nval(1,3)-val(3)=375\n\\]\n\\[\nval(1,2,3)-val(1,3)=125\n\\]\n\\[\nval(3)=50\n\\]\n\n\n\\[\n\\frac{1}{6}\n\\]\n\\[\n               321\n               \\]\n\\[\nval(1,2,3)-val(2,3)=150\n\\]\n\\[\nval(2,3)-val(3)=300\n\\]\n\\[\nval(3)=50\n\\]\n\n\n\n각 변수별 expected marginal contribution, 즉 shapley value를 구해보면 다음과 같다.\n\\(\\phi_1(val)\\)(1’s expected marginal contribution) : \\(\\frac{1}{6}(100+100+145+150+325+150)=\\frac{970}{6}\\)\n\\(\\phi_2(val)\\)(2’s expected marginal contribution) : \\(\\frac{1}{6}(170+125+125+125+125+300)=\\frac{970}{6}\\)\n\\(\\phi_3(val)\\)(3’s expected marginal contribution) : \\(\\frac{1}{6}(230+275+230+225+50+50)=\\frac{1060}{6}\\)\n1’s, 2’s, 3’s expected marginal contribution을 모두 합하면 500이 되고, 이 값은 \\(val(1,2,3)\\) 값과 같다.\n\\(\\frac{970+970+1060}{6}=\\frac{3000}{6} = 500=val(1,2,3)\\)\nR code\npackage: fastshap, DALEX, iml, shapper등이 있음"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#dalex-패키지를-이용해서-shap을-그리는-방법",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#dalex-패키지를-이용해서-shap을-그리는-방법",
    "title": "machine learning explainability",
    "section": "DALEX 패키지를 이용해서 SHAP을 그리는 방법",
    "text": "DALEX 패키지를 이용해서 SHAP을 그리는 방법\n\ntidymodels workflow를 동일하게 수행\nfit함수의 결과를 변수에 저장\n\n\nlibrary(DALEX)\ntitanic %>% is.na() %>% colSums()\n\n  gender      age    class embarked  country     fare    sibsp    parch \n       0        2        0        0       81       26       10       10 \nsurvived \n       0 \n\ntitanic_rec <- titanic %>% \n  recipe(survived ~ class + gender + age + \n                sibsp + parch + fare + embarked) %>% \n  step_impute_bag(age, fare, sibsp, parch)\n  \nrf_model <- \n    rand_forest(trees = 1000) %>% \n    set_engine(\"ranger\") %>% \n    set_mode(\"classification\")\n\nrf_wflow <- \n  workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(titanic_rec)\n  \n\nrf_fit <- rf_wflow %>% fit(data = titanic %>% select(-country))\nrf_ex_fit <- rf_fit %>% extract_fit_parsnip()\n\n\nexplain()\n\nmodel : fitting 완료된 모델\ndata : target 변수 제외한 데이터프레임\ny : target 변수\n\n다른 라이브러리를 이용해서 모델을 fitting 했을 때 구조가 서로 다를 수 있으므로 explain 함수를 이용해서 균일한 인터페이스를 갖는 객체로 만들어줌\nshapley value가 계산되는 것은 아님\n\n\nlibrary(DALEX)\ntitanic_rf_exp <-DALEX::explain(model = rf_fit, \n                       data = titanic %>% select(-country),\n                       y = titanic_imputed$survived == \"yes\", \n                       label = \"Random Forest\")\n\nPreparation of a new explainer is initiated\n  -> model label       :  Random Forest \n  -> data              :  2207  rows  8  cols \n  -> target variable   :  2207  values \n  -> predict function  :  yhat.workflow  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package tidymodels , ver. 0.2.0 , task classification (  default  ) \n  -> model_info        :  Model info detected classification task but 'y' is a logical . Converted to numeric.  (  NOTE  )\n  -> predicted values  :  numerical, min =  0.01157508 , mean =  0.3223648 , max =  0.9899299  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.9899299 , mean =  -0.3223648 , max =  -0.01157508  \n  A new explainer has been created!  \n\nhenry <- archivist::aread(\"pbiecek/models/a6538\") \nhenry\n\n  class gender age sibsp parch fare  embarked\n1   1st   male  47     0     0   25 Cherbourg\n\npredict(titanic_rf_exp, henry)\n\n      yes \n0.3115554 \n\n\nshapley value 계산\n\npredict_parts()\n\nexplainer : explain 함수를 적용한 모델\nnew_observation : obs\ntype : shap, oscillations, oscillations_uni, oscillations_emp, break_down or break_down_interactions 등의 다양한 옵션이 있음\nB : 25(default), shapley value 계산할 때 설명변수의 무작위 순서의 조합 개수\n\n\n\nshap_henry <- predict_parts(explainer = titanic_rf_exp,\n                      new_observation = henry, \n                                 type = \"shap\",\n                                    B = 25)\nshap_henry\n\n                                             min          q1       median\nRandom Forest: age = 47             -0.107769986 -0.08225301 -0.052534709\nRandom Forest: class = 1st           0.120858355  0.13847492  0.146725273\nRandom Forest: embarked = Cherbourg  0.006208021  0.02192237  0.026240676\nRandom Forest: fare = 25            -0.033254691 -0.02148800 -0.020869511\nRandom Forest: gender = male        -0.122478130 -0.11026003 -0.103272577\nRandom Forest: parch = 0            -0.018288814 -0.01108137 -0.002063531\nRandom Forest: sibsp = 0            -0.022942049 -0.00837284 -0.003404242\n                                            mean           q3          max\nRandom Forest: age = 47             -0.060144588 -0.036412284 -0.027904349\nRandom Forest: class = 1st           0.147761772  0.152505981  0.188276846\nRandom Forest: embarked = Cherbourg  0.030566624  0.034904153  0.070796838\nRandom Forest: fare = 25            -0.012288680 -0.009024606  0.022963328\nRandom Forest: gender = male        -0.106946452 -0.102185942 -0.093570494\nRandom Forest: parch = 0            -0.005774083 -0.001606197  0.001302964\nRandom Forest: sibsp = 0            -0.003983968  0.005573644  0.008947406\n\n\n\nplot(shap_henry)\n\n\n\n\n\nhenry의 나이는 47세로 shapley value 값이 음수로 표시되고, age는 생존에 부정적인 기여를 했다고 해석할 수 있음\nhenry의 class는 1등석으로 shapley value 값이 양수로 표시되고, class는 생존에 매우 긍정적인 기여를 했다고 해석할 수 있음"
  },
  {
    "objectID": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#fastshap-패키지를-이용한-방법",
    "href": "posts/2021-07-29-machine-learning-explainability/ML_explain.html#fastshap-패키지를-이용한-방법",
    "title": "machine learning explainability",
    "section": "fastshap 패키지를 이용한 방법",
    "text": "fastshap 패키지를 이용한 방법\n\nfastshap::explain()\n\nmodel : extract_fit_parsnip() 적용한 모델\nX : data\npred_wrapper : 예측값, classification의 경우 기준 범주의 확률\n\nxgb, lm은 적용안하고 exact = T만 해줘도 됨\n나머지 모델은 pred_wrapper을 넣어줘야함\n\nexact = T : 근사값이 아닌 정확한 shapley value 계산\n\n\nShapley feature importance\n\ndata(\"titanic_imputed\")  \n\ntitanic_rec <- titanic_imputed %>% \n  recipe(survived ~ .) %>% \n  step_mutate(survived = as.factor(survived))\n  \nrf_model <- \n    rand_forest(trees = 1000) %>% \n    set_engine(\"ranger\") %>% \n    set_mode(\"classification\")\n\nrf_wflow <- \n  workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(titanic_rec)\n  \n\nrf_fit <- rf_wflow %>% fit(data = titanic_imputed)\nrf_ex_fit <- rf_fit %>% extract_fit_parsnip()\n\n\nshap <- explain(xgb_ex_fit$fit, X = X, exact = T)\nxgboost, lm만 exact = T 가능, 나머지 불가능(공식 문서 참고)\n\n\nlibrary(fastshap)\n\n\n다음의 패키지를 부착합니다: 'fastshap'\n\n\nThe following object is masked from 'package:DALEX':\n\n    explain\n\n\nThe following object is masked from 'package:vip':\n\n    gen_friedman\n\n\nThe following object is masked from 'package:dplyr':\n\n    explain\n\npredict(rf_ex_fit$fit, titanic_imputed)\n\nRanger prediction\n\nType:                             Probability estimation \nSample size:                      2207 \nNumber of independent variables:  7 \n\npredict(rf_ex_fit$fit, titanic_imputed)$predictions %>% head()\n\n             0         1\n[1,] 0.8978185 0.1021815\n[2,] 0.7418424 0.2581576\n[3,] 0.8669889 0.1330111\n[4,] 0.4268959 0.5731041\n[5,] 0.3037664 0.6962336\n[6,] 0.7617627 0.2382373\n\npred_fun <- function(X.model, newdata) {\n  predict(X.model, newdata)$predictions[,2]\n}\n\nshap <- explain(rf_ex_fit$fit, X = titanic_imputed, pred_wrapper = pred_fun, nsim = 20)\n\nautoplot(shap)\n\n\n\n\nShapley dependence plot\n\npred_fun <- function(X.model, newdata) {\n  predict(X.model, newdata)$predictions[,2]\n}\n\nautoplot(shap, type = \"dependence\", feature = \"fare\", X = titanic_imputed, smooth = TRUE)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\ncontribution plot\n\nautoplot(shap, type = \"contribution\", row_num = 1)\n\n\n\n\n\nforce_plot(shap[1,], baseline = mean(pred_fun(rf_ex_fit, titanic_imputed)), feature_values = titanic_imputed[1,], display = \"html\")\n\nError: The Python package \"shap\" is needed for this function to work. Please install it; visit https://github.com/slundberg/shap for details.\n\n\nhttp://xai-tools.drwhy.ai/fastshap.html\n참고자료\nhttps://www.kaggle.com/dansbecker/use-cases-for-model-insights\nhttps://www.kaggle.com/dansbecker/permutation-importance\nhttps://www.kaggle.com/dansbecker/partial-plots?scriptVersionId=64768853&cellId=1\nhttps://christophm.github.io/interpretable-ml-book/pdp.html\nhttps://juliasilge.com/blog/mario-kart/\nhttps://www.hfshr.xyz/posts/2020-06-07-variable-importance-with-fastshap/#ref-R-vip\nhttps://stackoverflow.com/questions/67634344/r-partial-dependence-plots-from-workflow\nhttps://cran.r-project.org/web/packages/fastshap/fastshap.pdf\nhttps://www.youtube.com/watch?v=lIT5-piVtRw\nhttps://www.hfshr.xyz/posts/2020-06-07-variable-importance-with-fastshap/\nhttps://bradleyboehmke.github.io/HOML/iml.html\nhttps://ema.drwhy.ai/shapley.html#SHAPRcode"
  },
  {
    "objectID": "posts/2022-07-10-dbplyr/RMySQL.html",
    "href": "posts/2022-07-10-dbplyr/RMySQL.html",
    "title": "R과 MySQL 연동하기",
    "section": "",
    "text": "library(DBI)\nlibrary(RMySQL)\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nDBI 패키지는 R과 데이터베이스간 연결을 위한 패키지이고, RMySQL은 데이터베이스를 연결할 때, MySQL을 사용하도록 선언?하는 패키지이다.\n\nDBI::dbConnect(drv, \n              dname, \n              user, \n              password, \n              host, ...)\n\n\ndrv : DBIDriver 종류\ndbname : MySQL에 저장되어있는 DB 이름\nuser : “root” (default)\npassword : 비밀번호\nhost: localhost(로컬에서 작업할 경우)\n\n혼공 SQL책에 나와있는 market_db DB를 불러왔다.\n\n\n\n\ndrv <- dbDriver(\"MySQL\")\n\ncon = dbConnect(drv, \n                dbname = \"market_db\", \n                user = \"root\", \n                password = pat, \n                host = \"localhost\")\n\nmarket_db 안에 있는 buy table을 불러온 결과는 다음과 같다.\n\nmydata = dbGetQuery(con, \"select * from buy\")\nmydata\n\n   num mem_id prod_name group_name price amount\n1    1    BLK        ??       <NA>    30      2\n2    2    BLK      ????        ???  1000      1\n3    3    APN       ???        ???   200      1\n4    4    MMU       ???        ???   200      5\n5    5    BLK       ???         ??    50      3\n6    6    MMU       ???        ???    80     10\n7    7    GRL     ??SQL         ??    15      5\n8    8    APN     ??SQL         ??    15      2\n9    9    APN       ???         ??    50      1\n10  10    MMU        ??       <NA>    30      1\n11  11    APN     ??SQL         ??    15      1\n12  12    MMU        ??       <NA>    30      4\n\n\n한글이 깨질 경우 아래 3줄을 입력하면 해결된다(https://leti-lee.tistory.com/17 내용 참고).\n\ndbSendQuery(con, \"SET NAMES utf8;\") \n\n<MySQLResult:12,0,1>\n\ndbSendQuery(con, \"SET CHARACTER SET utf8;\") \n\n<MySQLResult:1729421904,0,2>\n\ndbSendQuery(con, \"SET character_set_connection=utf8;\")\n\n<MySQLResult:-2029701264,0,3>\n\n\n\nmydata = dbGetQuery(con, \"select * from buy\")\nmydata\n\n   num mem_id prod_name group_name price amount\n1    1    BLK      지갑       <NA>    30      2\n2    2    BLK  맥북프로     디지털  1000      1\n3    3    APN    아이폰     디지털   200      1\n4    4    MMU    아이폰     디지털   200      5\n5    5    BLK    청바지       패션    50      3\n6    6    MMU    에어팟     디지털    80     10\n7    7    GRL   혼공SQL       서적    15      5\n8    8    APN   혼공SQL       서적    15      2\n9    9    APN    청바지       패션    50      1\n10  10    MMU      지갑       <NA>    30      1\n11  11    APN   혼공SQL       서적    15      1\n12  12    MMU      지갑       <NA>    30      4\n\n\nmarket_db에 어떤 table이 저장되어 있는지 확인해볼 수 있다.\n\ndbListTables(con)\n\n[1] \"buy\"       \"emp_table\" \"hongong1\"  \"hongong2\"  \"hongong3\"  \"hongong4\" \n[7] \"member\"   \n\n\nmarket_db에 있는 buy table을 R data.frame으로 불러올 수 있다.\n\nbuy_dat <- dbReadTable(con, \"buy\")\nbuy_dat %>% head()\n\n  num mem_id prod_name group_name price amount\n1   1    BLK      지갑       <NA>    30      2\n2   2    BLK  맥북프로     디지털  1000      1\n3   3    APN    아이폰     디지털   200      1\n4   4    MMU    아이폰     디지털   200      5\n5   5    BLK    청바지       패션    50      3\n6   6    MMU    에어팟     디지털    80     10\n\nbuy_dat %>% str()\n\n'data.frame':   12 obs. of  6 variables:\n $ num       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ mem_id    : chr  \"BLK\" \"BLK\" \"APN\" \"MMU\" ...\n $ prod_name : chr  \"지갑\" \"맥북프로\" \"아이폰\" \"아이폰\" ...\n $ group_name: chr  NA \"디지털\" \"디지털\" \"디지털\" ...\n $ price     : int  30 1000 200 200 50 80 15 15 50 30 ...\n $ amount    : int  2 1 1 5 3 10 5 2 1 1 ...\n\n\n\n\n\nCitationBibTeX citation:@online{don2022,\n  author = {Don Don and Don Don},\n  title = {R과 {MySQL} {연동하기}},\n  date = {2022-07-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDon Don, and Don Don. 2022. “R과 MySQL 연동하기.” July 10,\n2022."
  },
  {
    "objectID": "posts/2022-07-10-RMySQL/RMySQL.html",
    "href": "posts/2022-07-10-RMySQL/RMySQL.html",
    "title": "R과 MySQL 연동하기",
    "section": "",
    "text": "R과 MySQL을 연동하는 패키지는 여러가지가 있다(RODBC, odbc…). 그 중에서 가장 쉽게 사용할 수 있는 패키지가 RMySQL 패키지이다. 다른 패키지의 경우 odbc driver를 설치해야 정상적으로 작동하는 것 같은데, mac의 경우 설치가 조금 복잡하다. 몇 가지 시도해본 후 내린 결론은 RMySQL 패키지가 가장 쉽게 사용할 수 있는 것 같다."
  },
  {
    "objectID": "posts/2022-07-10-RMySQL/RMySQL.html#packages",
    "href": "posts/2022-07-10-RMySQL/RMySQL.html#packages",
    "title": "R과 MySQL 연동하기",
    "section": "packages",
    "text": "packages\n\nlibrary(DBI)\nlibrary(RMySQL)\nlibrary(dplyr)\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nDBI 패키지는 R과 데이터베이스간 연결을 위한 패키지이고, RMySQL은 데이터베이스를 연결할 때, MySQL을 사용하도록 선언?하는 패키지이다."
  },
  {
    "objectID": "posts/2022-07-10-RMySQL/RMySQL.html#r과-db-연결",
    "href": "posts/2022-07-10-RMySQL/RMySQL.html#r과-db-연결",
    "title": "R과 MySQL 연동하기",
    "section": "R과 DB 연결",
    "text": "R과 DB 연결\n\nDBI::dbConnect(drv, \n              dname, \n              user, \n              password, \n              host, ...)\n\n\ndrv : DBIDriver 종류\ndbname : MySQL에 저장되어있는 DB 이름\nuser : “root” (default)\npassword : 비밀번호\nhost: localhost(로컬에서 작업할 경우)\n\n혼공 SQL책에 나와있는 market_db DB를 불러왔다.\n\n\n\n\ndrv <- dbDriver(\"MySQL\")\n\ncon = dbConnect(drv, \n                dbname = \"market_db\", \n                user = \"root\", \n                password = pat, \n                host = \"localhost\")"
  },
  {
    "objectID": "posts/2022-07-10-RMySQL/RMySQL.html#db에-있는-테이블-불러오기",
    "href": "posts/2022-07-10-RMySQL/RMySQL.html#db에-있는-테이블-불러오기",
    "title": "R과 MySQL 연동하기",
    "section": "DB에 있는 테이블 불러오기",
    "text": "DB에 있는 테이블 불러오기\nmarket_db 안에 있는 buy table을 불러온 결과는 다음과 같다.\n\nmydata = dbGetQuery(con, \"select * from buy\")\nmydata\n\n   num mem_id prod_name group_name price amount\n1    1    BLK        ??       <NA>    30      2\n2    2    BLK      ????        ???  1000      1\n3    3    APN       ???        ???   200      1\n4    4    MMU       ???        ???   200      5\n5    5    BLK       ???         ??    50      3\n6    6    MMU       ???        ???    80     10\n7    7    GRL     ??SQL         ??    15      5\n8    8    APN     ??SQL         ??    15      2\n9    9    APN       ???         ??    50      1\n10  10    MMU        ??       <NA>    30      1\n11  11    APN     ??SQL         ??    15      1\n12  12    MMU        ??       <NA>    30      4\n\n\n한글이 깨질 경우 아래 3줄을 입력하면 해결된다(https://leti-lee.tistory.com/17 내용 참고).\n\ndbSendQuery(con, \"SET NAMES utf8;\") \n\n<MySQLResult:2,0,1>\n\ndbSendQuery(con, \"SET CHARACTER SET utf8;\") \n\n<MySQLResult:-1327130544,0,2>\n\ndbSendQuery(con, \"SET character_set_connection=utf8;\")\n\n<MySQLResult:-939456456,0,3>\n\n\n\nmydata = dbGetQuery(con, \"select * from buy\")\nmydata\n\n   num mem_id prod_name group_name price amount\n1    1    BLK      지갑       <NA>    30      2\n2    2    BLK  맥북프로     디지털  1000      1\n3    3    APN    아이폰     디지털   200      1\n4    4    MMU    아이폰     디지털   200      5\n5    5    BLK    청바지       패션    50      3\n6    6    MMU    에어팟     디지털    80     10\n7    7    GRL   혼공SQL       서적    15      5\n8    8    APN   혼공SQL       서적    15      2\n9    9    APN    청바지       패션    50      1\n10  10    MMU      지갑       <NA>    30      1\n11  11    APN   혼공SQL       서적    15      1\n12  12    MMU      지갑       <NA>    30      4\n\n\nmarket_db에 어떤 table이 저장되어 있는지 확인해볼 수 있다.\n\ndbListTables(con)\n\n[1] \"buy\"       \"emp_table\" \"hongong1\"  \"hongong2\"  \"hongong3\"  \"hongong4\" \n[7] \"member\""
  },
  {
    "objectID": "posts/2022-07-10-RMySQL/RMySQL.html#r에-테이블-저장",
    "href": "posts/2022-07-10-RMySQL/RMySQL.html#r에-테이블-저장",
    "title": "R과 MySQL 연동하기",
    "section": "R에 테이블 저장",
    "text": "R에 테이블 저장\nmarket_db에 있는 buy table을 R data.frame으로 불러올 수 있다.\n\nbuy_dat <- dbReadTable(con, \"buy\")\nbuy_dat %>% head()\n\n  num mem_id prod_name group_name price amount\n1   1    BLK      지갑       <NA>    30      2\n2   2    BLK  맥북프로     디지털  1000      1\n3   3    APN    아이폰     디지털   200      1\n4   4    MMU    아이폰     디지털   200      5\n5   5    BLK    청바지       패션    50      3\n6   6    MMU    에어팟     디지털    80     10\n\nbuy_dat %>% str()\n\n'data.frame':   12 obs. of  6 variables:\n $ num       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ mem_id    : chr  \"BLK\" \"BLK\" \"APN\" \"MMU\" ...\n $ prod_name : chr  \"지갑\" \"맥북프로\" \"아이폰\" \"아이폰\" ...\n $ group_name: chr  NA \"디지털\" \"디지털\" \"디지털\" ...\n $ price     : int  30 1000 200 200 50 80 15 15 50 30 ...\n $ amount    : int  2 1 1 5 3 10 5 2 1 1 ..."
  },
  {
    "objectID": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html",
    "href": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html",
    "title": "Markov Chain Monte Carlo",
    "section": "",
    "text": "켤레 사전 분포처럼 분포함수 간에 관계가 있거나 함수가 간단한 형태의 경우 적분을 쉽게 할 수 있다. 하지만 복잡한 함수 형태이거나 high dimension인 경우 Monte carlo integration이나 numerical method를 이용한 적분 방법을 적용하기 힘들다. 이 때 사용하는 방법이 Markov chain Monte Carlo 방법이다.\nMonte Carlo integration의 경우 independence sample을 뽑는데 high dimension인 경우 independence sample을 뽑는 것이 어렵다. 따라서 dependence sample을 뽑아서 이 문제를 해결해보자는 것이 Markov Chain Monte Carlo(MCMC)의 아이디어이다. 앞에 Markov Chain이 붙은 것은 dependence sample을 Markov Chain 구조에서 뽑기 때문이다. 이상적인 Markov Chain의 경우 특정 정칙 조건을 만족해야한다.\n\n\n\nirreducibility\npositive recurrence\naperiodicity"
  },
  {
    "objectID": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html#example",
    "href": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html#example",
    "title": "Markov Chain Monte Carlo",
    "section": "Example",
    "text": "Example\nMetropolis-Hasting algorithm을 이용해서 Rayleigh 분포에서 표본을 추출하기 해보자.\n\\(f(x) = \\frac{x}{\\sigma^2}e^\\frac{-x^2}{2\\sigma^2}\\), \\(x\\ge0\\), \\(\\sigma>0\\)\n\ntarget distribution과 support set이 같은 임의의 proposal distribution로 \\(\\mathcal{X}^2(X)\\)를 선정한다.\n\n\\(\\mathcal{X}^2(1)\\)에서 초기값 \\(X_0\\)를 생성하고 x[1]에 저장한다.\n\n\\(i=2,...,N\\)까지 반복한다.\n\n\n\\(\\mathcal{X}^2(df = X_t)=\\mathcal{X}^2(df = X[i-1])\\)로부터 \\(Y\\)를 발생시킨다.\n\\(U(0,1)\\)에서 random number \\(U\\)를 발생시킨다.\n\\(X_t\\)=x[i-1], If \\(U\\le \\frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}\\) \\(Y\\)를 채택, \\(X_{t+1}=Y\\) otherwise \\(X_{t+1}=X_t\\) \\(X_t+1\\)을 x[i]에 저장한다.\nIncrement t\n\n\\(Y\\)가 accept 될 확률은 다음과 같다. \\(\\alpha (X_t, Y) = min(1, \\frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)})\\)\n\nrayleigh <- function(x, sigma){\n        return ((x/sigma^2)*exp(-x^2/(2*sigma^2))) # Rayleigh distribution를 함수로 정의\n}\n\nm <- 10000\nsigma <- 4\nx <- numeric(m)\nx[1] <- rchisq(1, df = 1) # initial value \nk <- 0 \nu <- runif(m) # generate u from U(0,1)\n\nfor (i in 2:m) {\n        xt <- x[i-1]\n        y <- rchisq(1, df = xt)\n        num <- rayleigh(y, sigma)*dchisq(xt, df = y) # posterior theta t\n        den <- rayleigh(xt, sigma)*dchisq(y, df = xt) # posterior theta t-1\n        \n        if (u[i] <= num/den) {\n                x[i] <- y # accept \n        }else {\n                x[i] <- xt # reject \n                k <- k+1\n        }\n}\nk # reject된 갯수 \n\n[1] 4000\n\n\n실제 Rayleigh 분포와 같은지를 비교하기 위해 QQplot을 그리면 다음과 같다.\n\nb <- 2001\ny <- x[b:m]\na <- ppoints(100)\nQR <- sigma*sqrt(-2*log(1-a))\nQ <- quantile(y, a)\n\nqqplot(QR, Q, main = '', cex = 0.5, xlab = 'Rayleigh Quantiles', ylab = 'Sample Quantiles')\nabline(0,1)\n\n\n\n\nproposal distribution을 gamma distribution으로 변경할 경우 다음과 같다."
  },
  {
    "objectID": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html#gibbs-sampler-algorithm",
    "href": "posts/2021-06-12-markov-chain-monte-carlo/mcmc.html#gibbs-sampler-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "Gibbs sampler algorithm",
    "text": "Gibbs sampler algorithm\n\\(X_{(-j)} = (X_1, ... , X_{j-1}, X_{j+1}, ... , X_d)\\)\n\n\\(t=0\\) 시점에서의 초기값 \\(X(0)\\)를 생성한다.\n\\(t=1, 2, ...\\) 에 대해 다음을 반복한다.\n\n\n\\(x_1=X_1(t-1)\\)을 구한다.\n각 \\(j = 1, 2,...,d\\)에 대해\n\n\n\\(f(X_j|x_{(-j)})\\)로 부터 \\(X_j^*(t)\\)를 발생시킨다.\n\\(x_j=X_j^*(t)\\)를 update한다.\n\n\n\\(X(t)=(X_1^*(t), ... , X_d^*(t)\\)를 구한다.\nIncrement t\n\n\nExample (beta-binomial distribution)\n\\(f(x,y) = {n \\choose x}y^{x+a-1}(1-y)^{n-x+b-1}\\), \\(x=0,1,....,n\\), \\(0 \\le y \\le 1\\) \\(X|y \\sim Bin(n, y)\\), \\(Y|x \\sim Beta(x+\\alpha, n-x+\\beta)\\)\nGibbs sampling의 목적은 conditional pdf로 모르는 형태의 joint pdf와 marginal pdf를 구하는 것이다. 따라서 \\(f(x,y)\\)는 실제로는 beta-binomial 분포로 구할 수 있지만 gibbs sampling을 위해서 \\(f(x,y)\\)를 모르고 \\(f(x|y)\\)와 \\(f(y|x)\\)는 안다고 가정한다.\n\n\nAlgorithm\n\n\\(Bin(n, p = Y(t-1))\\)로부터 \\(X^*(t)\\)를 발생시킨다.\n\\(x(t) = X^*(t)\\)를 update한다.\n\\(Beta(x(t)+\\alpha, n-x(t)+\\beta)\\)로부터 \\(Y^*(t)\\)를 발생시킨다.\nSet \\((X(t), Y(t)) = (X^*(t), Y^*(t))\\).\n\n추가적으로 \\((X^*(t), Y^*(t))\\)에 대해서 일정량을 burn in 하는데 이는 초기값의 영향을 없애기 위해서이다. burn in의 비율은 임의로 설정한다.\n\nN <- 500               \nburn <- 100            \nn <- 16\nalpha <- 2\nbeta <- 4\nx <- rep(0, N)\ny <- rep(0, N)\n\n\nx[1] <- rbinom(1, prob = 0.5, size = n)\ny[1] <- rbeta(1, x[1]+alpha, n-x[1]+beta)\n\nfor (i in 2:N) {\n        x[i] <- rbinom(1, prob = y[i-1], size = n)\n        y[i] <- rbeta(1, x[i]+alpha, n-x[i]+beta)\n}\n\nburn_x <- x[(burn+1):N]\n\n\n\nExample (Bivariate distribution)\n\nN <- 5000               \nburn <- 1000            \nX <- matrix(0, N, 2)    \n\nrho <- -.75             \nmu1 <- 0\nmu2 <- 2\nsigma1 <- 1\nsigma2 <- .5\ns1 <- sqrt(1-rho^2)*sigma1\ns2 <- sqrt(1-rho^2)*sigma2\n\n\nX[1, ] <- c(mu1, mu2) # 초기값\n\nfor (i in 2:N) {\n        x2 <- X[i-1, 2] # x2가 주어짐\n        m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2 # x2가 주어졌을 때 x1 조건부 분포의 평균 \n        X[i, 1] <- rnorm(1, m1, s1) # 조건부 분포로 생성된 x1 업데이트 \n        x1 <- X[i, 1]\n        m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1 # x1이 주어졌을 때 x2 조건부 분포의 평균\n        X[i, 2] <- rnorm(1, m2, s2) # 조건부 분포로 생성된 x1 업데이트\n}\n\nb <- burn + 1 # 임의로 부여한 initial value의 효과를 없앰. \nx <- X[b:N, ] # 1000개 버림\n\ncolMeans(x) # 0, 2에 거의 근사 \n\n[1] -0.02094119  2.00648931\n\ncov(x)\n\n           [,1]       [,2]\n[1,]  0.9582500 -0.3503596\n[2,] -0.3503596  0.2371582\n\ncor(x) # rho = -0.75에 거의 근사 \n\n           [,1]       [,2]\n[1,]  1.0000000 -0.7349462\n[2,] -0.7349462  1.0000000\n\nplot(x, main = '', cex = 0.5, xlab = bquote(X[1]), ylab = bquote(X[2]), ylim = range(x[, 2]))"
  }
]