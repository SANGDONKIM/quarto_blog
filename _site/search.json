[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "데이터분석전문가(ADP) 실기 머신러닝 강의 소개",
    "section": "",
    "text": "강의 홈페이지 : https://statisticsplaybook.com/p/adp-r\n4기 수업은 7월 말부터 시작합니다. 이전 수업 영상과 자료는 결제 후에 보실 수 있습니다."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I specialize in using R, python for data analysis, data visualizations. I am also passionate about making bike-sharing systems more efficient."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Don Don",
    "section": "",
    "text": "hexSticker\n\n\nR\n\n\n\n\nR 패키지 logo 만들기\n\n\n\n\n\n\nJun 24, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\nsensemakr 패키지 소개 및 간단한 논문 리뷰\n\n\n\n\n\n\nJun 24, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nglm\n\n\ncaret\n\n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nglm\n\n\n\n\nGLM에서 다중공선성 체크 해야 하는지\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\nQuarto로 블로그 만들기 튜토리얼\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nvscode에서 quarto와 파이썬 사용하기\n\n\n\n\n\n\nJun 22, 2022\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndocker\n\n\nrocker\n\n\n\n\nIntroduction to rocker\n\n\n\n\n\n\nOct 2, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npackrat\n\n\nrenv\n\n\n\n\nIntroduction to packrat, renv\n\n\n\n\n\n\nOct 2, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nfeature importance, partial dependence plot, shap value 소개\n\n\n\n\n\n\nJul 29, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nkernel density estimation 소개\n\n\n\n\n\n\nJun 22, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\nMultiple test에 대한 소개\n\n\n\n\n\n\nJun 18, 2021\n\n\nDon Don\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\noptimization\n\n\n\n\nADMM example code\n\n\n\n\n\n\nMay 30, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ntimeseries\n\n\ntidymodels\n\n\n\n\ntidymodels를 이용한 시계열 모델링\n\n\n\n\n\n\nMay 8, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nrecipes\n\n\ntidymodels\n\n\n\n\ntidymodels에 대한 간단한 소개\n\n\n\n\n\n\nMar 6, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ngbm\n\n\n\n\ngradient boosting machine 소개\n\n\n\n\n\n\nJan 6, 2021\n\n\nDon Don\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\nIntroduction to MCMC\n\n\n\n\n\n\nJan 6, 2021\n\n\nDon Don\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/logistic_reg/index.html",
    "href": "posts/logistic_reg/index.html",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(recipes)\nlibrary(broom)"
  },
  {
    "objectID": "posts/logistic_reg/index.html#example",
    "href": "posts/logistic_reg/index.html#example",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "Example",
    "text": "Example\n먼저 다중회귀모형에서 다중공선성이 있을 때, 회귀계수의 추정량의 분산이 어떻게 변화하는지를 알아보자.\n간단하게, 설명변수 \\(X_1, X_2\\), 절편이 존재하는 다중회귀모형의 \\(\\beta\\)의 공분산은 다음과 같이 구할 수 있다.\n\\[\n\\begin{aligned}\ncov(\\hat{\\beta}) = \\begin{bmatrix}\nvar(\\hat{\\beta_0}) & cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & cov(\\hat{\\beta}_0, \\hat{\\beta}_2) \\\\\ncov(\\hat{\\beta}_1, \\hat{\\beta}_0) & var(\\hat{\\beta}_1) & cov(\\hat{\\beta}_1, \\hat{\\beta}_2) \\\\\ncov(\\hat{\\beta}_2, \\hat{\\beta}_0) & cov(\\hat{\\beta}_2, \\hat{\\beta}_1) & var(\\hat{\\beta}_2)\n\\end{bmatrix}\n=\\sigma^2(X^tX)^{-1}\n\\end{aligned}\n\\]\n표준화된 설명변수 \\(X_1, X_2\\)에 대한 상관계수가 \\(0.99, 0.95, 0.85\\)로 높은 경우를 가정해보자.\n\nxtx <- matrix(c(1, 0.99, 0.85, 0.99, 1, 0.95, 0.85, 0.95, 1), ncol = 3)\nxtx\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.99 0.85\n[2,] 0.99 1.00 0.95\n[3,] 0.85 0.95 1.00\n\n\n\nsolve(xtx)\n\n       [,1]   [,2]    [,3]\n[1,] -15.60  29.20 -14.480\n[2,]  29.20 -44.40  17.360\n[3,] -14.48  17.36  -3.184\n\n\n\\(X\\)는 표준화된 설명변수라고 할 때, 상관계수가 \\(0.99, 0.95, 0.85\\)로 높을 경우 역행렬을 구해보면 역행렬의 각 element는 모두 큰 값으로 커지게 된다(\\(\\sigma = 1\\)). 즉, 위의 식에 대입해서 보면 \\(\\hat{\\beta}\\)의 분산이 매우 커지는 것을 볼 수 있다."
  },
  {
    "objectID": "posts/logistic_reg/index.html#example-2",
    "href": "posts/logistic_reg/index.html#example-2",
    "title": "다중공선성 개념 및 체크 방법",
    "section": "Example",
    "text": "Example\n\nset.seed(1)\nX1 <- rnorm(10)\nX2 <- X1*runif(10)\ny <- rbinom(10, 1, 0.3)\ndat <- data.frame(X1, X2, y)\n\nfit <- glm(y~., family = binomial, dat)\n\n\\((X^tWX)^{-1}\\)는 R base 함수인 vcov()를 통해 구할 수 있다.\n\nvcov(fit)\n\n            (Intercept)         X1         X2\n(Intercept)   0.9009862 -0.7664066  0.6210139\nX1           -0.7664066  3.5509802 -7.4328529\nX2            0.6210139 -7.4328529 28.2736799\n\n\n\\((X^tWX)^{-1}\\)를 직접 구해보면 다음과 같다. \\(w_{ii} = \\hat{\\pi}_i(1-\\hat{\\pi}_i)\\)\n\np <- fit$fitted.values\nw <- p*(1-p)\nX <- model.matrix(fit)\n\nsolve(t(X)%*%diag(w)%*%X)\n\n            (Intercept)         X1         X2\n(Intercept)   0.9009921 -0.7664113  0.6210083\nX1           -0.7664113  3.5509949 -7.4329015\nX2            0.6210083 -7.4329015 28.2739839\n\n\nvcov()의 \\((1, 1)\\) element와 \\(var(\\hat{\\beta_0})\\)의 결과가 같은 것을 볼 수 있다.\n\ntidy(fit)$std.error[1]^2\n\n[1] 0.9009862\n\n\n따라서 GLM에서도 다중회귀모형과 마찬가지로, 모형 적합 시에 다중공선성도 체크해줘야 한다."
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html",
    "href": "posts/ com_seperation/com_sep.html",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "",
    "text": "library(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html#complete-seperation-예제",
    "href": "posts/ com_seperation/com_sep.html#complete-seperation-예제",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "complete seperation 예제",
    "text": "complete seperation 예제\n\n\n\nY\nX1\nX2\n\n\n\n\n0\n1\n3\n\n\n0\n2\n2\n\n\n0\n3\n-1\n\n\n0\n3\n-1\n\n\n1\n5\n2\n\n\n1\n6\n4\n\n\n1\n10\n1\n\n\n1\n11\n0\n\n\n\n\n\\(X_1\\le 3\\)일 경우 \\(Y=0\\)이고, \\(X_1>3\\)일 경우 \\(Y=1\\)로 완벽하게 분리됨"
  },
  {
    "objectID": "posts/ com_seperation/com_sep.html#왜-complete-seperation이-문제인가",
    "href": "posts/ com_seperation/com_sep.html#왜-complete-seperation이-문제인가",
    "title": "logistic regression에서 complete seperation 문제",
    "section": "왜 complete seperation이 문제인가?",
    "text": "왜 complete seperation이 문제인가?\n\nLogistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음\n\n\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n\n\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n\n           1            2            3            4            5            6 \n2.220446e-16 9.861322e-11 3.179312e-12 3.179312e-12 1.000000e+00 1.000000e+00 \n           7            8 \n1.000000e+00 1.000000e+00 \n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred warning message 출력\n\n예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함\n즉, 너무 완벽하게 예측했을 경우 발생함\n\nglm.fit: algorithm did not converge warning message 출력\n\nMLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함"
  },
  {
    "objectID": "posts/com_sep/com_sep.html",
    "href": "posts/com_sep/com_sep.html",
    "title": "complete seperation in logistic regression",
    "section": "",
    "text": "library(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/com_sep/com_sep.html#complete-seperation-예제",
    "href": "posts/com_sep/com_sep.html#complete-seperation-예제",
    "title": "complete seperation in logistic regression",
    "section": "complete seperation 예제",
    "text": "complete seperation 예제\n\n\n\nY\nX1\nX2\n\n\n\n\n0\n1\n3\n\n\n0\n2\n2\n\n\n0\n3\n-1\n\n\n0\n3\n-1\n\n\n1\n5\n2\n\n\n1\n6\n4\n\n\n1\n10\n1\n\n\n1\n11\n0\n\n\n\n\n\\(X_1\\le 3\\)일 경우 \\(Y=0\\)이고, \\(X_1>3\\)일 경우 \\(Y=1\\)로 완벽하게 분리됨"
  },
  {
    "objectID": "posts/com_sep/com_sep.html#왜-complete-seperation이-문제인가",
    "href": "posts/com_sep/com_sep.html#왜-complete-seperation이-문제인가",
    "title": "complete seperation in logistic regression",
    "section": "왜 complete seperation이 문제인가?",
    "text": "왜 complete seperation이 문제인가?\n\nLogistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음\n\n\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n\n\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n\n           1            2            3            4            5            6 \n2.220446e-16 9.861322e-11 3.179312e-12 3.179312e-12 1.000000e+00 1.000000e+00 \n           7            8 \n1.000000e+00 1.000000e+00 \n\n\n\nglm.fit: fitted probabilities numerically 0 or 1 occurred warning message 출력\n\n예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함\n즉, 너무 완벽하게 예측했을 경우 발생함\n\nglm.fit: algorithm did not converge warning message 출력\n\nMLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함"
  },
  {
    "objectID": "posts/py_example/py_ex.html",
    "href": "posts/py_example/py_ex.html",
    "title": "quarto vscode and python",
    "section": "",
    "text": "참고\nhttps://quarto.org/docs/get-started/hello/vscode.html"
  },
  {
    "objectID": "posts/data/blog_building/build_blog.html",
    "href": "posts/data/blog_building/build_blog.html",
    "title": "Quarto로 블로그 만들기",
    "section": "",
    "text": "https://quarto.org/docs/websites/website-blog.html\nhttps://quarto.org/docs/output-formats/html-themes.html\nhttps://www.youtube.com/watch?v=DMgEGpqXEM4"
  },
  {
    "objectID": "posts/2021-06-12-admm/admm.html",
    "href": "posts/2021-06-12-admm/admm.html",
    "title": "ADMM example",
    "section": "",
    "text": "Given x, z and , to some initial value.\nRepeat:\n\n\\(x:= \\arg\\max_{x}(f(x) + \\frac{\\rho}{2}\\|Ax+Bz-c+\\mu\\|_2^2)\\)\n\\(x:= \\arg\\max_{x}(g(z) + \\frac{\\rho}{2}\\|Ax+Bz-c+\\mu\\|_2^2)\\)\n\\(\\mu:= \\mu + (Ax + Bz - c)\\)\nStopping criterion : quit \\(\\|r\\|_2<\\epsilon\\) and \\(\\|s\\|_2<\\epsilon\\) \n\n\n\n\nWe can define the primal and dual residuals in ADMM at step k+1.\n* Primal residuals : \\(r^{k+1} = Ax^{k+1} + Bz^{k+1} - c\\)\n* Dual residuals : \\(s^{k+1} = \\rho A^TB(z^{k+1} - z^k)\\)\n\nTherefore stopping criterion satisfies that \\(\\|r\\|_2\\) and \\(\\|s\\|_2\\) are smaller than any \\(\\epsilon\\)\n\n\n\n\n\\[\n\\begin{equation*}\n\\begin{aligned}\n& \\underset{\\beta}{\\text{minimize}}\n& & \\sum_{i=1}^n (y_i - \\beta_0 - x_i^t\\beta)^2 + \\lambda \\sum_{j = 1}^p |\\beta_j| \\\\\n\\end{aligned}\n\\end{equation*}\n\\]\n\\(\\Leftrightarrow\\)\n\\[\n\\begin{equation*}\n\\begin{aligned}\n& \\underset{\\beta}{\\text{minimize}}\n& & f(\\beta) + f(z) \\\\\n& \\text{subject to}\n& & I\\beta - IZ = 0\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\n\\[\n\\begin{align}\nr = I\\beta - IZ \\newline\nL_\\rho(\\beta, z, v) &= f(\\beta) + g(z) + v^tr + \\frac{\\rho}{2}||r||_2^2 \\newline\n                    &= f(\\beta) + g(z) + \\frac{\\rho}{2}||r+\\frac{1}{\\rho}v||_2^2 - \\frac{\\rho}{2}||v||_2^2 \\newline\n                    &= f(\\beta) + g(z) + \\frac{\\rho}{2}||r+\\mu||_2^2 - constant_v, \\quad \\mu = \\frac{1}{\\rho}v\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n\\beta^{k+1} &:= \\operatorname*{argmin}_\\beta (f(\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_\\beta (y-X\\beta)^t(y-X\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &\\Rightarrow -2X^ty + 2X^tX\\beta + \\rho\\beta - \\rho Z^k +\\rho\\mu^k = 0 \\newline\n            &\\Leftrightarrow (2X^tX + \\rho I)\\beta = 2X^ty + \\rho(Z^k - \\mu^k) \\newline\n            &\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n\\beta^{k+1} &:= \\operatorname*{argmin}_\\beta (f(\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_\\beta (y-X\\beta)^t(y-X\\beta) + \\frac{\\rho}{2}||I\\beta - IZ^k + \\mu^k||_2^2) \\newline\n            &\\Rightarrow -2X^ty + 2X^tX\\beta + \\rho\\beta - \\rho Z^k +\\rho\\mu^k = 0 \\newline\n            &\\Leftrightarrow (2X^tX + \\rho I)\\beta = 2X^ty + \\rho(Z^k - \\mu^k) \\newline\n            &\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\n\\end{align}\n\\]\n\n\nThe prox operatior for \\(g(z) = \\lambda||z||_1\\)\n\\[\n\\begin{align}\nprox_{\\lambda, g}(z) &= \\operatorname*{argmin}_v (\\lambda||z||_1 + \\frac{1}{2}||z-v||_2^2) \\newline\n                     &= \\operatorname*{argmin}_v (||v||_1 + \\frac{1}{2\\cdot \\lambda}||z-v||_2^2) \\newline\n                     &\\therefore \\operatorname*{argmin}_{v_i} (\\frac{1}{2}(v_i - z_i)^2 + \\lambda|v_i|)\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\nZ^{k+1} &:= \\operatorname*{argmin}_Z (g(Z) + \\frac{\\rho}{2}||I\\beta^{k+1} - IZ + \\mu^k||_2^2) \\newline\n            &= \\operatorname*{argmin}_Z (g(Z) + \\frac{\\rho}{2}||\\beta^{k+1} + \\mu^k - Z) ||_2^2 \\newline\n            &= \\operatorname*{argmin}_Z (g(Z) + \\frac{1}{2\\cdot \\frac{1}{\\rho}}||\\beta^{k+1} + \\mu^k - Z) ||_2^2 \\newline\n            &\\therefore prox_{\\frac{1}{\\rho}, g}(\\beta^{k+1} + \\mu^k)\n\\end{align}\n\\]\n\n\n\nGiven \\(\\beta\\), \\(z\\), \\(\\mu\\), \\(\\rho\\) to some initial value\nRepeat:\n\n\\(\\therefore \\beta^{k+1} = (2X^tX + \\rho I)^{-1}(2X^ty + \\rho(Z^k - \\mu^k))\\)\n\\(\\therefore Z^{k+1} = prox_{\\frac{1}{\\rho}, g}(\\beta^{k+1} + \\mu^k)\\)\n\\(\\mu^{k+1}:= \\mu^k + (\\beta^{k+1} - Z^{k+1})\\)\nStopping criterion : quit \\(\\|r\\|_2<\\epsilon\\) and \\(\\|s\\|_2<\\epsilon\\)\n\n\n\n\nprime resdual : \\(r^{k+1} = \\beta^{k+1} - z^{k+1}\\)\ndual resdual : $s^{k+1} = -(z^{k+1} - z^k) $"
  },
  {
    "objectID": "posts/2021-06-12-admm/admm.html#r-code",
    "href": "posts/2021-06-12-admm/admm.html#r-code",
    "title": "ADMM example",
    "section": "R code",
    "text": "R code\n\nnll <- function(X, Y, beta) {\n                A <- Y - X %*% beta\n                loglike <- crossprod(A)\n                return(loglike)\n}\n\n# Proximal operator\nprox.l1 <- function(u, lambda) {\n                uhat <- abs(u) - lambda\n                prox <- sign(u) * pmax(rep(0, length(u)), uhat)\n                return(prox)\n}\n\nl2norm <- function(x) sqrt(sum(x^2))\n\n\nADMM <- function(X,Y,rho=5,lambda=.1,iter=100, eps = 0.0001){\n                \n                n <- nrow(X)\n                p <- ncol(X)\n                \n                beta <- matrix(0, nrow=iter, ncol=p) \n                beta[1,] <- rep(0, p)\n                \n                obj <- rep(0, iter)\n                obj[1] <- nll(X, Y, beta[1,]) + lambda * sum(abs(beta[1,]))\n                \n                z <- matrix(0, nrow=iter, ncol=p)\n                v <- rep(0, p)     \n                \n                invmat <- solve(2*crossprod(X) + diag(rho, p))\n                \n                s <- 0    \n                r <- 0    \n                t <- 0\n                \n                for (t in 2:iter){\n                                \n                                beta[t,] <- invmat %*% (2*crossprod(X, Y) + rho * (z[t-1,]-v))\n                                z[t,] <- prox.l1(beta[t,] + v, lambda/rho)\n                                v <- v + beta[t,] - z[t,]\n                                obj[t] <- nll(X, Y, beta[t,]) + lambda * sum(abs(beta[t,]))\n                                \n                                r <- beta[t,] - z[t,]\n                                s <- -rho * (z[t,] - z[t-1,])\n                                \n                                r.norm <- l2norm(r)\n                                s.norm <- l2norm(s)\n                                \n                                if (r.norm < eps & s.norm < eps) {\n                                                break\n                                }\n                }\n                beta <- beta[-c(t+1:iter),]\n                obj <- obj[-c(t+1:iter)]\n                result <- list(\"beta.hat\" = beta[nrow(beta),], \"beta\"=beta, \"objective\"=obj, \"iter\"=t)\n                return(result)\n}\n\nx <- cbind(1, matrix(rnorm(1000*4), ncol = 4))\nbeta <- c(1.4, -2, -3, 4, 5)\n\neps <- rnorm(1000*1)　\ny <- x%*%beta + eps \n\nADMM(X = x, Y = y)\n\n$beta.hat\n[1]  1.394049 -1.978164 -2.981449  3.974279  4.951313\n\n$beta\n         [,1]      [,2]      [,3]     [,4]     [,5]\n[1,] 0.000000  0.000000  0.000000 0.000000 0.000000\n[2,] 1.390489 -1.973194 -2.974225 3.964208 4.938941\n[3,] 1.393988 -1.978098 -2.981382 3.974205 4.951232\n[4,] 1.394048 -1.978164 -2.981449 3.974279 4.951313\n[5,] 1.394049 -1.978164 -2.981449 3.974279 4.951313\n\n$objective\n[1] 55613.3328   980.2120   979.8707   979.8707   979.8707\n\n$iter\n[1] 5"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html",
    "href": "posts/2021-08-24-packrat/packrat_renv.html",
    "title": "packrat and renv",
    "section": "",
    "text": "데이터과학 전 분야에서 reproducible research가 중요해지고 있다. reproducible research를 위해서는 어떤 개발환경에서든 원 개발 버전과 동일한 환경을 보존해야하는데, 이 때 장애물이 되는 것이 운영체제, r 버전, r 패키지 버전 별로 동일한 환경을 구축하는 일이다. reproducible research를 쉽게 할 수 있게 해주는 대표적인 패키지인 renv, packrat에 대해 알아보자."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#docker-소개",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#docker-소개",
    "title": "packrat and renv",
    "section": "docker 소개",
    "text": "docker 소개\ndocker 설치 방법 및 구동 방법은 https://www.youtube.com/watch?v=VVxvL4xRPjU 에 잘 정리되어있다.\nR은 도커 허브에 rocker라는 이름으로 등록된 많은 image 파일이 있다. 필요한 상황에 맞는 image를 찾아서 다운을 받으면 된다.\nrocker image 설치 방법\nwindow powershell을 키고 아래 문법을 입력하면 설치가 된다.\n$ docker pull rocker/rstudio:3.6.0\n컴퓨터에 설치된 docker image 목록을 확인할 수 있다.\n$ docker image list\nrocekr image 실행 방법\n\n프로젝트 파일을 생성한다.\nr server에서 실행 후 저장한 R script, csv 등의 모든 파일을 local 컴퓨터에 저장하는 폴더를 만드는 것이다.\n\n\n아래 코드를 실행하면 다운받은 image가 실행된다.\n\n$ docker run -d -e USERID=$UID -e PASSWORD=1111 -v ${pwd}:/work -p 7009:8787 rocker/rstudio:3.6.0\n\n\n참고 : https://cultivo-hy.github.io/docker/image/usage/2019/03/14/Docker%EC%A0%95%EB%A6%AC/\n\n\n옵션\n설명\n\n\n\n\n-d\ndetach mode(백그라운드 모드)\n\n\n-p\n호스트와 컨테이너의 포트를 연결\n\n\n-e\n컨테이너 내에서 사용할 환경변수 설정\n\n\n-v\n호스트와 컨테이너의 디렉토리를 연결\n\n\n\n\n구글 크롬에 접속해서 localhost:7009를 주소창에 입력한다.\n\n아이디 : rstudio\n비밀번호 : -e PASSWORD에 입력한 1111\n아래와 같은 rstudio 창이 나오면 완료"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#packrat-사용법",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#packrat-사용법",
    "title": "packrat and renv",
    "section": "packrat 사용법",
    "text": "packrat 사용법\nrstudio 서버에서 아래 코드를 순차적으로 실행한다.\n먼저 docker run에서 마운드한 위치는 /work이므로 setwd로 위치를 변경해준다.\n\nsetwd('/work')\n\n새로운 프로젝트를 생성하는 것이므로 R script, csv 데이터 등을 저장할 새 디렉토리를 생성한다. packrat::init을 실행하게 되면 일반 프로젝트에서 packrat 프로젝트로 전환된다. 즉. 자체 개인 패키지 라이브러리가 있는 프로젝트로 전환되며, 설치된 패키지는 해당 프로젝트에서만 사용할 수 있다.\n\ninstall.packages(\"packrat\")\n\npackrat::init(\"/work\")\n\n\n\n.libPaths()\npackrat::on(\"/work\")\n\n.libPaths()\n\n\npackrat::on으로 packrat 모드를 키게 되면 work 디렉토리에 packrat 폴더로 패키지 저장 위치가 업데이트된다.\n\ninstall.packages(\"tidyverse\")\n\n\n패키지 snapshot을 보면 R 3.6 기준 2019-07-05에 업데이트된 tidyverse 패키지를 설치한다. 패키지 설치는 local에 설치하는 속도보다 몇 배 이상 오래걸릴 수 있다.\nR script를 새로 생성하고 적당한 예제 코드를 작성 후에 저장을 하게되면 work 디렉토리에 R script 파일이 저장되는 것을 볼 수 있다. server 상에서 저장한 이 파일은 local 컴퓨터에 생성했던 프로젝트 파일 test_project2에도 동일하게 저장된다.\n\n\npackrat::snapshot()\n\npackrat::snapshot()을 이용하면 라이브러리의 현재 상태(현재 설치된 패키지 버전)를 저장할 수 있다.\n이것도 저장하는 속도가 생각보다 많이 느리다. 한번 저장해놓으면 계속 쓸 수 있지만 속도가 느린 것은 치명적인 단점인 것 같다. binary package를 이용하면 속도 문제는 개선할 수 있다. (docker post 참고)\n\npackrat::restore()\n\npackrat::restore()을 이용하면 최근 스냅샷에 저장된 라이브러리 상태를 복원할 수 있다."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#init",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#init",
    "title": "packrat and renv",
    "section": "init",
    "text": "init\n다음의 코드를 실행하면, 프로젝트 환경을 초기화한다. 먼저 renv 파일이 생성되고, r version, package version이 json 파일로 저장된 renv.lock 파일이 생성된다. 또 .Rprofile 파일이 생성된다. renv 파일에는 프로젝트에 설치된 패키지가 저장되는 library 파일이 함께 생성된다.\n\nrenv::init()"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#install-package",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#install-package",
    "title": "packrat and renv",
    "section": "Install package",
    "text": "Install package\n프로젝트 내에 특정 패키지를 설치하고 싶을 때는 renv 내에 특정 함수를 이용하면 된다. devtools를 따로 설치하지 않아도 특정 버전만 지정해주면 old version 패키지를 설치할 수 있다. CRAN에 등록되지 않은 github 소스에서도 다이렉트로 패키지를 설치할 수 있다.\n\n# install the latest version of 'digest'\nrenv::install(\"digest\")\n\n# install an old version of 'digest' (using archives)\nrenv::install(\"digest@0.6.18\")\n\n# install 'digest' from GitHub (latest dev. version)\nrenv::install(\"eddelbuettel/digest\")\n\n# install a package from GitHub, using specific commit\nrenv::install(\"eddelbuettel/digest@df55b00bff33e945246eff2586717452e635032f\")"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#snapshot",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#snapshot",
    "title": "packrat and renv",
    "section": "snapshot",
    "text": "snapshot\nsnapshot은 프로젝트 환경의 현재 상태를 renv.lock 파일에 저장한다. 사용한 패키지와 버전에 대한 세부 정보가 기록된다. 만약 패키지를 추가 설치했을 경우, 다시 snapshot을 실행하면 최신상태로 업데이트 된다.\n\nrenv::snapshot()"
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#restore",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#restore",
    "title": "packrat and renv",
    "section": "restore",
    "text": "restore\nrestore는 renv::snapshot()을 실행했던 상태로 복구하는 함수이다. 즉 renv.lock 파일에 업데이트된 최신 버전 패키지를 로드하게 된다. 다른 사람들이 renv.lock 파일을 가져와서 개발환경을 재현하려고 하면 github에서 파일을 다운받고, renv::init() 실행 후 restore 함수를 실행하면 프로젝트 환경이 재현된다.\n\n#renv::init() \nrenv::restore()\n\nWINDOW에서 renv 패키지를 이용해서 프로젝트 환경을 세팅하고, MAC에서 실험했을 때 프로젝트 환경이 동일하게 세팅되는 것을 확인했다."
  },
  {
    "objectID": "posts/2021-08-24-packrat/packrat_renv.html#renv-and-docker",
    "href": "posts/2021-08-24-packrat/packrat_renv.html#renv-and-docker",
    "title": "packrat and renv",
    "section": "renv and docker",
    "text": "renv and docker\nDocker 이미지를 구울 때, renv를 함께 사용하기도 하는 것 같다. 굳이 docker를 쓰는 이유는?? 모르겠다. docker 이미지를 구울 때 renv::restore()를 이용하면 패키지 설치를 새로 안해도 되니까 이미지 굽는 속도가 많이 개선될 것 같다. 참고 링크\n\nFROM rocker/r-base:4.0.2\n# install renv package\nRUN Rscript -e \"install.packages('renv')\"\n# copy everything to docker, including renv.lock file\nCOPY . /app\n# set working directory\nWORKDIR /app\n# restore all the packages\nRUN Rscript -e \"renv::restore()\"\n# run our R code\nCMD [\"Rscript\", \"main.R\"]\n\n참고 자료\nhttps://www.youtube.com/watch?v=Z0Tm-Y7vzNQ\nhttps://www.youtube.com/watch?v=VVxvL4xRPjU\nhttps://rstudio.github.io/packrat/\nhttps://6chaoran.wordpress.com/2020/07/20/introduction-of-renv-package/\nhttps://www.seanwarlick.com/post/setting-up-renv/\nhttps://rstudio.github.io/renv/reference/install.html"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html",
    "title": "Modeltime with tidymodels",
    "section": "",
    "text": "modeltime 패키지는 tidymodels와 연동이 가능한 시계열 모델링 관련 패키지이다. tidymodels에도 auto arima, ma 모형 같은 간단한 시계열 모델이 있지만 다른 머신러닝 모델에 비해서 쓸 수 있는 모델이 제한적이다. 이러한 단점을 modeltime 패키지가 해결해준다.\nmodeltime 시계열 모델링에 특화된 패키지로 시계열에 특화된 모델링과 전처리 관련 함수가 내장되어 있고, tidymodels의 워크플로우를 거의 그대로 이용할 수 있어서 향후 tidymodels가 R의 대표적인 머신러닝 패키지가 된다면 시계열 파트에서는 modeltime 패키지가 주축이 될 것 같다.\nmodeltime 패키지는 하나의 단일 패키지가 아니라 다양한 머신러닝 패키지와 연동해서 하나의 시계열 생태계를 구축하고 있다. 대표적인 패키지는 다음과 같다.\n\nModeltime : 시계열 머신러닝 관련 패키지\nModeltime H2O : H2O의 autoML과 연동이 가능함\nModeltime GluonTS : 시계열 관련 딥러닝 패키지\nModeltime Ensemble : Modeltime 관련 앙상블 패키지\nModeltime Resample : Backtesting 관련 패키지\nTimetk : feature engineering, Data wrangling, time series visualization"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#preparations-준비작업",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#preparations-준비작업",
    "title": "Modeltime with tidymodels",
    "section": "Preparations (준비작업)",
    "text": "Preparations (준비작업)\n\nLibraries\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(skimr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(gt)\nlibrary(timetk)\nlibrary(tidyquant)\nlibrary(sknifedatar)\n\n# Visualization\nlibrary(ggthemes)\nlibrary(ggsci)\nlibrary(viridis)\nlibrary(ggExtra)\n\n\ntheme_set(theme_bw())\n\n\n\nData load\n\nfile_path <- getwd()\nfiles <- list.files(file_path)\nfiles\n\n[1] \"modeltime.qmd\"       \"modeltime.rmarkdown\" \"rdata.csv\"          \n\nrdata <- read.csv(file.path(file_path, \"rdata.csv\"), fileEncoding = \"CP949\", encoding = \"UTF-8\")"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-overview-데이터-기본정보",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-overview-데이터-기본정보",
    "title": "Modeltime with tidymodels",
    "section": "Data overview (데이터 기본정보)",
    "text": "Data overview (데이터 기본정보)\n\nData\n\nglimpse(rdata)\n\nRows: 55,392\nColumns: 20\n$ time      <chr> \"2015-01-01 01:00:00\", \"2015-01-01 02:00:00\", \"2015-01-01 03…\n$ 기온      <dbl> -4.4, -4.6, -4.7, -5.0, -5.0, -5.3, -5.7, -5.7, -5.4, -5.0, …\n$ 강수      <dbl> 0.00, 0.00, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ 풍속      <dbl> 5.4, 4.9, 6.2, 5.0, 5.5, 4.0, 5.0, 4.6, 6.7, 6.5, 6.4, 6.0, …\n$ 풍향      <dbl> 340.0000, 340.0000, 320.0000, 320.0000, 320.0000, 340.0000, …\n$ 습도      <dbl> 47, 50, 50, 56, 52, 58, 58, 56, 57, 59, 60, 62, 58, 62, 61, …\n$ 증기압    <dbl> 2.1, 2.2, 2.2, 2.4, 2.2, 2.4, 2.3, 2.3, 2.3, 2.5, 2.6, 2.6, …\n$ 이슬점    <dbl> -14.0, -13.4, -13.5, -12.4, -13.3, -12.2, -12.6, -13.0, -12.…\n$ 기압      <dbl> 1020.3, 1020.3, 1020.7, 1020.6, 1020.4, 1020.7, 1021.3, 1021…\n$ 해면기압  <dbl> 1024.0, 1024.0, 1024.5, 1024.4, 1024.2, 1024.5, 1025.1, 1025…\n$ 일조      <dbl> NA, NA, NA, NA, NA, NA, NA, 0.0, 0.0, 0.6, 0.9, 0.5, 0.8, 0.…\n$ 일사      <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.025, 0.14…\n$ 전운량    <int> 6, NA, 6, 6, 6, 6, 6, 3, 6, 6, 7, 8, 8, 8, 8, 6, 6, 8, NA, N…\n$ 시정      <dbl> 1500.000, 1750.000, 2000.000, 2000.000, 2000.000, 2000.000, …\n$ 지면온도  <dbl> -4.4, -4.6, -4.7, -5.0, -5.0, -5.3, -5.7, -5.7, -5.4, -5.0, …\n$ floating  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ warehouse <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 12, 150, 195, 270, 254, 184, 131, 18…\n$ dangjin   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 8, 227, 263, 356, 333, 225, 199, 218…\n$ ulsan     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n\nskim(rdata)\n\n\nData summary\n\n\nName\nrdata\n\n\nNumber of rows\n55392\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntime\n0\n1\n19\n19\n0\n55392\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n기온\n0\n1.00\n12.21\n10.36\n-19.3\n3.60\n12.60\n21.10\n36.80\n▁▅▇▇▂\n\n\n강수\n0\n1.00\n0.12\n1.05\n0.0\n0.00\n0.00\n0.00\n102.70\n▇▁▁▁▁\n\n\n풍속\n0\n1.00\n1.96\n1.56\n0.0\n0.70\n1.60\n2.90\n11.70\n▇▃▁▁▁\n\n\n풍향\n0\n1.00\n164.29\n129.50\n0.0\n20.00\n180.00\n290.00\n360.00\n▇▁▃▂▆\n\n\n습도\n0\n1.00\n75.34\n20.14\n10.0\n61.00\n79.00\n94.00\n100.00\n▁▂▃▅▇\n\n\n증기압\n82\n1.00\n12.93\n8.92\n1.0\n5.40\n10.20\n19.20\n42.90\n▇▅▃▂▁\n\n\n이슬점\n85\n1.00\n7.37\n11.04\n-22.4\n-1.70\n7.30\n16.90\n30.20\n▁▆▇▇▅\n\n\n기압\n79\n1.00\n1014.05\n8.36\n983.6\n1007.40\n1014.30\n1020.60\n1036.30\n▁▃▇▇▂\n\n\n해면기압\n78\n1.00\n1017.37\n8.48\n986.6\n1010.60\n1017.60\n1024.00\n1039.70\n▁▃▇▇▂\n\n\n일조\n25453\n0.54\n0.52\n0.45\n0.0\n0.00\n0.60\n1.00\n1.00\n▇▁▁▁▇\n\n\n일사\n0\n1.00\n0.54\n0.83\n0.0\n0.00\n0.01\n0.89\n4.85\n▇▂▁▁▁\n\n\n전운량\n12427\n0.78\n5.23\n3.84\n0.0\n1.00\n6.00\n9.00\n10.00\n▇▂▃▅▇\n\n\n시정\n0\n1.00\n1754.20\n1024.17\n3.0\n1100.00\n1800.00\n2029.00\n6454.00\n▅▇▁▁▁\n\n\n지면온도\n0\n1.00\n12.21\n10.36\n-19.3\n3.60\n12.60\n21.10\n36.80\n▁▅▇▇▂\n\n\nfloating\n28344\n0.49\n122.42\n192.34\n0.0\n0.00\n0.00\n192.25\n753.00\n▇▁▁▁▁\n\n\nwarehouse\n2040\n0.96\n95.81\n150.72\n0.0\n0.00\n0.00\n152.00\n595.00\n▇▁▁▁▁\n\n\ndangjin\n2040\n0.96\n140.11\n221.67\n0.0\n0.00\n0.00\n227.00\n881.00\n▇▁▁▁▁\n\n\nulsan\n3528\n0.94\n66.28\n104.17\n0.0\n0.00\n0.00\n104.00\n448.00\n▇▁▁▁▁\n\n\nhour\n0\n1.00\n11.50\n6.92\n0.0\n5.75\n11.50\n17.25\n23.00\n▇▇▆▇▇"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-preprocessing",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#data-preprocessing",
    "title": "Modeltime with tidymodels",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\nrdata %>% \n    select(-hour) %>% \n    mutate(time = ymd_hms(time)) %>% \n    filter(between(time, ymd('2018-03-01'), ymd('2021-01-31'))) -> rdata\nrdata %>% glimpse()  \n\nRows: 25,609\nColumns: 19\n$ time      <dttm> 2018-03-01 00:00:00, 2018-03-01 01:00:00, 2018-03-01 02:00:…\n$ 기온      <dbl> 3.1, 2.8, 2.6, 2.0, 2.2, 4.1, 3.5, 2.2, 1.0, 0.3, 0.6, 0.5, …\n$ 강수      <dbl> 0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 1.80, 0.00, 0.00, 0.05, …\n$ 풍속      <dbl> 3.6, 0.7, 3.2, 1.9, 2.1, 4.4, 7.9, 6.4, 7.7, 8.9, 7.9, 9.1, …\n$ 풍향      <dbl> 340, 140, 320, 230, 180, 270, 320, 290, 320, 320, 320, 320, …\n$ 습도      <dbl> 96, 97, 95, 97, 97, 97, 93, 86, 82, 71, 63, 58, 60, 60, 56, …\n$ 증기압    <dbl> 7.3, 7.2, 7.0, 6.8, 6.9, 7.9, 7.3, 6.1, 5.4, 4.4, 4.0, 3.7, …\n$ 이슬점    <dbl> 2.5, 2.3, 1.8, 1.5, 1.7, 3.6, 2.4, 0.0, -1.7, -4.3, -5.6, -6…\n$ 기압      <dbl> 1001.3, 1001.9, 1002.6, 1002.8, 1003.0, 1001.8, 1002.7, 1004…\n$ 해면기압  <dbl> 1004.9, 1005.5, 1006.2, 1006.4, 1006.6, 1005.4, 1006.3, 1007…\n$ 일조      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0…\n$ 일사      <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.69, …\n$ 전운량    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ 시정      <dbl> 922, 4315, 2601, 1717, 1957, 571, 57, 436, 593, 593, 711, 82…\n$ 지면온도  <dbl> 3.1, 2.8, 2.6, 2.0, 2.2, 4.1, 3.5, 2.2, 1.0, 0.3, 0.6, 0.5, …\n$ floating  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 313, 532, 607, 614, 608, 641,…\n$ warehouse <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 209, 296, 315, 474, 544, 496,…\n$ dangjin   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 318, 490, 550, 727, 733, 672,…\n$ ulsan     <int> 0, 0, 0, 0, 0, 0, 0, 0, 4, 35, 71, 82, 334, 372, 346, 318, 2…\n\nrdata %>% \n  summarise(across(.fns = ~sum(is.na(.))/length(.)))\n\n  time 기온 강수 풍속 풍향 습도      증기압     이슬점        기압    해면기압\n1    0    0    0    0    0    0 0.001835292 0.00191339 0.001718146 0.001679097\n       일조 일사    전운량 시정 지면온도 floating warehouse dangjin ulsan\n1 0.4545668    0 0.1552579    0        0        0         0       0     0"
  },
  {
    "objectID": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#univariate-timeseries-analysis",
    "href": "posts/2021-06-12-modeltime-with-tidymodels/modeltime.html#univariate-timeseries-analysis",
    "title": "Modeltime with tidymodels",
    "section": "Univariate timeseries analysis",
    "text": "Univariate timeseries analysis\n울산 지역의 전력 발전량 데이터만 활용할 것이기 때문에 날짜 변수를 제외한 나머지 변수는 제거했다.\n\nulsan <- rdata %>% \n  select(-c(dangjin, warehouse, floating)) %>% \n  select(time, ulsan) %>% \n  rename(date = time, value = ulsan)\n\n\nTime series visualization\ntidymodels의 경우 train/test 분리를 위해서 initial_split()을 활용했는데 시계열 데이터의 경우 특정 날짜를 기준으로 잘라야하기 때문에 Modeltime 패키지에 내장되어있는 initial_time_split()을 이용한다. 특정 날짜를 기준으로 자르고 싶을 경우 timeseries_split()을 이용할 수도 있다.\n\ntk_time_series_cv_plan() : split된 object를 데이터프레임으로 전환\nplot_time_series_cv_plan() : sampling된 데이터를 이용해서 시계열 그래프 생성\n\n\ninitial_time_split(data = ulsan, prop = 0.9) %>% \n  tk_time_series_cv_plan() %>% \n  plot_time_series_cv_plan(date, value,\n                           .interact = FALSE, \n                           .title = \"Partition Train / Test\")\n\n\n\n\n\n\nSplit train/test\n\nmonths <- 1\n\ntotal_months <- lubridate::interval(base::min(ulsan$date),\n                                    base::max(ulsan$date)) %/%  \n                                    base::months(1)\n\n\nprop <- (total_months - months) / total_months\n\nsplits <- rsample::initial_time_split(ulsan, prop = prop)\n\n\nsplits %>%\n  timetk::tk_time_series_cv_plan() %>%  \n  timetk::plot_time_series_cv_plan(date, value) \n\n\n\n\n\n\n\nModel fitting\n모델 fitting은 tidymodels 패키지의 방식과 동일하다. 현재는 default 세팅으로 모델을 fitting했지만 튜닝 파라미터가 있을 경우 이전에 tidymodels에서 했던 방식 그대로 grid_latin_hypercube(), grid_random, bayes tuning 등을 이용해서 최적의 파라미터를 찾고 모델을 fitting 해야한다.\n\n# Exponential smoothing \nmodel_fit_ets <- modeltime::exp_smoothing() %>%\n    parsnip::set_engine(engine = \"ets\") %>%\n    parsnip::fit(value ~ date, data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# ARIMA \nmodel_fit_arima <- modeltime::arima_reg() %>%\n    parsnip::set_engine(\"auto_arima\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# ARIMA Boost\nmodel_fit_arima_boost <- modeltime::arima_boost() %>%\n    parsnip::set_engine(\"auto_arima_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date), \n        data = training(splits))\n\nfrequency = 24 observations per 1 day\n\n# Prophet\nmodel_fit_prophet <- modeltime::prophet_reg() %>%\n    parsnip::set_engine(\"prophet\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# Prophet Boost\nmodel_fit_prophet_boost <- modeltime::prophet_boost() %>%\n    parsnip::set_engine(\"prophet_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date), \n        data = training(splits))\n\n\n\nModel time table\nmodeltime_table에 fitting한 모델을 추가한다. modeltime_table은 이전에 서술했다시피 각 모델이 재대로 적합되었는지 확인하고, 이후 예측 워크플로우를 위해서 modeltime_table 구조를 이용하므로 model table을 에러 없이 세팅하는 것이 중요하다.\n\nmodel_tbl <- modeltime::modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet,\n    model_fit_prophet_boost)\n\nmodel_tbl\n\n# Modeltime Table\n# A tibble: 5 × 3\n  .model_id .model   .model_desc                              \n      <int> <list>   <chr>                                    \n1         1 <fit[+]> ETS(A,N,A)                               \n2         2 <fit[+]> ARIMA(5,0,0)(2,1,0)[24]                  \n3         3 <fit[+]> ARIMA(2,0,1)(2,1,0)[24] W/ XGBOOST ERRORS\n4         4 <fit[+]> PROPHET                                  \n5         5 <fit[+]> PROPHET W/ XGBOOST ERRORS                \n\n\n\n\ncalibration\n이전에 만든 modeltime_table을 test 데이터에 적합시켜서 보정을 하는 단계이다.\n\ncalibration_tbl <- model_tbl %>%\n    modeltime::modeltime_calibrate(testing(splits))  \n\n\ncalibration_tbl %>%\n    modeltime::modeltime_accuracy() %>%   \n    flextable::flextable() %>% \n    flextable::bold(part = \"header\") %>% \n    flextable::bg(bg = \"#D3D3D3\", part = \"header\") %>% \n    flextable::autofit()\n\nℹ We have detected a possible intermittent series, you can change the default metric set to the extended_forecast_accuracy_metric_set() containing the MAAPE metric, which is more appropriate for this type of series.\n\n\n\n\n\n\n\n\n\n\n.model_id\n\n\n\n\n.model_desc\n\n\n\n\n.type\n\n\n\n\nmae\n\n\n\n\nmape\n\n\n\n\nmase\n\n\n\n\nsmape\n\n\n\n\nrmse\n\n\n\n\nrsq\n\n\n\n\n\n\n\n\n1\n\n\n\n\nETS(A,N,A)\n\n\n\n\nTest\n\n\n\n\n47.69905\n\n\n\n\nInf\n\n\n\n\n2.191688\n\n\n\n\n146.0680\n\n\n\n\n73.01855\n\n\n\n\n0.7655620\n\n\n\n\n\n\n2\n\n\n\n\nARIMA(5,0,0)(2,1,0)[24]\n\n\n\n\nTest\n\n\n\n\n22.46559\n\n\n\n\nInf\n\n\n\n\n1.032255\n\n\n\n\n140.5838\n\n\n\n\n46.02106\n\n\n\n\n0.7889317\n\n\n\n\n\n\n3\n\n\n\n\nARIMA(2,0,1)(2,1,0)[24] W/ XGBOOST ERRORS\n\n\n\n\nTest\n\n\n\n\n42.10938\n\n\n\n\nInf\n\n\n\n\n1.934853\n\n\n\n\n142.5776\n\n\n\n\n58.27198\n\n\n\n\n0.7888361\n\n\n\n\n\n\n4\n\n\n\n\nPROPHET\n\n\n\n\nTest\n\n\n\n\n30.97759\n\n\n\n\nInf\n\n\n\n\n1.423366\n\n\n\n\n144.1818\n\n\n\n\n48.23531\n\n\n\n\n0.7779553\n\n\n\n\n\n\n5\n\n\n\n\nPROPHET W/ XGBOOST ERRORS\n\n\n\n\nTest\n\n\n\n\n58.43433\n\n\n\n\nInf\n\n\n\n\n2.684955\n\n\n\n\n146.4095\n\n\n\n\n70.19536\n\n\n\n\n0.7788829\n\n\n\n\n\n\n\n\n\n\n\nvisualization the forecast test\n\ncalibration_tbl %>%\n    modeltime::modeltime_forecast(new_data = testing(splits), \n                                  actual_data = ulsan,\n                                  conf_interval = 0.90) %>%\n    modeltime::plot_modeltime_forecast(.legend_show = TRUE, \n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\nRefit\ntrain/test를 합한 original 데이터를 이용해서 refitting을 진행하는 단계이다.\n\nrefit_tbl <- calibration_tbl %>%\n    modeltime::modeltime_refit(data = ulsan)\n\nfrequency = 24 observations per 1 day\nfrequency = 24 observations per 1 day\nfrequency = 24 observations per 1 day\n\n\n\n\nforecast\nrefitting된 모델을 이용해서 지정한 time interval에 대해 forecast를 수행하는 단계이다.\n\nforecast_tbl <- refit_tbl %>%\n    modeltime::modeltime_forecast(\n        h = \"1 month\",\n        actual_data = ulsan,\n        conf_interval = 0.90\n    ) \n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nWarning in time_adder(index, period): Missing values created during time\naddition. This can happen if dates do not exist.\n\nforecast_tbl %>%\n    modeltime::plot_modeltime_forecast(.interactive = TRUE,\n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\nAggregate model\nAccuracy 향상을 위해서 적합시킨 5 가지 모델을 평균내서 최종 모델을 산출한다. Modeltime.ensemble을 이용하면 더 세련된 앙상블 기법을 이용할 수 있다.\n\nmean_forecast_tbl <- forecast_tbl %>%\n    dplyr::filter(.key != \"actual\") %>%\n    dplyr::group_by(.key, .index) %>%\n    dplyr::summarise(across(.value:.conf_hi, mean)) %>%\n    dplyr::mutate(\n        .model_id   = 6,\n        .model_desc = \"AVERAGE OF MODELS\"\n    )\n\n`summarise()` has grouped output by '.key'. You can override using the\n`.groups` argument.\n\n# Visualize aggregate model \nforecast_tbl %>%\n    dplyr::filter(.key == \"actual\") %>%\n    dplyr::bind_rows(mean_forecast_tbl) %>%\n    modeltime::plot_modeltime_forecast()  \n\n\n\n\n\n\n\n참고 자료\nH2O 관련\nhttps://www.r-bloggers.com/2021/03/introducing-modeltime-h2o-automatic-forecasting-with-h2o-automl/\ngluonTS 관련\nhttps://cran.r-project.org/web/packages/modeltime.gluonts/vignettes/getting-started.html#references\nModeltime 관련\nhttps://cran.r-project.org/web/packages/modeltime/vignettes/getting-started-with-modeltime.html\nhttps://www.adam-d-mckinnon.com/posts/2020-10-10-forecastpeopleanalytics/"
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html",
    "title": "multiple test",
    "section": "",
    "text": "다중검정은 multiple test, simultaneous tes, joint test 등으로 불리우는데, 보통 ANOVA 이후 집단 간의 세부적인 차이를 알아보기 위한 사후 검정으로 활용된다. 이번에 다중 검정 관련 논문을 읽을 일이 있어서 기본 개념에 대해 정리를 해보려고 한다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#one-way-anova",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#one-way-anova",
    "title": "multiple test",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n세 집단에 약물을 복용했을 때 치료 효과가 있는지 ANOVA를 실시한다고 해보자. ANOVA를 수식으로 표현하면 다음과 같다.\n\\[\nY_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}, \\quad i = 1,2, 3 \\quad j = 1, \\cdots ,n \\\\\n\\alpha_i : \\text{i - th treatment effect}\n\\]\n수식을 직관적으로 표로 표현하면 다음과 같다.\n\n\n\n\n\n\n\n\n\n\nTreat \\ Rep\n\\[\n1\n\\]\n\\[\n2\n\\]\n\\[\n\\cdots\n\\]\n\\[\nn\n\\]\n\n\n\n\n\\[\n1\n\\]\n\\[\n                Y_{11}\n                \\]\n\\[\n                         Y_{12}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{1n}\n                                           \\]\n\n\n\\[\n2\n\\]\n\\[\n                Y_{21}\n                \\]\n\\[\n                         Y_{22}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{2n}\n                                           \\]\n\n\n\\[\n3\n\\]\n\\[\n                Y_{31}\n                \\]\n\\[\n                         Y_{32}\n                         \\]\n\\[\n                                  \\cdots\n                                  \\]\n\\[\n                                           Y_{3n}\n                                           \\]\n\n\n\nANOVA에 대한 가설은 다음과 같이 도출될 수 있다.\n\\[\nH_0 : \\alpha_1 = \\alpha_2 = \\alpha_3 \\quad \\text{(no treatment effect)} \\\\\nH_1: \\text{Not }H_0\n\\]\n다음과 같은 가설 설정에 대해서 등분산 가정 하에 ANOVA를 수행했을 때 항상 두 가지 결론이 도출될 수 있다.\n\n귀무가설을 기각하지 못할 경우\n\n즉 세 집단 간의 평균 차이가 없다(혹은 약물의 치료 효과가 없다)\n\n귀무가설을 기각할 경우\n\n즉, 세 집단 간의 평균 차이가 존재한다.\n\n\n여기서 귀무가설을 기각할 경우를 주목해보자. 귀무가설을 기각할 경우 내릴 수 있는 결론은 “세 집단 간의 평균 차이가 존재한다”이다. 다시 풀어서 써보면 “세 집단 간 중에 적어도 하나는 평균 차이가 존재한다”이며, 이는 어느 집단 간에 차이가 나는지 모른다는 의미와 같다. 즉, 다음과 같은 경우의 수가 도출될 수 있다.\n\\[H_1 : \\\\\n\\alpha_1 = \\alpha_2 > \\alpha3 \\\\\n\\alpha_1 = \\alpha_2 < \\alpha3 \\\\\n\\alpha_1 < \\alpha_2 = \\alpha3 \\\\\n\\vdots \\\\\n\\alpha_1 \\neq \\alpha_2 \\neq \\alpha3\n\\]\nANOVA test로는 어느 집단 간에 평균 차이가 존재하는지 모르기 때문에 사후검정 을 추가적으로 실시해야만 이를 알 수 있다. 이 때 수행하는 것이 다중 검정(multiple test)이다.\n\\[\nH_{01}: \\alpha_1 = \\alpha_2, \\quad H_{02}: \\alpha_1 = \\alpha_3, \\quad H_{03}: \\alpha_2 = \\alpha_3,\n\\]\n가설의 개수는 treatment 3일 때, 2개의 쌍을 뽑는 것이므로 \\(3 \\choose 2\\)가 된다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#multiple-test",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#multiple-test",
    "title": "multiple test",
    "section": "Multiple test",
    "text": "Multiple test\n다중 검정이 왜 필요한지에 대해 이해했으니, 이제 다중 검정을 실시할 때의 문제점에 대해 알아보자.\n\\(m\\)개의 가설이 있다고 해보자.\n\\[\nH_{0i} : \\mu_{1i} = \\mu_{2i}, \\quad i = 1, \\cdots, m\n\\]\n각 가설에 대한 제 1종 오류를 수식으로 표현하면 다음과 같다.\n\\[\nE_i = \\{\\text{reject }H_{0i} \\text{ when }H_{0i}\\text{ is true}\\}, \\quad i = 1, \\cdots, m\n\\]\n\\(m\\) 개의 가설에 대한 제 1종 오류의 upper bound를 표현하면\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\bigcup_{i=1}^{m} E_{i}) &= 1-P(\\bigcap_{i=1}^{m} E_{i}^c) \\\\\n&= 1 -  \\prod_{i=1}^{m} P(E_i^c) \\\\\n&\\le 1 - (1-\\alpha)^m, \\quad P(E_i) \\le \\alpha\n\\end{aligned}\n\\end{equation*}\n\\]\n여기서 \\(\\bigcap\\)이 \\(\\prod\\)로 표현되는 이유는 가설 설정 시에 모든 가설(\\(E_i\\))은 서로 독립이기 때문이다. \\(m\\) 에 값을 대입하면서 1종 오류의 upper bound에 대해 알아보자.\n\\[\n\\begin{equation*}\n\\begin{aligned}\n&\\alpha = 0.05 \\text{일 때}, \\\\\n&m = 2,\\quad 1 - (1-\\alpha)^m=0.0975 \\\\\n&m = 3,\\quad 1 - (1-\\alpha)^m=0.1426 \\\\\n&m = 100,\\quad 1 - (1-\\alpha)^m \\backsimeq 0.9941 \\backsimeq 1 \\\\\n\\end{aligned}\n\\end{equation*}\n\\]\n\\(m\\) 이 증가할 때, 즉 가설의 개수가 많아질 때 제 1종 오류가 발생할 upper bound는 거의 1에 가깝게 되며, 이는 거의 항상 error가 발생한다고 할 수 있다. 따라서 이러한 문제를 해결하기 위해서 제 1종 오류에 대한 일종의 보정을 수행해야 하는데 대표적인 방법이 Bonferroni correction이다. Bonferroni correction을 설명하기 이전에 광의적인 개념인 FWER에 대해 집고 넘어간다."
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#family-wise-error-ratefwer",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#family-wise-error-ratefwer",
    "title": "multiple test",
    "section": "Family-wise error rate(FWER)",
    "text": "Family-wise error rate(FWER)\n\\(m\\) 개의 가설검정(\\(H^1, H^2,\\cdots, H^m\\))을 수행한다고 가정했을 때 결과를 table로 정리해보면 다음과 같다.\n\nResult of m multiple hypothesis\n\n\n\n\n\n\n\n\ntrue state \\ decision\n\\[\n\\text{accept }H_0\n\\]\n\\[\n\\text{reject }H_0\n\\]\n\n\n\n\n\n\\[\nH_0: \\text{true}\n\\]\n\\[\n                          U\\text{(unknown)}\n                          \\]\n\\[\n                                              V\\text{(unknown)}\n                                              \\]\n\\[\n                                                                  m_0\\text{(unknown)}\n                                                                  \\]\n\n\n\\[\nH_1: \\text{true}\n\\]\n\\[\n                          T\\text{(unknown)}\n                          \\]\n\\[\n                                              S\\text{(unknown)}\n                                              \\]\n\\[\n                                                                  m_1\\text{(unknown)}\n                                                                  \\]\n\n\n\\[\n\\text{Total}\n\\]\n\\[\n                          m-R\\text{(known)}\n                          \\]\n\\[\n                                              R\\text{(known)}\n                                              \\]\n\\[\n                                                                  m\\text{(known)}\n                                                                  \\]\n\n\n\nFWER의 의미는 the probability of at least one type 1 error, 즉 1번이라도 1종 오류가 발생할 확률을 의미한다. 이를 수식으로 표현하면 \\(P(V \\ge1)\\) 이다. FWER을 통제한다는 의미는 \\(P(V \\ge1) \\le \\alpha\\) , 즉 최소 1번이라도 제 1종 오류를 범할 확률을 \\(\\alpha\\) 이하로 조절한다는 의미이고, 이를 위해서는 개별 검정 결과에 대한 제 1종 오류를 범할 확률을 조절해야한다.\n\nBonferroni Correction\nFWER을 조절하는 대표적인 방법이 본페로니 보정방법이다. FWER은 Bonferroni inequality에 의해 다음과 같은 upper bound를 얻을 수 있다.\n\\[\nP(V \\ge 1)=P(\\bigcup_{i=1}^{m} E_{i}) \\le \\sum_{i=1}^m P(E_i) \\le \\alpha\n\\]\nBonferroni Correction은 위의 upper bound를 이용해서 개별 가설에 대한 \\(\\alpha^*\\) 값을 전체 \\(\\alpha\\) 를 개별 가설의 개수 \\(m\\)으로 나눠준다.\n\\[\\alpha^* = \\frac{\\alpha}{m}\\]\nBonferroni Correction은 \\(m\\) 이 작을 때, 비교적 잘 작동한다. \\(m\\) 이 클 경우 Bonferroni Correction은 잘 작동하지 않는다. 수식을 보면 직관적으로 알 수 있는데 \\(m\\)이 커질 경우 \\(\\alpha^*\\) 값이 매우 작아지기 때문에 개별 가설에 대해서 거의 기각하지 못하는 문제가 발생한다. 즉 Bonferroni Correction은 매우 보수적인 방법이라고 할 수 있다.\n\n\nSidak Correction\n시닥 보정 방법은 본페로니 보정 방법에서 조금 개선된 버전이다. 하지만 본페로니 보정의 문제점을 그대로 가지고 있다.\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\bigcup_{i=1}^{m} E_{i}) &= 1-P(\\bigcap_{i=1}^{m} E_{i}^c) \\\\\n&= 1 - \\prod_{i=1}^{m} P(E_i^c) \\\\\n&= 1 - \\{1-P(E_i)\\}^m \\\\\n&= 1 - (1-\\alpha^*)^m \\le \\alpha \\\\\n&\\therefore \\alpha^* = 1-(1-\\alpha)^\\frac{1}{m}\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nExample\n\nNo correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-0.05)^{20} \\\\\n&\\simeq 0.64\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nBonferroni correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-\\frac{0.05}{20})^{20} \\\\\n&\\simeq 0.0488\n\\end{aligned}\n\\end{equation*}\n\\]\n\n\nSidak correction\n\\[\n\\begin{equation*}\n\\begin{aligned}\nP(\\text{at least one significant result}) &= 1-P(\\text{no significant result}) \\\\\n&= 1 - (1-\\alpha^*)^{20} \\\\\n&\\simeq 0.1854\n\\end{aligned}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "posts/2021-06-18-multiple-test/multiple_test.html#false-discovery-ratefdr",
    "href": "posts/2021-06-18-multiple-test/multiple_test.html#false-discovery-ratefdr",
    "title": "multiple test",
    "section": "False Discovery Rate(FDR)",
    "text": "False Discovery Rate(FDR)\nFDR은 FWER의 단점인 가설의 수가 많아질 때 개별 가설에 대해서 거의 기각하지 못하는 단점을 보완한 방법이다.\nBenjamini and Hochberg(1995)\n\\(E[\\frac{V}{R}] \\le q, \\quad q \\text{ : correspond to level of significance}\\)\n식을 해석해보면 잘못기각되는 가설의 비율의 기댓값을 \\(q\\) 수준으로 조절하는 것이라고 할 수 있다.\nBenjamini and hochberg FDR procedure\nTo control FDR at level \\(\\delta\\)\n\nOrder the unadjusted p-values: \\(p_1 \\le p_2 \\le \\cdots \\le p_m\\)\nThen find the test with the highest rank, \\(j\\), for which the p-value, \\(p_j\\), is less than or equal to \\(\\frac{j}{m}\\cdot \\delta\\)\nDeclare the tests of rank \\(1, 2, \\cdots, j\\) as significant \\(p(j) \\le \\delta \\cdot \\frac{j}{m}\\)\n\n\nCode\n\nNo corrections\n\nset.seed(311)\nx <- c(rnorm(900), rnorm(100, mean = 3))\nplot(density(x))\n\n\n\np <- pnorm(x, lower.tail = F)\nlength(p)\n\n  [1] 1000\n\ntest <- p > 0.05\n\nsummary(test[1:900])\n\n     Mode   FALSE    TRUE \n  logical      45     855\n\nsummary(test[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      95       5\n\n\n\n\nBonferroni correction\n\nbonftest <- p > 0.00005\nsummary(bonftest[1:900])\n\n     Mode    TRUE \n  logical     900\n\nsummary(bonftest[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      20      80\n\n\n\n\nFDR\n\npsort <- sort(p)\nfdrtest <- NULL\n\nfor (i in 1:1000){\n  fdrtest <- c(fdrtest, p[i] > match(p[i],psort) * .05/1000)\n\n}\nsummary(fdrtest[1:900])\n\n     Mode   FALSE    TRUE \n  logical       2     898\n\nsummary(fdrtest[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      62      38\n\n\n\n\nFDR using R stat package\n\np.adj <- stats::p.adjust(p, method = 'BH', n = length(p))\nfdrtest2 <- p.adj > 0.05\nsummary(fdrtest2[1:900])\n\n     Mode   FALSE    TRUE \n  logical       2     898\n\nsummary(fdrtest2[901:1000])\n\n     Mode   FALSE    TRUE \n  logical      62      38\n\nmatplot(p, p.adj)\n\n\n\n\n\n\n\n참고 자료\nhttps://www.stat.berkeley.edu/~mgoldman/Section0402.pdf\nhttps://be-favorite.tistory.com/26\nhttps://www.tandfonline.com/doi/suppl/10.1080/01621459.2020.1859379?scroll=top\nhttps://ndownloader.figstatic.com/files/25717051\nhttps://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture10.pdf"
  },
  {
    "objectID": "posts/2021-06-12-kernel-density-estimation/KDE.html",
    "href": "posts/2021-06-12-kernel-density-estimation/KDE.html",
    "title": "kernel density estimation",
    "section": "",
    "text": "확률밀도함수(pdf)는 확률변수의 분포를 나타내는 함수로 보통 확률변수가 연속형일 때를 지칭한다. 확률 밀도 함수는 두 가지 조건을 만족해야 한다.\n\n모든 실수값 x에 대해 \\(f(x)\\ge 0\\)\n\\(\\int_{-\\infty}^{\\infty} f(x) dx\\)=1\n\npdf 조건에서 알 수 있듯이 확률밀도함수는 확률이 아니며, 확률밀도함수를 적분해야만 확률이 나온다."
  },
  {
    "objectID": "posts/2021-06-12-kernel-density-estimation/KDE.html#gaussian-kernel-example",
    "href": "posts/2021-06-12-kernel-density-estimation/KDE.html#gaussian-kernel-example",
    "title": "kernel density estimation",
    "section": "Gaussian kernel example",
    "text": "Gaussian kernel example\n\nx <- c(65, 75, 67, 79, 81, 91) # observed data \ny <- 50:99  \nh <- 5.5\nn <- length(y)\n\nB <- numeric(n)\nK <- numeric(n)\n\n\n\\(x_i\\) = 65일 때\n\nfor (j in 1:n) {\n        A <- 1/(h*sqrt(2*pi))\n        B[j] <- (-0.5)*((y[j] - 65)/h)^2\n        K[j] <- A*exp(B[j])\n}\n\nplot(y, K, type = 'l', main = 'kernel at xi = 65')\n\n\n\n\n\n\n각 \\(x_i\\) 별 kernel plot\n\nm <- length(x)\n\nB <- matrix(0, nrow = n, ncol = m)\nK <- matrix(0, nrow = n, ncol = m)\n\nfor (i in 1:m) {\n        for (j in 1:n) {\n                A <- 1/(h*sqrt(2*pi)*m)\n                B[j, i] <- (-0.5)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\n\n\nplot(y, K[,1], type = 'l', main = '', xlim = c(45, 110), ylim = c(0, 0.04))\nfor (i in 2:6) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\n# 최종 kernel\nK <- round(K, digit = 7)\nd <- rowSums(K)\n\nlines(y, d, type = 'l', main = 'Kernel density')\n\n\n\n\n\n\n\\(h\\) 값에 따른 kernel의 형태 변화 (Gaussian kernel 일 때)\n\\(h\\) 값을 작게 하면 undersmooth되고, 반면에 h값을 크게 하면 oversmooth된다. 따라서 적절한 \\(h\\)를 찾는 것이 중요하다. \\(h\\) 값은 MLCV(Maximum likelihood cross validation)에 의해 추정할 수 있다. \\(MLCV_{max} = \\frac{1}{n}\\sum_{i=1}^nlog[\\sum_{j}w(\\frac{x_j-X_i}{h})]-log[(n-1)h]\\)\n\nx <- c(-0.77, -0.6, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)\ny <- seq(-4, 4, 0.1)\nn <- NROW(x)\nm <- NROW(y)\n\nB <- matrix(0, nrow = m, ncol = n)\nK <- matrix(0, nrow = m, ncol = n)\n\nh <- 0.25\n\n\nfor (i in 1:n) {\n        for (j in 1:m) {\n                A <- 1/(h*sqrt(2*pi)*n)\n                B[j, i] <- (-1/2)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\nplot(y, K[,1], type = 'l', main = 'h = 0.25', ylim = c(0, 0.55), ylab = '', xlab = '')\nfor (i in 2:10) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\nK <- round(K, digit = 7)\nd <- rowSums(K)\nlines(y, d, type = 'l', main = '')\n\n\n\nx <- c(-0.77, -0.6, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)\ny <- seq(-4, 4, 0.1)\nn <- NROW(x)\nm <- NROW(y)\n\nB <- matrix(0, nrow = m, ncol = n)\nK <- matrix(0, nrow = m, ncol = n)\n\nh <- 1\n\n\nfor (i in 1:n) {\n        for (j in 1:m) {\n                A <- 1/(h*sqrt(2*pi)*n)\n                B[j, i] <- (-1/2)*((y[j] - x[i])/h)^2\n                K[j, i] <- A*exp(B[j, i])\n        }\n}\n\nplot(y, K[,1], type = 'l', main = 'h = 1', ylim = c(0, 0.55), ylab = '', xlab = '')\nfor (i in 2:10) {\n        lines(y, K[,i], type = 'l', main = '')\n}\n\n\nK <- round(K, digit = 7)\nd <- rowSums(K)\nlines(y, d, type = 'l', main = '')"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html",
    "title": "tidymodels tutorial",
    "section": "",
    "text": "tidymodels는 R 유저라면 한번쯤 써봤을 패키지인 tidyverse와 결이 같은 패키지이다. tidyverse가 데이터 전처리, 시각화를 간결한 파이프라인으로 만들 수 있는 것과 마찬가지로, tidymodels는 데이터 모델링 관점에서의 데이터 전처리, 모델링, 시각화 등을 쉽게 할 수 있게 고안된 패키지이다. tidyverse 수준으로 굉장히 완성도가 있고, R을 좋아하는 진성 유저들이 기존에 존재하는 패키지를 tidymodels와 결합하거나 새로운 함수를 끊임없이 만들고 있는 중이다."
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#recipe",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#recipe",
    "title": "tidymodels tutorial",
    "section": "Recipe",
    "text": "Recipe\n데이터 전처리를 위한 단계를 정의하는 object이다. 특이한 점은 즉시 실행되지 않고 단계를 정의만한다는 것이다. 왜 번거롭게 recipe를 사용해야 하는지 의문이 생길 수 있다.\nRecipe의 장점은 다음과 같다.\n\nrecipe object를 여러가지 모델에 재사용 가능\nrecipe 내에 사전 정의된 함수를 이용하면 코드의 간결성 확보 가능\n\n즉, recipe를 이용해서 사전 정의된 object는 linear regression, random forest, xgboost 등등 tidymodel과 연동된 여러가지 모델에 대해 동일하게 적용할 수 있다. 또 recipe에는 생각보다 다양한 데이터 전처리 관련 함수가 있는데 이를 이용하면 기존에 각 변수별로 정의를 해주어야했던 데이터 전처리 과정을 간결하고 가독성 있는 코드로 구현이 가능하다.\n\names_rec <- \n  recipe(sale_price ~ ., data = ames_train) %>%\n  step_string2factor(all_nominal()) %>% \n  step_other(all_nominal(), threshold = 0.01) %>%\n  step_nzv(all_nominal())\n\nrecipe 내에 step_function은 다음과 같다. 이외에도 imputation 관련 다양한 함수가 존재하며, 데이터 전처리에 필요한 거의 대부분의 함수가 내장되어있다.\n\nNormalization\n\n\nstep_center(var) - 평균을 빼서 중심 이동\nstep_normalize(var) - 평균 빼고, 분산으로 나눠서 표준화\n\n\nFilters\n\n\nstep_corr(threshold = 0.9) - 상관계수 절대값이 큰 변수 제거\nstep_rm(var) - 변수 제거\nstep_zv() - 분산이 0인 변수 제거\nstep_nzv() - 분산이 거의 0인 변수 제거\n\n\nTransformations\n\n\nstep_log(var, base = exp(1) ) - 로그 변환\nstep_logit(var) - 로짓 변환\nstep_poly(var, degree = 2) - 변수에 polynomial term 추가(glm에서 poly() 와 동일, 즉 orthogonal polynomial 이용)\nstep_BoxCox() - Boxcox 변환\nstep_YeoJohnson - YeoJohnson 변환\n\n\nDiscretization\n\n\nstep_discretize(var, num_breaks = 4) - 연속형 변수 이산형으로 변환\nstep_cut() - 연속형 변수를 지정한 값을 기준으로 이산형으로 변환\n\n\ninclude_outside_range - 지정한 범위를 넘어선 값을 양끝 break에 포함시킬지 여부. default = FALSE이며 결측치 처리됨\nbreaks - 절단 기준이 되는 값\n\n\n\n\nDummy variables and encodings\n\n\nstep_date() - date 변수에서 year, month, day of week 변수를 새롭게 생성\n\nfeature = c(‘dow’, ‘month’, ‘year’) - 요일, 달, 연도 변수 추가\nabbr = T - Sunday or Sun\nlabel = Sunday or number\n\nstep_holiday() - date 변수에서 공휴일에 관한 이진변수 새롭게 생성\n\n\nholidays = c(‘LaborDay’, ‘NewYearDay’, ‘ChristmasDay’)\nholidays = timeDate::listHolidays(‘US’)\n\n\nstep_dummy() - character or factor 변수를 더미변수로 변환\n\n\none_hot = TRUE - C +1개의 더미변수 생성(one_hot = F: C-1개 더미변수 생성\n\n\nstep_other() - 범주형 변수의 level이 여러개일 때, 하위 범주를 기타로 묶음\n\n\nthreshold = 0.05 - 하위 5% 범주는 기타로 묶임\nother : 기타로 지정할 level 이름 지정\n\n\nstep_interact() - 상호작용 항 추가"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#prep",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#prep",
    "title": "tidymodels tutorial",
    "section": "Prep",
    "text": "Prep\nrecipe object를 설정한 후에 prep을 이용해서 계산을 한다.\n\names_rec_prepped <- prep(ames_rec)"
  },
  {
    "objectID": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#bake",
    "href": "posts/2021-06-12-tidymodels-tutorial/tidymodels_tutorial.html#bake",
    "title": "tidymodels tutorial",
    "section": "Bake",
    "text": "Bake\nrecipe, prep을 거쳐서 전처리된 데이터를 output으로 내보내는 단계이다.\n데이터 전처리를 완료한 결과를 보고 싶으면 bake 함수를 이용하면 되는데 training data를 기준으로 이전에 데이터 전처리를 했기 때문에 new_data = training set을 넣고 중복 계산할 필요가 없다.\n\names_train_prepped <- bake(ames_rec_prepped, new_data = NULL)\n\ntest 데이터를 기준으로 전처리를 진행할 때 new_data = test set을 넣어주기만 하면 recipe, prep을 재지정해줄 필요 없이 곧바로 데이터 전처리가 가능하다.\n\names_test_prepped <- bake(ames_rec_prepped, ames_test)"
  },
  {
    "objectID": "posts/2021-06-12-gradient-boosting-regression/gbm.html#pseudo-code",
    "href": "posts/2021-06-12-gradient-boosting-regression/gbm.html#pseudo-code",
    "title": "gradient boosting machine tutorial",
    "section": "Pseudo code",
    "text": "Pseudo code\nInput : Data \\(\\{x_i, y_i\\}^n_{i=1}\\), and a differentiable Loss function \\(L(y_i, F(x))\\)\nStep 1 : Initialized model with a constant value: \\(F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)\\)\nStep 2 : for \\(m = 1\\) to M:\n(A) Compute \\(r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1,...n\\)\n(B) Fit a regression tree to the \\(r_{im}\\) values and create terminal regions \\(R_{im}\\), for \\(j = 1,...,J_m\\)\n(C) For \\(j = 1,...,j_m\\) compute \\(r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)\\)\n(D) Update \\(F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\)\nStep 3 : Output \\(F_M(x)\\)"
  },
  {
    "objectID": "posts/2021-06-12-gradient-boosting-regression/gbm.html#details",
    "href": "posts/2021-06-12-gradient-boosting-regression/gbm.html#details",
    "title": "gradient boosting machine tutorial",
    "section": "Details",
    "text": "Details\n\nInput : Data \\(\\{x_i, y_i\\}^n_{i=1}\\), and a differentiable Loss function \\(L(y_i, F(x))\\)\n\n미분 가능한 loss function으로 GBM에서는 L2 norm을 선택한다. 이 때 \\(\\frac{1}{2}\\)는 계산상의 편의를 위해서 scaling constant이다.\nLoss function : \\(L(y_i, F(x)) = \\frac{1}{2} \\sum_{i=1}^n (y_i -F(x))^2\\)\n\nloss <- function(y, yhat){\n                return(mean(1/2*(y-yhat)^2))\n}\n\n\nStep 1 : Initialized model with a constant value: \\(F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\\[\n\\begin{align}\nF_0(x) &= \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma) \\newline\n&={d \\over d\\gamma} \\frac{1}{2} \\sum_{i=1}^n (y_i -\\gamma^2)\\newline\n&= -\\sum_{i=1}^n(y_i - \\gamma) \\newline\n&= 0 \\newline\n&\\Leftrightarrow \\hat{\\gamma} = \\bar{y}\n\\end{align}\n\\]\n초기값은 y의 평균으로 계산한다.\n\n\nCompute \\(r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1,...n\\)\n\n\\[\n\\begin{align}\nr_{im} &= -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}, \\quad i = 1,...n, \\quad m = \\#tree \\newline\n&\\Leftrightarrow r_{im} = y_i - F_{m-1}(x_i)\n\\end{align}\n\\]\n\\(r_{im}\\)은 negative gradient or pseudo residual이라고 한다. GBM은 residual을 기반으로 regression tree를 생성하는데 이 때 이용되는 residual 값이 \\(r_{im}\\)으로 계산된 pseudo residual이다.\n\n\nnegative_residual <- function(y, yhat) {\n                return(y - yhat)\n}\n\n\n\nFit a regression tree to the \\(r_{im}\\) values and create terminal regions \\(R_{im}\\), for \\(j = 1,...,J_m\\)\n\ntree의 깊이는 보통 8~32 정도로 구성된다. full tree가 아닌 weak learner or weak tree를 만들기 때문에 tree의 terminal regions \\(R_{im}\\) 에는 여러 개의 값이 존재할 수 있다.\n\nFor \\(j = 1,...,j_m\\) compute \\(r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)\\)\n\n이 때 tree의 terminal regions \\(R_{im}\\) 에 존재하는 여러 개의 값은 \\(r_{jm}\\) : terminal region의 평균으로 계산된다.\n\n\\[\n\\begin{align}\nr_{jm} &= \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma) \\newline\n       &=\\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}}\\frac{1}{2}(y_i - (F_{m-1}(x_i)+\\gamma)) \\newline\n       &\\Leftrightarrow -\\sum_{x_i \\in R_{ij}}(y_i - F_{m-1}(x_i)-\\gamma)) = 0 \\newline\n       &\\Leftrightarrow \\hat{\\gamma} := terminal\\; region의\\;평균\n\\end{align}\n\\]\n\n\nUpdate \\(F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\)\n\n\n완성된 tree는 \\(\\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})\\) 로 표현되며 learning rate \\(\\nu\\)를 이용해서 예측값에 대한 개별 tree의 영향력을 조절한다. \\(\\nu\\)가 작으면 개별 tree의 영향력이 줄어들고, 계산량이 많아지지만 accuracy는 향상된다. \\(\\nu\\)가 크면 개별 tree의 영향력이 커지고, 계산량이 상대적으로 적으며, accuracy가 상대적으로 줄어든다.\n\nFull code\n구글 서치 중에 OLS 기반으로 gradient boosting 수행하는 코드를 발견했는데 gradient boosting 알고리즘을 이해하는데 많은 도움이 되었다(실제 패키지에서는 regression tree 기반으로 계산되기 때문에 theta 값은 계산되지 않는다)\n\ngrad_boost <- function(formula, data, nu = 0.01, stop, \n                       grad.fun, loss.fun, yhat.init = 0) {\n  \n  data <- as.data.frame(data)\n  formula <- terms.formula(formula)\n  X <- model.matrix(formula, data)\n  \n  y <- data[, as.character(formula)[2]] # as.character(formula)[2] : formula y~.에서 y에 해당하는 명칭\n\n  fit <- yhat.init\n  \n  u <- grad.fun(y = y, yhat = fit) # pseudo residual 계산 \n  \n  theta <- rep(0, ncol(X))\n  \n  loss <- c()\n \n  for (i in 1:stop) {\n    \n    # Design matrix를 이용한 regression, OLS 기반, Tree X \n    base_prod <- lm.fit(x = X, y = u) \n    theta_i <- coef(base_prod)\n    \n    # theta 값 업데이트 \n    theta <- theta + nu*as.vector(theta_i)\n    \n    # yhat 값 업데이트\n    fit <- fit + nu * fitted(base_prod)\n    \n    # pseudo residual 계산\n    u <- grad.fun(y = y, yhat = fit)\n    \n    # loss 값 업데이트 \n    loss <- append(loss, loss.fun(y = y, yhat = fit))\n  }  \n  names(theta) <- colnames(X)\n  return(list(theta = theta, u = u, fit = fit, loss = loss, \n              formula = formula, data = data))\n}\n\nfit <- grad_boost(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, data = iris, stop = 1000, grad.fun = negative_residual, loss.fun = loss)\nfit$theta\n\n      (Intercept)       Sepal.Width      Petal.Length       Petal.Width \n        2.1711726         0.4958675         0.8292081        -0.3151416 \nSpeciesversicolor  Speciesvirginica \n       -0.7235307        -1.0234536 \n\n\n\n\nUsing GBM package\n패키지는 Tree 기반으로 계산되므로 고정된 coefficient 결과를 산출하지 않는다.\n대신에 feature importance 값으로 변수별 상대적인 영향력을 볼 수 있다.\n\n\nn.trees : tree의 갯수(the number of gradient boosting iteration), pseudo code에서 \\(M\\)에 해당\ninteraction.depth : tree당 최대 노드의 개수, 보통 8~32\nshringkage : learning rate(\\(\\nu\\))\nn.minobsinnode : terminal nodes의 최소 관찰값의 수\nbag.fraction (Subsampling fraction) : training set을 나눌 비율. 기본적으로 stochastic gradient boosting 전략 채택. default : 0.5\ntrain.fraction : 첫 train.fraction * nrows(data) 관찰값은 gbm fitting에 사용되고 나머지는 loss function에서의 out-of-sample 추정량을 계산하는데 사용됨. default = 1\ncv.folds : cross validation fold의 개수\nverbose : 모델 진행 상황을 모니터링할건지 유무\ndistribution : 분류 문제일 경우 - bernoulli, multinomial, regression 문제일 경우 - gaussian or tdist\n\n보통 bag.fraction, train.fraction은 따로 지정하지 않음.\n\nR code\n\nlibrary(gbm)\nfit_pack <- gbm(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, \n                data = iris, \n                verbose = T, \n                shrinkage = 0.01, \n                distribution = 'gaussian')\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.6732             nan     0.0100    0.0080\n     2        0.6663             nan     0.0100    0.0077\n     3        0.6580             nan     0.0100    0.0072\n     4        0.6502             nan     0.0100    0.0079\n     5        0.6426             nan     0.0100    0.0076\n     6        0.6357             nan     0.0100    0.0076\n     7        0.6285             nan     0.0100    0.0070\n     8        0.6206             nan     0.0100    0.0071\n     9        0.6137             nan     0.0100    0.0066\n    10        0.6079             nan     0.0100    0.0058\n    20        0.5428             nan     0.0100    0.0060\n    40        0.4444             nan     0.0100    0.0041\n    60        0.3683             nan     0.0100    0.0020\n    80        0.3145             nan     0.0100    0.0022\n   100        0.2734             nan     0.0100    0.0016\n\nsummary(fit_pack)\n\n\n\n\n                      var   rel.inf\nPetal.Length Petal.Length 96.855875\nPetal.Width   Petal.Width  3.144125\nSepal.Width   Sepal.Width  0.000000\nSpecies           Species  0.000000\n\n\n\npretty.gbm.tree(fit_pack, i = 1)\n\n  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n0        2   1.150000000        1         2           3        29.2772     75\n1       -1  -0.007754762       -1        -1          -1         0.0000     28\n2       -1   0.005162411       -1        -1          -1         0.0000     47\n3       -1   0.000340000       -1        -1          -1         0.0000     75\n    Prediction\n0  0.000340000\n1 -0.007754762\n2  0.005162411\n3  0.000340000\n\n\npretty.gbm.tree()를 이용하면 개별 tree를 적합할 때 진행상황을 모니터링할 수 있다. 여기서의 predict 값은 개별 tree에 대한 값이므로 pseudo residual 값에 해당한다(참고 3).\n\n\n\n참고 자료\n참고 1 : https://www.youtube.com/watch?v=2xudPOBz-vs&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6&index=2\n참고 2 : https://medium.com/@statworx_blog/coding-gradient-boosted-machines-in-100-lines-of-code-d06b1d7bc084\n참고 3 : https://stats.stackexchange.com/questions/237582/interpretation-of-gbm-single-tree-prediction-in-pretty-gbm-tree"
  },
  {
    "objectID": "posts/2021-10-02-docker/docker.html",
    "href": "posts/2021-10-02-docker/docker.html",
    "title": "Docker 소개",
    "section": "",
    "text": "docker 파일을 구울 때 사전에 패키지를 설치하고 싶으면 install.packages를 이용하면 되는데, 이 경우 너무 많은 시간이 걸린다. docker 파일을 굽고 서버에서 install.packages를 해도 마찬가지다. 해결책을 찾아보니 pre-compiled binary package를 이용하면 설치 시간을 10배 이상 단축할 수 있는 것 같다.\n잠시 운영체제에 대한 설명으로 넘어가면 linux에서는 package 관리 방법이 두가지가 있는데 source package와 binary package다. source package는 소스 코드가 있는 패키지로 컴파일 과정을 통해 binary package로 만드는 과정을 거쳐야만 시행될 수 있다. 설치할 때 컴파일 과정도 함께 진행되어야 하므로 설치 시간이 길고, 컴파일 작업 과정에서 오류가 생길 수 있다. 반면 binary package는 컴파일이 완료된 바이너리 파일이 들어있는 패키지이다. 사전에 컴파일이 되어 있으므로, 소스 패키지에 비해 설치 시간이 짧고, 오류가 발생될 가능성이 적다.\n대부분 바이너리 패키지를 이용하지만 소프트웨어를 원하는데로 수정하고 싶을 때는 소스 패키지를 이용한다. 바이너리 패키지는 설치시간이 비교적 짧지만 바이너리 패키지를 실행하기 위해서는 다른 특정 패키지가 필요할 수 있는데, 이를 패키지 의존성이라고 한다.\n패키지 의존성을 해결해주는, 즉 패키지 간의 연결관계를 파악하고 자동으로 필요한 패키지를 설치해주는 도구가 존재하는데 apt-get, apt 등이 있다.\n다시 R로 넘어가서 linux에 대한 이해를 바탕으로 유추를 해보면 docker 파일을 구울 때 install.packages는 source package에 해당하고, pre-compiled binary package는 binary package를 의미하는 것 같다. 즉 binary package의 장점을 이용해서 속도가 빨라지는 것이라고 이해하면 될 것 같다."
  },
  {
    "objectID": "posts/2021-10-02-docker/docker.html#reference",
    "href": "posts/2021-10-02-docker/docker.html#reference",
    "title": "Docker 소개",
    "section": "Reference",
    "text": "Reference\nlinux package 설명\nhttps://bradbury.tistory.com/227?category=768468\ndocker tutorial\nhttps://jsta.github.io/r-docker-tutorial/\ndocker build 명령어 요약\nhttps://blog.d0ngd0nge.xyz/docker-dockerfile-write/\nrocker에서 package 빠르게 설치해서 굽는 법\nhttps://stackoverflow.com/questions/51500385/how-to-speed-up-r-packages-installation-in-docke\nr-base vs r-apt 속도 비교\nhttps://datawookie.dev/blog/2019/01/docker-images-for-r-r-base-versus-r-apt/"
  },
  {
    "objectID": "posts/network/network.html",
    "href": "posts/network/network.html",
    "title": "networkx",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#example",
    "href": "posts/sensmakr/sensmakr.html#example",
    "title": "sensemakr",
    "section": "Example",
    "text": "Example\n\n\\(D\\) : treatment variable\n\\(X\\) : observed covariates\n\\(Z\\) : unobserved covariates\n\n\\[\n\\begin{align*}\nY = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}\n\\end{align*}\n\\]\nFrisch-waugh-Lovell theorem을 적용하면 \\(Y = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}\\)에 FWL을 적용하면\n\n\\(Y ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Y^{\\perp X}\\)\n\\(D ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(D^{\\perp X}\\)\n\\(Z ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Z^{\\perp X}\\)\n\\(Y^{\\perp X} \\sim D^{\\perp X} + Z^{\\perp X}\\) \\(\\longrightarrow\\) \\(\\hat{\\tau}, \\, \\hat{\\gamma}\\)\n\n\\(Y^{\\perp X} = \\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X}\\)\n\n\n\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    Z = rnorm(N, 1),\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\nZperpX <- lm(Z ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX, ZperpX)\n\nprint(coef(lm(YperpX ~ DperpX+ZperpX, resid_df))[c(2, 3)], digits = 2)\n\n  DperpX   ZperpX \n-0.00072  0.01351 \n\nprint(coef(lm(Y~D+X+Z, df))[c(2, 4)], digits = 2)\n\n       D        Z \n-0.00072  0.01351 \n\n\n\\[\n\\begin{align*}\nY = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}\n\\end{align*}\n\\]\n\\(Y = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}\\)에 FWL을 적용하면\n\n\\(y ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(Y^{\\perp X}\\)\n\\(D ~ \\sim X\\)의 잔차 \\(\\longrightarrow\\) \\(D^{\\perp X}\\)\n\\(Y^{\\perp X} \\sim D^{\\perp X}\\) \\(\\longrightarrow\\) \\(\\hat{\\tau}_{res}\\)\n\n\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX)\n\nprint(coef(lm(YperpX ~ DperpX, resid_df))[2], digits = 2)\n\nDperpX \n0.0078 \n\nprint(coef(lm(Y~D+X, df))[2], digits = 2)\n\n     D \n0.0078"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#the-traditional-omitted-variable-bias",
    "href": "posts/sensmakr/sensmakr.html#the-traditional-omitted-variable-bias",
    "title": "sensemakr",
    "section": "3.1 The traditional omitted variable bias",
    "text": "3.1 The traditional omitted variable bias\n\\[\n\\begin{align*}\n\\hat{\\tau}_{res} &= \\frac{cov(D^{\\perp X}, Y^{\\perp X})}{var(D^{\\perp X})} \\\\\n&= \\frac{cov(D^{\\perp X},\\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X})}{var(D^{\\perp X})} \\\\\n&= \\frac{\\hat{\\tau}cov(D^{\\perp X},D^{\\perp X}) + \\hat{\\gamma}cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})} \\\\\n&=\\hat{\\tau} + \\hat{\\gamma} \\cdot \\hat{\\delta}, \\quad \\hat{\\delta} = \\frac{cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})}, \\quad \\hat{\\gamma} = \\frac{cov(Y^{\\perp X, D}, Z^{\\perp X, D})}{var(Z^{\\perp X, D})}\n\\end{align*}\n\\]\n따라서 unobserved confounder에 의한 추정량의 bias는 다음과 같다.\n\\[\n\\begin{align*}\n\\hat{bias} = \\hat{\\tau}_{res} - \\hat{\\tau} =  \\hat{\\gamma} \\cdot \\hat{\\delta}\n\\end{align*}\n\\]\n\\(Z\\)는 unobserved confounder이므로 \\(\\hat{\\gamma}, \\, \\hat{\\delta}\\)의 부호를 알 수 없다. 따라서 unobserved confounder의 추정량에 영향을 미치는 크기를 고려해야 한다. 즉, 연구의 주요 결론에 영향을 줄 정도로 추정량을 변경하려면 unobserved confounder \\(Z\\)의 효과는 어느 정도 크기여야 하는가?\n이는 sensitivity analysis를 통해 파악할 수 있다."
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#contour-plot",
    "href": "posts/sensmakr/sensmakr.html#contour-plot",
    "title": "sensemakr",
    "section": "contour plot",
    "text": "contour plot"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#ovb-with-the-partial-r2-parameterization",
    "href": "posts/sensmakr/sensmakr.html#ovb-with-the-partial-r2-parameterization",
    "title": "sensemakr",
    "section": "OVB with the partial \\(R^2\\) parameterization",
    "text": "OVB with the partial \\(R^2\\) parameterization\n\\[\n\\begin{align*}\n\\hat{bias} &= \\hat{\\gamma} \\cdot \\hat{\\delta} \\\\\n&=\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}} \\cdot \\frac{sd(Y^{\\perp X,D})}{sd(D^{\\perp X})} \\\\\n&=\\hat{se}(\\hat{\\tau}_{res})\\cdot\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}}(df)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/sensmakr/sensmakr.html#robustness-value",
    "href": "posts/sensmakr/sensmakr.html#robustness-value",
    "title": "sensemakr",
    "section": "Robustness value",
    "text": "Robustness value\n\\[\n\\begin{align*}\nRV_q = \\frac{1}{2}(\\sqrt{f^4_q + 4f^2_q - f^2_q})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2021-06-26-create-logo/hexsticker.html",
    "href": "posts/2021-06-26-create-logo/hexsticker.html",
    "title": "hexSticker",
    "section": "",
    "text": "remotes::install_github(\"GuangchuangYu/hexSticker\")\n\n\nlibrary(hexSticker)\nlibrary(ggplot2)\nlibrary(ggdag)\n\n\n다음의 패키지를 부착합니다: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.3\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\nhexSticker는 R 패키지 로고를 쉽게 만들어주는 패키지이다. 육각형 패키지 로고 안에 글씨, 이미지, url 등을 넣을 수 있으며, 색, 글씨체, 글씨크기, 배경 색, 밝기 등 다양한 커스텀이 가능하다.\n\n\nggplot2 base의 그래프의 경우 별도의 이미지 저장 없이 hexSticker에서 사용이 가능하다. ggplot base가 아닐 경우 이미지로 저장한 뒤 불러와서 사용해야 한다.\nggdag()는 ggplot base인 것처럼 보이지만 hexSticker에서 ggplot base로 인식을 안하기 때문에 별도의 이미지로 저장해서 사용해야 한다.\n\ntidy_ggdag <- dagify(\n  y ~ x + z2 + w2 + w1,\n  x ~ z1 + w1 + w2,\n  z1 ~ w1 + v,\n  z2 ~ w2 + v,\n  w1 ~~ w2, # bidirected path\n  exposure = \"x\",\n  outcome = \"y\"\n) %>% \n  tidy_dagitty()\n\ntidy_ggdag\n\n# A DAG with 7 nodes and 12 edges\n#\n# Exposure: x\n# Outcome: y\n#\n# A tibble: 13 × 8\n   name      x     y direction to     xend   yend circular\n   <chr> <dbl> <dbl> <fct>     <chr> <dbl>  <dbl> <lgl>   \n 1 v     -2.45 1.20  ->        z1    -3.16  2.42  FALSE   \n 2 v     -2.45 1.20  ->        z2    -3.68  0.487 FALSE   \n 3 w1    -4.10 2.07  ->        x     -4.64  2.40  FALSE   \n 4 w1    -4.10 2.07  ->        y     -4.39  1.16  FALSE   \n 5 w1    -4.10 2.07  ->        z1    -3.16  2.42  FALSE   \n 6 w1    -4.10 2.07  <->       w2    -4.99  1.29  FALSE   \n 7 w2    -4.99 1.29  ->        x     -4.64  2.40  FALSE   \n 8 w2    -4.99 1.29  ->        y     -4.39  1.16  FALSE   \n 9 w2    -4.99 1.29  ->        z2    -3.68  0.487 FALSE   \n10 x     -4.64 2.40  ->        y     -4.39  1.16  FALSE   \n11 z1    -3.16 2.42  ->        x     -4.64  2.40  FALSE   \n12 z2    -3.68 0.487 ->        y     -4.39  1.16  FALSE   \n13 y     -4.39 1.16  <NA>      <NA>  NA    NA     FALSE   \n\nggdag(tidy_ggdag) +\n  theme_dag()\n\n\n\n\n저장된 이미지를 불러올 경우 magick 패키지의 image_read()를 이용할 수 있다.\n\nimg <- image_read('Rplot.png')\nimg\n\n\n\n\n불러온 이미지는 hexSticker의 sticker() 함수를 이용하면 간단하게 패키지 로고를 만들 수 있다. 여러가지 옵션이 있는데, 기호에 맞게 커스텀해서 이용하면 된다.\n\nsticker(subplot = img, # 저장된 image 불러오기 \n        package = \"causal inference\", # 패키지로 사용할 명칭 \n        s_width = 0.9, # subplot width\n        s_height = 1, # subplot height\n        s_x = 1, # subplot left/right position \n        s_y = 0.75, # subplot up/down position\n        p_size = 18, # package name font size \n        h_fill = 'steelblue', # \n        h_color = 'orange',\n        h_size = 4, # boundary size \n        url = \"https://pseudolabcausalinference.tistory.com\",\n        u_size = 4, # url size \n        spotlight = T, # spotlight 넣기 \n        l_y = 1, # spotlight y-position\n        l_x = 1, # spotlight x-position\n        l_width = 3, # spotlight width\n        l_height = 3, # spotlight height\n        l_alpha = 0.3, # splotlight level \n        u_color = 'white' \n       #filename=\"logo.png\"\n        )%>% \n        print()\n\n\n\n\n\n\n\nhttps://github.com/GuangchuangYu/hexSticker\nhttps://www.youtube.com/watch?v=O34vzdHOaEk&t=132s"
  }
]