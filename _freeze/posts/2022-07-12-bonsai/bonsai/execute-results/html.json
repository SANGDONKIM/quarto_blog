{
  "hash": "a96781b976a467bc264d3c8f970eeffc",
  "result": {
    "markdown": "---\ntitle: \"bonsai and treesnip\"\ndescription: |\n  R lightgbm, xgboost, catboost <ea><b4><80><eb><a0><a8> <ed><8c><a8><ed><82><a4><ec><a7><80>\nauthor: \"Don Don\"\ndate: \"2022-07-01\"\ncategories: [R, ligthtgbm, xgboost, catboost]\nimage: \"https://bonsai.tidymodels.org/logo.png\"\n---\n\n\n## Library\n\ntidymodels<ec><97><90><ec><84><9c> xgboost<ec><9d><98> <ea><b2><bd><ec><9a><b0> <ea><b3><b5><ec><8b><9d> <ec><a7><80><ec><9b><90><ed><95><9c><eb><8b><a4>. <eb><8b><a4><eb><a7><8c> <eb><a7><8e><ec><9d><b4> <ec><82><ac><ec><9a><a9><eb><90><98><eb><8a><94> lightgbm, catboost<ec><9d><98> <ea><b2><bd><ec><9a><b0> <ea><b3><b5><ec><8b><9d> <ec><a7><80><ec><9b><90><ed><95><98><ec><a7><80> <ec><95><8a><eb><8a><94><eb><8b><a4>. <eb><94><b0><eb><9d><bc><ec><84><9c> CRAN<ec><97><90> <eb><93><b1><eb><a1><9d><eb><90><98><ec><96><b4><ec><9e><88><ec><a7><80> <ec><95><8a><ec><9d><80> treesnip <ed><8c><a8><ed><82><a4><ec><a7><80><eb><a5><bc> <ec><84><a4><ec><b9><98><ed><95><b4><ec><84><9c> <ec><82><ac><ec><9a><a9><ed><96><88><ec><96><b4><ec><95><bc> <ed><96><88><eb><8b><a4>. <ec><b5><9c><ea><b7><bc><ec><97><90> treesnip <ed><8c><a8><ed><82><a4><ec><a7><80> <ea><b0><9c><eb><b0><9c><ec><9d><84> <eb><a9><88><ec><b6><94><ea><b3><a0> bonsai <ed><8c><a8><ed><82><a4><ec><a7><80><eb><a5><bc> <ec><83><88><eb><a1><ad><ea><b2><8c> <eb><a7><8c><eb><93><a4><ec><96><b4><ec><84><9c> <ea><b0><9c><eb><b0><9c> <ec><a4><91><ec><9d><b8> <ea><b2><83> <ea><b0><99><eb><8b><a4>. bonsai <ed><8c><a8><ed><82><a4><ec><a7><80> <ec><82><ac><ec><9a><a9><eb><b2><95><ec><97><90> <eb><8c><80><ed><95><b4> <ed><95><9c><eb><b2><88> <ec><a0><95><eb><a6><ac><eb><a5><bc> <ed><95><b4><eb><b3><b8><eb><8b><a4>. bonsai <ed><8c><a8><ed><82><a4><ec><a7><80><ec><9d><98> <ea><b2><bd><ec><9a><b0> lightgbm<ec><9d><80> <ec><a7><80><ec><9b><90><ed><95><98><ec><a7><80><eb><a7><8c> catboost<eb><8a><94> <ec><a7><80><ec><9b><90><ed><95><98><ec><a7><80> <ec><95><8a><ea><b8><b0> <eb><95><8c><eb><ac><b8><ec><97><90>, treesnip <ed><8c><a8><ed><82><a4><ec><a7><80><eb><a5><bc> <ed><99><9c><ec><9a><a9><ed><95><b4><ec><84><9c> catboost <eb><aa><a8><ed><98><95><ec><9d><84> <ec><a0><81><ed><95><a9><ed><95><98><eb><8a><94> <eb><b0><a9><eb><b2><95><ec><97><90> <eb><8c><80><ed><95><b4> <ec><a0><95><eb><a6><ac><ed><95><b4><eb><b3><b8><eb><8b><a4>. bonsai, treesnip <ed><8c><a8><ed><82><a4><ec><a7><80><ec><9d><98> <ea><b2><bd><ec><9a><b0> tidymodels <ec><9b><8c><ed><81><ac><ed><94><8c><eb><a1><9c><ec><9a><b0><ec><97><90> <ed><95><b4><eb><8b><b9> <eb><aa><a8><eb><8d><b8><ec><9d><b4> <ec><b6><94><ea><b0><80><eb><90><9c> <ed><98><95><ed><83><9c><ec><9d><b4><eb><af><80><eb><a1><9c> tidymodels<ec><9d><98> <ec><9b><8c><ed><81><ac><ed><94><8c><eb><a1><9c><ec><9a><b0><eb><a5><bc> <ea><b7><b8><eb><8c><80><eb><a1><9c> <eb><94><b0><eb><9d><bc><ea><b0><84><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"pak\")\n#install.packages(\"lightgbm\")\n#pak::pak(\"tidymodels/bonsai\")\n#remotes::install_github(\"curso-r/treesnip@catboost\")\n#remotes::install_github(\"curso-r/treesnip\")\n\n\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv broom        1.0.0     v recipes      1.0.1\nv dials        1.0.0     v rsample      1.0.0\nv dplyr        1.0.9     v tibble       3.1.7\nv ggplot2      3.3.6     v tidyr        1.2.0\nv infer        1.0.2     v tune         0.2.0\nv modeldata    1.0.0     v workflows    1.0.0\nv parsnip      1.0.0     v workflowsets 0.2.1\nv purrr        0.3.4     v yardstick    1.0.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n* Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(bonsai)\nlibrary(skimr)\nlibrary(treesnip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'treesnip':\n  method                     from  \n  multi_predict._lgb.Booster bonsai\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'treesnip'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:bonsai':\n\n    predict_lightgbm_classification_class,\n    predict_lightgbm_classification_prob,\n    predict_lightgbm_classification_raw,\n    predict_lightgbm_regression_numeric, train_lightgbm\n```\n:::\n\n```{.r .cell-code}\nlibrary(catboost)\n```\n:::\n\n\n## Data description\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- MASS::Boston\ndat %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 506\nColumns: 14\n$ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,~\n$ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1~\n$ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.~\n$ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,~\n$ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,~\n$ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9~\n$ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505~\n$ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,~\n$ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31~\n$ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15~\n$ black   <dbl> 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90~\n$ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10~\n$ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15~\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(dat)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |dat  |\n|Number of rows           |506  |\n|Number of columns        |14   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |14   |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|     p0|    p25|    p50|    p75|   p100|hist                                     |\n|:-------------|---------:|-------------:|------:|------:|------:|------:|------:|------:|------:|:----------------------------------------|\n|crim          |         0|             1|   3.61|   8.60|   0.01|   0.08|   0.26|   3.68|  88.98|<U+2587><U+2581><U+2581><U+2581><U+2581> |\n|zn            |         0|             1|  11.36|  23.32|   0.00|   0.00|   0.00|  12.50| 100.00|<U+2587><U+2581><U+2581><U+2581><U+2581> |\n|indus         |         0|             1|  11.14|   6.86|   0.46|   5.19|   9.69|  18.10|  27.74|<U+2587><U+2586><U+2581><U+2587><U+2581> |\n|chas          |         0|             1|   0.07|   0.25|   0.00|   0.00|   0.00|   0.00|   1.00|<U+2587><U+2581><U+2581><U+2581><U+2581> |\n|nox           |         0|             1|   0.55|   0.12|   0.38|   0.45|   0.54|   0.62|   0.87|<U+2587><U+2587><U+2586><U+2585><U+2581> |\n|rm            |         0|             1|   6.28|   0.70|   3.56|   5.89|   6.21|   6.62|   8.78|<U+2581><U+2582><U+2587><U+2582><U+2581> |\n|age           |         0|             1|  68.57|  28.15|   2.90|  45.02|  77.50|  94.07| 100.00|<U+2582><U+2582><U+2582><U+2583><U+2587> |\n|dis           |         0|             1|   3.80|   2.11|   1.13|   2.10|   3.21|   5.19|  12.13|<U+2587><U+2585><U+2582><U+2581><U+2581> |\n|rad           |         0|             1|   9.55|   8.71|   1.00|   4.00|   5.00|  24.00|  24.00|<U+2587><U+2582><U+2581><U+2581><U+2583> |\n|tax           |         0|             1| 408.24| 168.54| 187.00| 279.00| 330.00| 666.00| 711.00|<U+2587><U+2587><U+2583><U+2581><U+2587> |\n|ptratio       |         0|             1|  18.46|   2.16|  12.60|  17.40|  19.05|  20.20|  22.00|<U+2581><U+2583><U+2585><U+2585><U+2587> |\n|black         |         0|             1| 356.67|  91.29|   0.32| 375.38| 391.44| 396.22| 396.90|<U+2581><U+2581><U+2581><U+2581><U+2587> |\n|lstat         |         0|             1|  12.65|   7.14|   1.73|   6.95|  11.36|  16.96|  37.97|<U+2587><U+2587><U+2585><U+2582><U+2581> |\n|medv          |         0|             1|  22.53|   9.20|   5.00|  17.02|  21.20|  25.00|  50.00|<U+2582><U+2587><U+2585><U+2581><U+2581> |\n:::\n:::\n\n\n## train/test split\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplits <- initial_split(dat, prop = 0.7)\n```\n:::\n\n\n## data preprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(medv ~ ., training(splits))\n```\n:::\n\n\n## Catboost\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_mod <- boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune()) %>%\n  set_engine(engine = \"catboost\" \n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(cat_mod)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(),  size = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- tune_grid(\n    cat_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 9\n  min_n tree_depth learn_rate .metric .estimator  mean     n std_err .config    \n  <int>      <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      \n1    27          8 0.0703     rmse    standard    3.65     5   0.273 Preprocess~\n2    31         11 0.00798    rmse    standard    4.70     5   0.240 Preprocess~\n3    22          5 0.00125    rmse    standard    7.58     5   0.391 Preprocess~\n4     7          9 0.0000988  rmse    standard    9.08     5   0.489 Preprocess~\n5    35         12 0.00000379 rmse    standard    9.23     5   0.497 Preprocess~\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(cat_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n0 Recipe Steps\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 27\n  tree_depth = 8\n  learn_rate = 0.0702985501331971\n\nComputational engine: catboost \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_cat <- \n    predict(cat_fit, testing(splits)) %>% \n    mutate(modelo = \"cat\")\n\n\nresult3 <- data.frame(pred = pred_cat$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result3, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       2.45 \n2 rsq     standard       0.928\n3 mae     standard       1.78 \n```\n:::\n:::\n\n\n## lightgbm\n\ntidymodels<ec><97><90><ec><84><9c> lightgbm<ec><9d><84> <ec><82><ac><ec><9a><a9><ed><95><a0> <ea><b2><bd><ec><9a><b0> `set_engine()`<ec><97><90> ligthtgbm<ec><9d><84> <eb><aa><85><ec><8b><9c><ed><95><b4><ec><a3><bc><eb><a9><b4> <eb><90><9c><eb><8b><a4>. cuda gpu<eb><a5><bc> <ec><82><ac><ec><9a><a9><ed><95><a0> <ea><b2><bd><ec><9a><b0> `params=list(tree_method = 'gpu_hist')`<eb><a5><bc> <eb><aa><85><ec><8b><9c><ed><95><b4><ec><a4><98><ec><95><bc> <ed><95><9c><eb><8b><a4>. <eb><98><90><ed><95><9c> CRAN<ec><97><90> <eb><93><b1><eb><a1><9d><eb><90><98><ec><96><b4><ec><9e><88><eb><8a><94> lightgbm <ed><8c><a8><ed><82><a4><ec><a7><80><ea><b0><80> <ec><95><84><eb><8b><8c> gpu <ec><98><b5><ec><85><98><ec><9d><84> <ec><a7><80><ec><9b><90><ed><95><98><eb><8a><94> lightgbm<ec><9d><84> <eb><94><b0><eb><a1><9c> <ec><84><a4><ec><b9><98><ed><95><b4><ec><a4><98><ec><95><bc><ed><95><9c><eb><8b><a4>(<ec><b0><b8><ea><b3><a0> : <https://cran.r-project.org/web/packages/lightgbm/readme/README.html#installing-a-gpu-enabled-build>). <ec><9d><b4> <ea><b2><bd><ec><9a><b0> Docker file<ec><9d><84> <eb><8b><a4><ec><9a><b4><eb><b0><9b><ec><95><84><ec><84><9c> <ec><84><a4><ec><b9><98><ed><95><98><eb><8a><94> <ea><b2><83><ec><9d><b4> <ec><98><a4><eb><a5><98><ea><b0><80> <eb><82><a0> <ea><b0><80><eb><8a><a5><ec><84><b1><ec><9d><b4> <ec><a0><81><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbt_mod <- \n  boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune(),\n        loss_reduction = tune(),\n        stop_iter = tune()) %>%\n  set_engine(engine = \"lightgbm\" \n             #params=list(tree_method = 'gpu_hist')\n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n\nR lightgbm <ea><b3><b5><ec><8b><9d> <eb><ac><b8><ec><84><9c><eb><a5><bc> <eb><b3><b4><eb><a9><b4> <ec><b6><94><ea><b0><80><ec><a0><81><ec><9d><b8> parameter<ea><b0><80> <eb><a7><8e><ec><9d><80><eb><8d><b0>, <ec><b6><94><ea><b0><80><ed><95><98><ea><b3><a0> <ec><8b><b6><ec><9d><80> parameter<eb><8a><94> set_engine<ec><97><90> <ed><95><b4><eb><8b><b9> parameter<eb><a5><bc> <ec><b6><94><ea><b0><80><ed><95><98><eb><a9><b4> <eb><90><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlight_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(bt_mod)\n```\n:::\n\n\n<ec><9b><8c><ed><81><ac><ed><94><8c><eb><a1><9c><ec><9a><b0><ec><97><90> <ec><a0><84><ec><b2><98><eb><a6><ac> recipe<ec><99><80> <ec><a0><95><ec><9d><98><ed><95><9c> <eb><aa><a8><eb><8d><b8><ec><9d><84> <ec><97><85><eb><8d><b0><ec><9d><b4><ed><8a><b8><ed><95><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n\nvalidation set<ec><9d><84> <ec><a7><80><ec><a0><95><ed><95><9c><eb><8b><a4>. fold=5<ec><9d><b8> cross validation <eb><b0><a9><ec><8b><9d><ec><9c><bc><eb><a1><9c> <ec><84><a4><ec><a0><95><ed><95><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)\n```\n:::\n\n\ntuning <eb><b0><a9><ec><8b><9d><ec><9d><84> <ec><a0><95><ec><9d><98><ed><95><98><eb><8a><94><eb><8d><b0> latine hypercube <eb><b0><a9><ec><8b><9d><ec><9d><84> <ec><9d><b4><ec><9a><a9><ed><96><88><eb><8b><a4>. tuning <eb><b0><a9><ec><8b><9d><ec><9d><80> tidymodels <ea><b3><b5><ec><8b><9d><eb><ac><b8><ec><84><9c><eb><a5><bc> <ec><b0><b8><ea><b3><a0><ed><95><98><eb><a9><b4> <eb><8b><a4><ec><96><91><ed><95><9c> <eb><b0><a9><eb><b2><95><ec><9d><84> <ec><a0><9c><ea><b3><b5><ed><95><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doFuture)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: future\n```\n:::\n\n```{.r .cell-code}\nregisterDoFuture()\ncl <- parallel::makeCluster(6)\nplan(cluster, workers = cl)\n```\n:::\n\n\n<eb><b3><91><eb><a0><ac> <ec><b2><98><eb><a6><ac> <ec><98><b5><ec><85><98><ec><9d><98> <ea><b2><bd><ec><9a><b0> doFuture <ed><8c><a8><ed><82><a4><ec><a7><80><eb><a5><bc> <ec><9d><b4><ec><9a><a9><ed><96><88><eb><8b><a4>. <ed><95><b4><eb><8b><b9> <ec><bb><b4><ed><93><a8><ed><84><b0><ec><9d><98> <ec><bd><94><ec><96><b4> <ec><88><98><eb><a5><bc> <ed><99><95><ec><9d><b8><ed><95><98><ea><b3><a0> <ec><a0><81><ec><a0><88><ed><95><98><ea><b2><8c> <ec><84><b8><ed><8c><85><ed><95><b4><ec><a3><bc><eb><a9><b4> <eb><90><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- tune_grid(\n    light_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE, allow_par = T))\n```\n:::\n\n\n`tune_grid()`<eb><a5><bc> <ec><9d><b4><ec><9a><a9><ed><95><b4><ec><84><9c>, workflow, validation set, metric, <ec><98><88><ec><b8><a1><ea><b0><92> <ec><a0><80><ec><9e><a5> <ec><98><b5><ec><85><98> <eb><93><b1><ec><9d><84> <ec><84><a4><ec><a0><95><ed><95><98><ea><b3><a0>, <ed><8c><8c><eb><9d><bc><eb><af><b8><ed><84><b0> <ed><8a><9c><eb><8b><9d><ec><9d><84> <ec><a7><84><ed><96><89><ed><95><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 11\n  min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean\n  <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl>\n1    34          5  0.0158     0.000183              5 rmse    standard    3.92\n2    27          7  0.00210    0.00000126           16 rmse    standard    4.59\n3    18          3  0.000239   0.00000000667        10 rmse    standard    7.94\n4    17         14  0.0000300  0.0000274             7 rmse    standard    9.09\n5     6         13  0.0000199  1.05                  4 rmse    standard    9.15\n# ... with 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n\n`show_best()`<eb><a5><bc> <ed><86><b5><ed><95><b4> tuning <ed><9b><84> <ec><b5><9c><ec><a0><81><ec><9d><98> <ed><8c><8c><eb><9d><bc><eb><af><b8><ed><84><b0><eb><a5><bc> <ed><99><95><ec><9d><b8><ed><95><a0> <ec><88><98> <ec><9e><88><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(light_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n0 Recipe Steps\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 34\n  tree_depth = 5\n  learn_rate = 0.0158299029849725\n  loss_reduction = 0.000183367712483135\n  stop_iter = 5\n\nComputational engine: lightgbm \n```\n:::\n:::\n\n\n<ec><b5><9c><ec><a0><81><ec><9d><98> <ed><8c><8c><eb><9d><bc><eb><af><b8><ed><84><b0><eb><a5><bc> <ec><84><a0><ed><83><9d><ed><95><9c> <ed><9b><84> `finalize_workflow()`<eb><a5><bc> <ec><84><a0><ed><83><9d><eb><90><9c> <ec><b5><9c><ec><a0><81><ec><9d><98> <ed><8c><8c><eb><9d><bc><eb><af><b8><ed><84><b0><eb><a5><bc> workflow<ec><97><90> <ec><97><85><eb><8d><b0><ec><9d><b4><ed><8a><b8> <ec><8b><9c><ec><bc><9c><ec><a4><80><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlight_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n\n<ec><b5><9c><ec><a0><81><ec><9d><98> <ed><8c><8c><eb><9d><bc><eb><af><b8><ed><84><b0><eb><a5><bc> <ec><97><85><eb><8d><b0><ec><9d><b4><ed><8a><b8><ed><95><9c> <eb><aa><a8><eb><8d><b8><ec><9d><84> <ed><9b><88><eb><a0><a8> <eb><8d><b0><ec><9d><b4><ed><84><b0><eb><a5><bc> <ec><9d><b4><ec><9a><a9><ed><95><b4><ec><84><9c> <ed><95><99><ec><8a><b5><ec><9d><84> <ec><a7><84><ed><96><89><ed><95><b4><ec><a4><80><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_light <- \n    predict(light_fit, testing(splits)) %>% \n    mutate(modelo = \"lightgbm\")\n\npred_light %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  .pred modelo  \n  <dbl> <chr>   \n1  26.6 lightgbm\n2  36.0 lightgbm\n3  31.9 lightgbm\n4  19.5 lightgbm\n5  17.6 lightgbm\n6  21.9 lightgbm\n```\n:::\n\n```{.r .cell-code}\nresult1 <- data.frame(pred = pred_light$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n\n<eb><a7><88><ec><a7><80><eb><a7><89><ec><9c><bc><eb><a1><9c> test <eb><8d><b0><ec><9d><b4><ed><84><b0><ec><85><8b><ec><97><90> <ed><95><99><ec><8a><b5><ec><9d><84> <ed><95><b4><ec><84><9c> <ec><98><88><ec><b8><a1><ea><b0><92><ec><9d><84> <ec><82><b0><ec><b6><9c><ed><95><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result1, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       2.89 \n2 rsq     standard       0.896\n3 mae     standard       2.18 \n```\n:::\n:::\n\n\nyardstick <ed><8c><a8><ed><82><a4><ec><a7><80><ec><9d><98> `metrics()`<ec><9d><98> <ea><b2><bd><ec><9a><b0> <ed><9a><8c><ea><b7><80>, <eb><b6><84><eb><a5><98> <ec><84><b8><ed><8c><85><ec><97><90> <eb><a7><9e><ec><b6><b0><ec><84><9c> <ec><a0><81><ec><a0><88><ed><95><9c> <ed><8f><89><ea><b0><80><ec><a7><80><ed><91><9c><eb><a5><bc> <eb><b3><b4><ec><97><ac><ec><a4><80><eb><8b><a4>. `metrics()`<ec><9d><98> <ea><b2><bd><ec><9a><b0> <ec><98><88><ec><b8><a1><ea><b0><92><ea><b3><bc> <ec><8b><a4><ec><a0><9c><ea><b0><92><ec><9d><b4> <ed><8f><ac><ed><95><a8><eb><90><9c> <eb><8d><b0><ec><9d><b4><ed><84><b0><ec><85><8b><ec><9d><84> <eb><84><a3><ec><96><b4><ec><a4><98><ec><95><bc> <ec><9e><91><eb><8f><99><ed><95><9c><eb><8b><a4>.\n\n## XGBOOST\n\nxgboost<ec><9d><98> <ea><b2><bd><ec><9a><b0> tidymodels<ec><97><90><ec><84><9c> <ea><b3><b5><ec><8b><9d> <ec><a7><80><ec><9b><90><ed><95><98><ea><b8><b0> <eb><95><8c><eb><ac><b8><ec><97><90>, tidymodels <ec><9b><8c><ed><81><ac><ed><94><8c><eb><a1><9c><ec><9a><b0><ec><97><90> <eb><a7><9e><ec><b6><b0><ec><84><9c> <ea><b7><b8><eb><8c><80><eb><a1><9c> <ec><82><ac><ec><9a><a9><ed><95><98><eb><a9><b4> <eb><90><9c><eb><8b><a4>. <eb><8c><80><ec><8b><a0> gpu<eb><a5><bc> <ec><82><ac><ec><9a><a9><ed><95><a0> <ea><b2><bd><ec><9a><b0> lightgbm<ea><b3><bc> <eb><a7><88><ec><b0><ac><ea><b0><80><ec><a7><80><eb><a1><9c> gpu<ea><b0><80> <ec><a7><80><ec><9b><90><eb><90><98><eb><8a><94> xgboost <eb><b2><84><ec><a0><84><ec><9d><84> <eb><94><b0><eb><a1><9c> <ec><84><a4><ec><b9><98><ed><95><b4><ec><a4><98><ec><95><bc> <ed><95><9c><eb><8b><a4>(<ec><b0><b8><ea><b3><a0> : <https://github.com/dmlc/xgboost/issues/6654#issuecomment-813828671>). lightgbm<ea><b3><bc> <eb><8f><99><ec><9d><bc><ed><95><98><ea><b2><8c> gpu<ea><b0><80> <ec><a7><80><ec><9b><90><eb><90><98><eb><8a><94> xgboost<eb><a5><bc> <ec><84><a4><ec><b9><98><ed><96><88><ec><9d><84> <ea><b2><bd><ec><9a><b0> `params=list(tree_method = 'gpu_hist')` <ec><98><b5><ec><85><98><ec><9d><84> <eb><84><a3><ec><96><b4><ec><a3><bc><eb><a9><b4> <eb><90><9c><eb><8b><a4>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_mod <- boost_tree(\n        trees = 1000,\n        min_n = tune(),\n        tree_depth = tune(),\n        learn_rate = tune(),\n        loss_reduction = tune(),\n        stop_iter = tune()) %>%\n  set_engine(engine = \"xgboost\" \n             #params=list(tree_method = 'gpu_hist')\n             ) %>%\n  set_mode(mode = \"regression\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow() %>% \n    add_recipe(rec) %>% \n    add_model(xgb_mod)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nvb_folds <- vfold_cv(training(splits), v = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doFuture)\nregisterDoFuture()\ncl <- parallel::makeCluster(6)\nplan(cluster, workers = cl)\n\n\nres <- tune_grid(\n    xgb_wf, \n    resamples = vb_folds, \n    grid = grid, \n    metrics = yardstick::metric_set(rmse, rsq, mae), \n    control = control_grid(save_pred = TRUE, allow_par = T))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold1: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold2: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold3: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold4: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold5: internal: A correlation computation is required, but `estimate` is const...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(res)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 11\n  min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean\n  <int>      <int>      <dbl>          <dbl>     <int> <chr>   <chr>      <dbl>\n1    12          6  0.0894           6.42e-2        17 rmse    standard    3.71\n2    23         11  0.00647          2.09e-3        10 rmse    standard    3.92\n3     8          1  0.000643         4.19e-9         9 rmse    standard   14.0 \n4    28          4  0.0000392        6.95e-6         8 rmse    standard   23.3 \n5    35         11  0.0000159        1.25e+1         4 rmse    standard   23.8 \n# ... with 3 more variables: n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_param <- select_best(res, 'rmse')\nfinal_model <- finalize_workflow(xgb_wf, best_param)\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n0 Recipe Steps\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 12\n  tree_depth = 6\n  learn_rate = 0.0894462983351361\n  loss_reduction = 0.0642365279282407\n  stop_iter = 17\n\nComputational engine: xgboost \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_fit <- fit(final_model, data = training(splits))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred_xgb <- \n    predict(xgb_fit, testing(splits)) %>% \n    mutate(modelo = \"xgb\")\n\npred_xgb %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  .pred modelo\n  <dbl> <chr> \n1  25.5 xgb   \n2  37.1 xgb   \n3  35.0 xgb   \n4  20.9 xgb   \n5  18.5 xgb   \n6  20.5 xgb   \n```\n:::\n\n```{.r .cell-code}\nresult2 <- data.frame(pred = pred_xgb$.pred, \n                     real = testing(splits)$medv)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::metrics(result2, pred, real)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       2.64 \n2 rsq     standard       0.915\n3 mae     standard       1.94 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}