---
title: "bonsai and treesnip"
description: |
  R lightgbm, xgboost, catboost 관련 패키지
author: "Don Don"
date: "2022-07-01"
categories: [R, ligthtgbm, xgboost, catboost]
image: "https://bonsai.tidymodels.org/logo.png"
---

## Library

tidymodels에서 xgboost의 경우 공식 지원한다. 다만 많이 사용되는 lightgbm, catboost의 경우 공식 지원하지 않는다. 따라서 CRAN에 등록되어있지 않은 treesnip 패키지를 설치해서 사용했어야 했다. 최근에 treesnip 패키지 개발을 멈추고 bonsai 패키지를 새롭게 만들어서 개발 중인 것 같다. bonsai 패키지 사용법에 대해 한번 정리를 해본다. bonsai 패키지의 경우 lightgbm은 지원하지만 catboost는 지원하지 않기 때문에, treesnip 패키지를 활용해서 catboost 모형을 적합하는 방법에 대해 정리해본다. bonsai, treesnip 패키지의 경우 tidymodels 워크플로우에 해당 모델이 추가된 형태이므로 tidymodels의 워크플로우를 그대로 따라간다.

```{r}
#install.packages("pak")
#install.packages("lightgbm")
#pak::pak("tidymodels/bonsai")
#remotes::install_github("curso-r/treesnip@catboost")
#remotes::install_github("curso-r/treesnip")


library(tidymodels)
library(rsample)
library(bonsai)
library(skimr)
library(treesnip)
library(catboost)
```

## Data description

```{r}
dat <- MASS::Boston
dat %>% glimpse()
```

```{r}
skimr::skim(dat)
```

## train/test split

```{r}
splits <- initial_split(dat, prop = 0.7)

```

## data preprocessing

```{r}
rec <- recipe(medv ~ ., training(splits))

```

## Catboost

```{r}
cat_mod <- boost_tree(
        trees = 1000,
        min_n = tune(),
        tree_depth = tune(),
        learn_rate = tune()) %>%
  set_engine(engine = "catboost" 
             ) %>%
  set_mode(mode = "regression")

```

```{r}
cat_wf <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(cat_mod)
```

```{r}
set.seed(1)
vb_folds <- vfold_cv(training(splits), v = 5)
```

```{r}
grid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(),  size = 10)

```

```{r}
res <- tune_grid(
    cat_wf, 
    resamples = vb_folds, 
    grid = grid, 
    metrics = yardstick::metric_set(rmse, rsq, mae), 
    control = control_grid(save_pred = TRUE))
```

```{r}
show_best(res)
```

```{r}
best_param <- select_best(res, 'rmse')
final_model <- finalize_workflow(cat_wf, best_param)
final_model

```

```{r}
cat_fit <- fit(final_model, data = training(splits))

```

```{r}
pred_cat <- 
    predict(cat_fit, testing(splits)) %>% 
    mutate(modelo = "cat")


result3 <- data.frame(pred = pred_cat$.pred, 
                     real = testing(splits)$medv)

```

```{r}
yardstick::metrics(result3, pred, real)
```

## lightgbm

tidymodels에서 lightgbm을 사용할 경우 `set_engine()`에 ligthtgbm을 명시해주면 된다. cuda gpu를 사용할 경우 `params=list(tree_method = 'gpu_hist')`를 명시해줘야 한다. 또한 CRAN에 등록되어있는 lightgbm 패키지가 아닌 gpu 옵션을 지원하는 lightgbm을 따로 설치해줘야한다(참고 : <https://cran.r-project.org/web/packages/lightgbm/readme/README.html#installing-a-gpu-enabled-build>). 이 경우 Docker file을 다운받아서 설치하는 것이 오류가 날 가능성이 적다.

```{r}
bt_mod <- 
  boost_tree(
        trees = 1000,
        min_n = tune(),
        tree_depth = tune(),
        learn_rate = tune(),
        loss_reduction = tune(),
        stop_iter = tune()) %>%
  set_engine(engine = "lightgbm" 
             #params=list(tree_method = 'gpu_hist')
             ) %>%
  set_mode(mode = "regression")
```

R lightgbm 공식 문서를 보면 추가적인 parameter가 많은데, 추가하고 싶은 parameter는 set_engine에 해당 parameter를 추가하면 된다.

```{r}
light_wf <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(bt_mod)
```

워크플로우에 전처리 recipe와 정의한 모델을 업데이트한다.

```{r}
vb_folds <- vfold_cv(training(splits), v = 5)
```

validation set을 지정한다. fold=5인 cross validation 방식으로 설정한다.

```{r}
grid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)

```

tuning 방식을 정의하는데 latine hypercube 방식을 이용했다. tuning 방식은 tidymodels 공식문서를 참고하면 다양한 방법을 제공한다.

```{r}
library(doFuture)
registerDoFuture()
cl <- parallel::makeCluster(6)
plan(cluster, workers = cl)
```

병렬 처리 옵션의 경우 doFuture 패키지를 이용했다. 해당 컴퓨터의 코어 수를 확인하고 적절하게 세팅해주면 된다.

```{r}
res <- tune_grid(
    light_wf, 
    resamples = vb_folds, 
    grid = grid, 
    metrics = yardstick::metric_set(rmse, rsq, mae), 
    control = control_grid(save_pred = TRUE, allow_par = T))

```

`tune_grid()`를 이용해서, workflow, validation set, metric, 예측값 저장 옵션 등을 설정하고, 파라미터 튜닝을 진행한다.

```{r}
show_best(res)
```

`show_best()`를 통해 tuning 후 최적의 파라미터를 확인할 수 있다.

```{r}
best_param <- select_best(res, 'rmse')
final_model <- finalize_workflow(light_wf, best_param)
final_model

```

최적의 파라미터를 선택한 후 `finalize_workflow()`를 선택된 최적의 파라미터를 workflow에 업데이트 시켜준다.

```{r}
light_fit <- fit(final_model, data = training(splits))

```

최적의 파라미터를 업데이트한 모델을 훈련 데이터를 이용해서 학습을 진행해준다.

```{r}
pred_light <- 
    predict(light_fit, testing(splits)) %>% 
    mutate(modelo = "lightgbm")

pred_light %>% head()

result1 <- data.frame(pred = pred_light$.pred, 
                     real = testing(splits)$medv)

```

마지막으로 test 데이터셋에 학습을 해서 예측값을 산출한다.

```{r}
yardstick::metrics(result1, pred, real)
```

yardstick 패키지의 `metrics()`의 경우 회귀, 분류 세팅에 맞춰서 적절한 평가지표를 보여준다. `metrics()`의 경우 예측값과 실제값이 포함된 데이터셋을 넣어줘야 작동한다.

## XGBOOST

xgboost의 경우 tidymodels에서 공식 지원하기 때문에, tidymodels 워크플로우에 맞춰서 그대로 사용하면 된다. 대신 gpu를 사용할 경우 lightgbm과 마찬가지로 gpu가 지원되는 xgboost 버전을 따로 설치해줘야 한다(참고 : <https://github.com/dmlc/xgboost/issues/6654#issuecomment-813828671>). lightgbm과 동일하게 gpu가 지원되는 xgboost를 설치했을 경우 `params=list(tree_method = 'gpu_hist')` 옵션을 넣어주면 된다.

```{r}
xgb_mod <- boost_tree(
        trees = 1000,
        min_n = tune(),
        tree_depth = tune(),
        learn_rate = tune(),
        loss_reduction = tune(),
        stop_iter = tune()) %>%
  set_engine(engine = "xgboost" 
             #params=list(tree_method = 'gpu_hist')
             ) %>%
  set_mode(mode = "regression")

```

```{r}
xgb_wf <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(xgb_mod)
```

```{r}
set.seed(1)
vb_folds <- vfold_cv(training(splits), v = 5)
```

```{r}
grid <- grid_latin_hypercube(min_n(), tree_depth(), learn_rate(), loss_reduction(), stop_iter(), size = 10)

```

```{r}
library(doFuture)
registerDoFuture()
cl <- parallel::makeCluster(6)
plan(cluster, workers = cl)


res <- tune_grid(
    xgb_wf, 
    resamples = vb_folds, 
    grid = grid, 
    metrics = yardstick::metric_set(rmse, rsq, mae), 
    control = control_grid(save_pred = TRUE, allow_par = T))
```

```{r}
show_best(res)
```

```{r}
best_param <- select_best(res, 'rmse')
final_model <- finalize_workflow(xgb_wf, best_param)
final_model

```

```{r}
xgb_fit <- fit(final_model, data = training(splits))

```

```{r}
pred_xgb <- 
    predict(xgb_fit, testing(splits)) %>% 
    mutate(modelo = "xgb")

pred_xgb %>% head()

result2 <- data.frame(pred = pred_xgb$.pred, 
                     real = testing(splits)$medv)

```

```{r}
yardstick::metrics(result2, pred, real)
```
