{
  "hash": "404b167add5681770368e5a4d2ac5184",
  "result": {
    "markdown": "---\ntitle: \"complete seperation in logistic regression\"\ndescription: |\n  glm.fit: fitted probabilities numerically 0 or 1 occurred\nauthor: \"Don Don\"\ndate: \"2022-06-22\"\ncategories: [R, glm, caret]\nimage: \"box_im.png\"\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(GGally)\nlibrary(caret)\nlibrary(recipes)\nlibrary(janitor)\nlibrary(ggmosaic)\nlibrary(pROC)\nlibrary(tidyverse)\nlibrary(tictoc)\n\ntheme_set(theme_bw())\n```\n:::\n\n\n\n# complete seperation \n\n-   설명변수를 이용해서 반응변수를 완벽하게 예측할 수 있는 경우를 의미함 \n\n-   설명변수를 이용해서 반응변수를 거의 완벽하게 예측할 수 있는 경우 quasi-complete seperation 이라고 함 \n\n## complete seperation 예제\n\n| Y   | X1  | X2  |\n|-----|-----|-----|\n| 0   | 1   | 3   |\n| 0   | 2   | 2   |\n| 0   | 3   | -1  |\n| 0   | 3   | -1  |\n| 1   | 5   | 2   |\n| 1   | 6   | 4   |\n| 1   | 10  | 1   |\n| 1   | 11  | 0   |\n\n-   $X_1\\le 3$일 경우 $Y=0$이고, $X_1>3$일 경우 $Y=1$로 완벽하게 분리됨\n\n\n\n## 왜 complete seperation이 문제인가? \n\n-   Logistic regression의 경우 MLE로 회귀계수를 추정할 때 수치적인 알고리즘을 이용하여 계산하는데 complete seperation일 경우 알고리즘이 수렴하지 않는 문제가 발생할 수 있음 \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- data.frame(x1 = c(1, 2, 3, 3, 5, 6, 10, 11), \n           x2 = c(3, 2, -1, -1, 2, 4, 1, 0), \n           y = c(0, 0, 0, 0, 1, 1, 1, 1))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- glm(y~., family = binomial, dat)\nfit$fitted.values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           1            2            3            4            5            6 \n2.220446e-16 9.861322e-11 3.179312e-12 3.179312e-12 1.000000e+00 1.000000e+00 \n           7            8 \n1.000000e+00 1.000000e+00 \n```\n:::\n:::\n\n\n-   `glm.fit: fitted probabilities numerically 0 or 1 occurred` warning message 출력 \n\n    -   예측 확률이 0, 1에 거의 근접한 값이 나왔기 때문에 발생함 \n    \n    -   즉, 너무 완벽하게 예측했을 경우 발생함 \n\n-   `glm.fit: algorithm did not converge` warning message 출력 \n\n    -   MLE를 계산할 때, 알고리즘이 수렴하지 않을 경우 발생함  \n\n    \n\n\n\n# Complete seperation이 발생할 경우 해결책 \n\n**1. Complete seperation을 발생하게 하는 설명변수 제거**\n\ncomplete seperation이 일어나는 경우는 $Y$와 너무 밀접하게 연관된 $X$ 변수를 수집했을 때 발생합니다. 혹은, 데이터가 매우 작은 경우 데이터 수집과정에서 우연히 발생할 수도 있을 것입니다. 먼저, complete seperation이 일어나게 하는 설명변수 $X$가 타당한 설명변수인지 생각해봐야 합니다. 아래 예제 데이터를 보겠습니다. \n\n**Data description**\n\n-   temperature : 온도(섭씨)\n\n-   Humidity : 습도(%)\n\n-   Light : 밝기(lux)\n\n-   Co2 : 이산화탄소 농도(ppm)\n\n-   Occupancy : 방 이용 유무(1 : 이용 x, 0 : 이용 o)\n\n<https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+>\n\n온도, 밝기, 습도, 이산화탄소 농도 등의 변수를 활용해서 방 이용 유무를 예측하는 이진 분류 문제입니다. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- read.csv('/Users/sangdon/myblog/posts/com_sep/dat9.csv')\n```\n:::\n\n\n먼저, train/test를 나누고 EDA를 진행해보겠습니다. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  temperature       humidity         light             co2        \n Min.   :19.20   Min.   :17.60   Min.   :   0.0   Min.   : 418.0  \n 1st Qu.:21.29   1st Qu.:25.21   1st Qu.: 428.2   1st Qu.: 697.0  \n Median :21.79   Median :27.94   Median : 444.0   Median : 891.2  \n Mean   :21.78   Mean   :28.18   Mean   : 424.5   Mean   : 928.0  \n 3rd Qu.:22.20   3rd Qu.:31.26   3rd Qu.: 472.3   3rd Qu.:1069.4  \n Max.   :24.39   Max.   :39.07   Max.   :1380.0   Max.   :2022.5  \n                                                  NA's   :5       \n   occupancy   \n Min.   :0.00  \n 1st Qu.:1.00  \n Median :1.00  \n Mean   :0.88  \n 3rd Qu.:1.00  \n Max.   :1.00  \n               \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- dat %>% \n    mutate(occupancy = factor(occupancy, labels = c(\"No\", \"Yes\")))\n\nlibrary(rsample)\n\nset.seed(1231)\nsplits <- initial_split(dat, prop =  0.7, strata=occupancy)\n\ntrain<-training(splits)\ntest<-testing(splits)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrec <- train %>%\n  recipe(occupancy~.) %>%\n  step_bagimpute(co2, impute_with=imp_vars(all_predictors()))\n\ntrain<- rec %>% prep() %>% juice()\ntest <- rec %>% prep() %>% bake(new_data=test)\ntrain %>% is.na() %>% colSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntemperature    humidity       light         co2   occupancy \n          0           0           0           0           0 \n```\n:::\n:::\n\n\n다른 전처리는 하지 않고, 결측치 대치만 진행한 후, logistic regression 모형을 적합해보겠습니다. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\ncontrol <- trainControl(method='cv', \n                        number=3, \n                        classProbs = T,\n                        summaryFunction = twoClassSummary, \n                        savePredictions = \"all\"\n                        )\n\n\ntic()\nglm_gridsearch <- train(occupancy ~ .,             \n                       data = train,               \n                       method = 'glm',\n                       family = \"binomial\",\n                       trControl = control,\n                       metric = \"ROC\") \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.187 sec elapsed\n```\n:::\n:::\n\n\n\n다음과 같은 warning message가 출력됩니다. \n\n-   glm.fit: algorithm did not converge \n\n-   glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n위에서 설명했던 것처럼 complete seperation이 있을 때에 해당합니다. 따라서 데이터를 보고 어떤 설명변수 $X$가 이러한 문제를 발생시켰는지 확인이 필요합니다. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain %>% \n    ggplot(aes(x = occupancy, y = light, fill = occupancy)) + \n    geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](com_sep_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain %>% \n    group_by(occupancy) %>% \n    summarise(min = min(light), \n              mean = mean(light), \n              max = max(light))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  occupancy   min   mean   max\n  <fct>     <dbl>  <dbl> <dbl>\n1 No            0   6.68   153\n2 Yes         393 481.    1380\n```\n:::\n:::\n\n\noccupancy ~ light의 boxplot과 통계량을 보면 occupancy가 $No$일 때, light는 대부분 $0$의 값을 갖고, 최대값은 $153$입니다. 반면, occupancy가 $Yes$일 때, light는 $393 \\sim 1380$까지의 값을 갖는 것을 알 수 있습니다. \n\n변수의 의미를 생각해보면 방을 이용하지 않을 경우 불은 꺼져있을 것이고, 방을 이용할 경우 불이 켜져있는 것은 당연하다고 볼 수 있습니다. 따라서 이상치가 없다는 가정 하에 light 변수가 설명변수에 있을 경우 occupancy를 정확히 예측할 수 있습니다. \n\n이러한 예제의 경우 light 변수를 제거하지 않고, 모형에 포함시키는 것이 합리적입니다. 다만, 데이터가 작을 경우, 연관성이 없는 변수에 대해 이러한 현상이 발생할 수 있습니다. 혹은 반응변수의 의미와 완벽하게 중복되는 설명변수가 포함되는 경우도 있을 수 있습니다. 이러한 경우 변수를 제거하는 것이 합리적일 것입니다. \n\n이렇듯, 분석가의 판단 하에 변수를 제거할지 여부를 판단하는 것이 필요합니다. \n\n\n**2. Logistic regression 모형 말고 다른 모형 적합** \n\nLogistic regression 모형이 아닌 다른 ML 모형의 경우 (ex. tree based model) complete seperation 문제 때문에 계산이 안되는 문제는 없기 때문에 다른 모형을 적합하면 됩니다. \n\n\n**3. Penalized logistic regression** \n\nLogistic regression 모형에 penalty term을 추가할 경우 logistic regression의 알고리즘이 수렴하지 문제를 해결할 수 있습니다. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#devtools::install_version(\"LiblineaR\", version = \"2.10-8\")\nset.seed(1231)\ncontrol<-trainControl(method=\"cv\",\n                      number=5,\n                      classProbs=T,\n                      summaryFunction=twoClassSummary,\n                      savePredictions = T)\n\nlogistic_gridsearch<-train(occupancy~.,\n                           data=train,\n                           method=\"regLogistic\",\n                           trControl=control,\n                           tuneLength=3,\n                           metric=\"ROC\")\n\nlogistic_gridsearch\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegularized Logistic Regression \n\n209 samples\n  4 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 168, 167, 167, 167, 167 \nResampling results across tuning parameters:\n\n  cost  loss       epsilon  ROC  Sens  Spec\n  0.5   L1         0.001    1    0.96  1   \n  0.5   L1         0.010    1    1.00  1   \n  0.5   L1         0.100    1    1.00  1   \n  0.5   L2_dual    0.001    1    0.96  1   \n  0.5   L2_dual    0.010    1    0.96  1   \n  0.5   L2_dual    0.100    1    0.96  1   \n  0.5   L2_primal  0.001    1    0.96  1   \n  0.5   L2_primal  0.010    1    0.96  1   \n  0.5   L2_primal  0.100    1    0.96  1   \n  1.0   L1         0.001    1    1.00  1   \n  1.0   L1         0.010    1    1.00  1   \n  1.0   L1         0.100    1    0.96  1   \n  1.0   L2_dual    0.001    1    0.96  1   \n  1.0   L2_dual    0.010    1    0.96  1   \n  1.0   L2_dual    0.100    1    0.96  1   \n  1.0   L2_primal  0.001    1    0.96  1   \n  1.0   L2_primal  0.010    1    0.96  1   \n  1.0   L2_primal  0.100    1    0.96  1   \n  2.0   L1         0.001    1    0.96  1   \n  2.0   L1         0.010    1    0.96  1   \n  2.0   L1         0.100    1    1.00  1   \n  2.0   L2_dual    0.001    1    0.96  1   \n  2.0   L2_dual    0.010    1    0.96  1   \n  2.0   L2_dual    0.100    1    0.96  1   \n  2.0   L2_primal  0.001    1    0.96  1   \n  2.0   L2_primal  0.010    1    0.96  1   \n  2.0   L2_primal  0.100    1    0.96  1   \n\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were cost = 0.5, loss = L1 and epsilon\n = 0.001.\n```\n:::\n:::\n\n\n해당 데이터의 경우 ADP 23회 기출문제입니다. 시험 답안을 작성하실 때, 어떻게 작성하면 좋을지 한번 생각해보세요! \n\n\n\n",
    "supporting": [
      "com_sep_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}