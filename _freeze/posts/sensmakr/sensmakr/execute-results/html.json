{
  "hash": "354ed3da3e1811714e214aca14fd3637",
  "result": {
    "markdown": "---\ntitle: \"sensmakr\"\ndescription: |\n  sensmakr 패키지 소개 및 간단한 논문 리뷰\nauthor: \"Don Don\"\ndate: \"2022-06-24\"\ncategories: [causal inference]\n---\n\n\n# Making Sense of Sensitivity: Extending Omitted Variable Bias\n\n# Frisch-waugh-Lovell theorem\n\n설명 추가\n\n$X_1, X_2$는 독립변수의 집합을 의미함\n\n\n$$\n\\begin{align*}\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon\n\\end{align*}\n$$ 추정하고자 하는 $\\hat{\\beta}$는 다음과 같음\n\n\n1.  $y ~ \\sim X_1$의 잔차 $\\longrightarrow$ $y^*$\n\n    -   $y ~ \\sim X_1$로 회귀모형을 fitting해서 잔차 $y^*$을 구한다.\n\n2.  $X_2 ~ \\sim X_1$의 잔차 $\\longrightarrow$ $X_2^*$\n\n    -   $X_2 ~ \\sim X_1$로 회귀모형을 fitting해서 잔차 $X_2^*$를 구한다.\n\n3.  $y^* \\sim X_2^*$ $\\longrightarrow$ $\\hat{\\beta}_2$\n\n    -   잔차 $y^*$와 잔차 $X_2^*$를 회귀모형으로 fitting해서 얻은 coefficient는 $\\hat{\\beta}_2$와 같다.\n\n\n$$\n\\begin{align*}\ny = X_1 \\hat{\\beta_1} + X_2 \\hat{\\beta_2} + e\n\\end{align*}\n$$ normal equation으로 표현하면 다음과 같음\n\n$$\n\\begin{align*}\n\\begin{bmatrix}\nX_1^t X_1 & X_1^t X_2 \\\\\nX_2^t X_1 & X_1^2 X_2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\beta_1}  \\\\\n\\hat{\\beta_2} \n\\end{bmatrix} = \n\\begin{bmatrix}\nX_1^t y  \\\\\nX_2^t y \n\\end{bmatrix} \n\\end{align*}\n$$\n\n\n먼저 $\\hat{\\beta_1}$에 대해 풀면 다음과 같다.\n\n\n$$\n\\begin{align*}\n(X_1^tX_1)\\hat{\\beta_1} + (X_1^tX_2)\\hat{\\beta_2} &= X_1^ty \\\\\n(X_1^tX_1)\\hat{\\beta_1} &= X_1^ty - (X_1^tX_2)\\hat{\\beta_2} \\\\ \n\\hat{\\beta_1} &= (X_1^tX_1)^{-1}X_1^ty - (X_1^tX_1)^{-1}X_1^tX_2\\hat{\\beta_2} \\\\\n\\hat{\\beta_1} &= (X_1^tX_1)^{-1}X_1^t(y - X_2\\hat{\\beta_2}) \n\\end{align*}\n$$\n\n\n다음으로 $\\hat{\\beta_2}$에 대해 정리하면 다음과 같다.\n\n\n$$\n\\begin{align*}\n(X_2^tX_1)\\hat{\\beta_1} + (X_2^tX_2)\\hat{\\beta_2} &= X_2^ty \\\\\n(X_2^tX_1) (X_1^tX_1)^{-1}X_1^t(y - X_2\\hat{\\beta_2}) +(X_2^tX_2)\\hat{\\beta_2} &= X_2^ty \\\\ \nX_2^tX_1(X_1^tX_1)^{-1}X_1^ty - X_2^tX_1(X_1^tX_1)^{-1}X_2^t\\hat{\\beta}_2 + X_2^tX_2\\hat{\\beta}_2 &=X_2^ty \\\\\nX_2^ty - X_2^tX_1(X_1^tX_1)^{-1}X_1^ty &= X_2^tX_2\\hat{\\beta}_2 - X_2^tX_1(X_1^tX_1)^{-1}X_2^t\\hat{\\beta}_2 \\\\ \nX_2^t(I - X_1(X_1^tX_1)^{-1}X_1^t)y &= [X_2^t(I - X_1(X_1^tX_1)^{-1}X_1^t)X_2]\\hat{\\beta}_2 \\\\\n\\hat{\\beta}_2 &= [X_2^t(I - X_1(X_1^tX_1)^{-1}X_1^t)X_2]^{-1}X_2^t(I - X_1(X_1^tX_1)^{-1}X_1^t)y \\\\\n&=[X_2^tM_1X_2]^{-1}X_2^tM_1y, \\quad M_1 = I - X_1(X_1^tX_1)^{-1}X_1^t\n\\end{align*}\n$$\n\n\n회귀분석에서 $M$은 residual maker로 $My$는 $y \\sim X$의 잔차를 의미한다.\n\n\n$$\n\\begin{align*}\ne &= y - X\\hat{\\beta} \\\\ \n&=y - X(X^tX)^{-1}X^ty \\\\ \n&=(I - X(X^tX)^{-1}X^t)y \\\\ \n&=My\n\\end{align*}\n$$\n\n\n따라서 위의 식 $\\hat{\\beta}_2 = [X_2^tM_1X_2]^{-1}X_2^tM_1y$에서\n\n$M_1y$는 $y \\sim X_1$의 잔차, $M_1X_2$는 $X_2 \\sim X_1$를 의미한다.\n\n\n$$\n\\begin{align*}\n\\hat{\\beta}_2 &= [X_2^tM_1X_2]^{-1}X_2^tM_1y \\\\ \n&=[X_2^tM_1^tM_1X_2]^{-1}X_2^tM_1^tM_1y, \\quad M_1^tM1 = M_1 : \\text{idempotent matrix} \\\\ \n&= [X_2^{*t}X_2^*]^{-1}X_2^{*t}y^*, \\quad X_2^* = M_1X_2, \\quad y^* = M_1y\n\\end{align*}\n$$\n\n\n즉, 정리하면 다음과 같다.\n\n\n$$\n\\begin{align*}\ny = X_1\\beta_1 + X_2\\beta_2 + \\epsilon \\longrightarrow y &= X_1\\hat{\\beta}_1 + X_2\\hat{\\beta}_2 + e \\\\ \n\\therefore \\hat{\\beta}_2 &= [X_2^{*t}X_2^*]^{-1}X_2^{*t}y^*, \\quad X_2^* = M_1X_2, \\quad y^* = M_1y\n\\end{align*}\n$$\n\n\n## Example\n\n-   $D$ : treatment variable\n\n-   $X$ : observed covariates\n\n-   $Z$ : unobserved covariates\n\n\n$$\n\\begin{align*}\nY = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}\n\\end{align*}\n$$\n\n\n**Frisch-waugh-Lovell theorem**을 적용하면 $Y = \\hat{\\tau}D + X\\hat{\\beta} + \\hat{\\gamma}Z + \\hat{\\epsilon}_{null}$에 **FWL**을 적용하면\n\n1.  $Y ~ \\sim X$의 잔차 $\\longrightarrow$ $Y^{\\perp X}$\n\n2.  $D ~ \\sim X$의 잔차 $\\longrightarrow$ $D^{\\perp X}$\n\n3.  $Z ~ \\sim X$의 잔차 $\\longrightarrow$ $Z^{\\perp X}$\n\n4.  $Y^{\\perp X} \\sim D^{\\perp X} + Z^{\\perp X}$ $\\longrightarrow$ $\\hat{\\tau}, \\, \\hat{\\gamma}$\n\n    -   $Y^{\\perp X} = \\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    Z = rnorm(N, 1),\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\nZperpX <- lm(Z ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX, ZperpX)\n\nprint(coef(lm(YperpX ~ DperpX+ZperpX, resid_df))[c(2, 3)], digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  DperpX   ZperpX \n-0.00072  0.01351 \n```\n:::\n\n```{.r .cell-code}\nprint(coef(lm(Y~D+X+Z, df))[c(2, 4)], digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       D        Z \n-0.00072  0.01351 \n```\n:::\n:::\n\n$$\n\\begin{align*}\nY = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}\n\\end{align*}\n$$\n\n\n$Y = \\hat{\\tau}_{res}D + X\\hat{\\beta}_{res} + \\hat{\\epsilon}_{res}$에 **FWL**을 적용하면\n\n1.  $y ~ \\sim X$의 잔차 $\\longrightarrow$ $Y^{\\perp X}$\n\n2.  $D ~ \\sim X$의 잔차 $\\longrightarrow$ $D^{\\perp X}$\n\n3.  $Y^{\\perp X} \\sim D^{\\perp X}$ $\\longrightarrow$ $\\hat{\\tau}_{res}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(13)\nN = 10000\n\ndf <- data.frame(\n    X = rnorm(N, 1.5),\n    D = rnorm(N, 2.5), \n    Y = rnorm(N, 3))\n\nYperpX <- lm(Y ~ X, df)$residuals\nDperpX <- lm(D ~ X, df)$residuals\n\nresid_df <- data.frame(YperpX, DperpX)\n\nprint(coef(lm(YperpX ~ DperpX, resid_df))[2], digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDperpX \n0.0078 \n```\n:::\n\n```{.r .cell-code}\nprint(coef(lm(Y~D+X, df))[2], digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     D \n0.0078 \n```\n:::\n:::\n\n\n## 3.1 The traditional omitted variable bias\n\n\n$$\n\\begin{align*}\n\\hat{\\tau}_{res} &= \\frac{cov(D^{\\perp X}, Y^{\\perp X})}{var(D^{\\perp X})} \\\\ \n&= \\frac{cov(D^{\\perp X},\\hat{\\tau}D^{\\perp X} + \\hat{\\gamma}Z^{\\perp X})}{var(D^{\\perp X})} \\\\ \n&= \\frac{\\hat{\\tau}cov(D^{\\perp X},D^{\\perp X}) + \\hat{\\gamma}cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})} \\\\\n&=\\hat{\\tau} + \\hat{\\gamma} \\cdot \\hat{\\delta}, \\quad \\hat{\\delta} = \\frac{cov(D^{\\perp X}, Z^{\\perp X})}{var(D^{\\perp X})}, \\quad \\hat{\\gamma} = \\frac{cov(Y^{\\perp X, D}, Z^{\\perp X, D})}{var(Z^{\\perp X, D})}\n\\end{align*}\n$$\n\n\n따라서 unobserved confounder에 의한 추정량의 bias는 다음과 같다.\n\n\n$$\n\\begin{align*}\n\\hat{bias} = \\hat{\\tau}_{res} - \\hat{\\tau} =  \\hat{\\gamma} \\cdot \\hat{\\delta}\n\\end{align*}\n$$\n\n\n$Z$는 unobserved confounder이므로 $\\hat{\\gamma}, \\, \\hat{\\delta}$의 부호를 알 수 없다. 따라서 unobserved confounder의 추정량에 영향을 미치는 크기를 고려해야 한다. 즉, 연구의 주요 결론에 영향을 줄 정도로 추정량을 변경하려면 unobserved confounder $Z$의 효과는 어느 정도 크기여야 하는가?\n\n이는 sensitivity analysis를 통해 파악할 수 있다.\n\n## contour plot\n\n## OVB with the partial $R^2$ parameterization\n\n\n$$\n\\begin{align*}\n\\hat{bias} &= \\hat{\\gamma} \\cdot \\hat{\\delta} \\\\ \n&=\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}} \\cdot \\frac{sd(Y^{\\perp X,D})}{sd(D^{\\perp X})} \\\\\n&=\\hat{se}(\\hat{\\tau}_{res})\\cdot\\sqrt{\\frac{R^2_{Y \\sim Z|D, X} \\cdot R^2_{D \\sim Z|X}}{1-R^2_{D \\sim Z|X}}}(df)\n\\end{align*}\n$$\n\n\n## Robustness value\n\n\n$$\n\\begin{align*}\nRV_q = \\frac{1}{2}(\\sqrt{f^4_q + 4f^2_q - f^2_q})\n\\end{align*}\n$$\n\n\n# Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\ndf <- read.csv(\"ex_data.csv\")[, -1]\n\ndf %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age gender education wage\n1  62   male         6 3800\n2  44   male         8 4500\n3  63   male         8 4700\n4  33   male         7 3500\n5  57 female         6 4000\n6  59   male         9 3900\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$gender <- as.factor(df$gender)\n\nfit <- lm(wage ~ ., df)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = wage ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-958.27 -194.90   -1.32  256.00  967.54 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2657.89     445.00   5.973 3.18e-07 ***\nage            12.31       6.11   2.015   0.0498 *  \ngendermale    335.11     132.69   2.526   0.0151 *  \neducation      95.94      38.75   2.476   0.0170 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 455.1 on 46 degrees of freedom\nMultiple R-squared:  0.2549,\tAdjusted R-squared:  0.2063 \nF-statistic: 5.246 on 3 and 46 DF,  p-value: 0.003379\n```\n:::\n:::\n\n\n-   education의 coef는 양수임 \n\n-   unobserved confounder(ability)가 있으므로, OVB가 존재함 \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sensemakr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSee details in:\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nCarlos Cinelli and Chad Hazlett (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. Journal of the Royal Statistical Society, Series B (Statistical Methodology).\n```\n:::\n\n```{.r .cell-code}\nsens <- sensemakr(model = fit, treatment = \"education\")\nsens\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wage ~ age + gender + education\n\nNull hypothesis: q = 1 and reduce = TRUE \n\nUnadjusted Estimates of ' education ':\n  Coef. estimate: 95.94369 \n  Standard Error: 38.75212 \n  t-value: 2.47583 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.11759 \n  Robustness Value, q = 1 : 0.30444 \n  Robustness Value, q = 1 alpha = 0.05 : 0.06273 \n\nFor more information, check summary.\n```\n:::\n:::\n\n\n-   교육이 임금에 미치는 영향의 부호가 바뀌려면, 교육과 임금의 residual variance 중 어느 정도를 ability로 설명해야 하는지? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wage ~ age + gender + education\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'education': \n  Coef. estimate: 95.9437 \n  Standard Error: 38.7521 \n  t-value (H0:tau = 0): 2.4758 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.1176 \n  Robustness Value, q = 1: 0.3044 \n  Robustness Value, q = 1, alpha = 0.05: 0.0627 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 11.76% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 30.44% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 30.44% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 6.27% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 6.27% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sens, xlab = \"Partial R^2 of ability with education\", \n     ylab = \"Partial $R^2$ of ability with wage\")\n```\n\n::: {.cell-output-display}\n![](sensmakr_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n-   unadjusted는 unobserved confounder인 ability가 wage, education에 미치는 효과가 없을 때를 의미함 \n\n    -   위에서 lm 결과와 동일함 \n\n-   우측 상단으로 갈수록 ability의 power는 증가하고, 추정된 회귀계수 값은 감소함 \n\n-   빨간 점선일 때, 추정된 회귀계수 값은 0이 됨 \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsens2 <- sensemakr(model = fit, treatment = \"education\", \n                  benchmark_covariates = \"age\", \n                  kd = c(0.5, 1, 2), \n                  ky = c(0.5, 1, 2))\n\nplot(sens2)\n```\n\n::: {.cell-output-display}\n![](sensmakr_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sens, type = \"extreme\")\n```\n\n::: {.cell-output-display}\n![](sensmakr_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n# 참고자료\n\n<https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html>\n\n<https://arelbundock.com/posts/robustness_values/>\n\n<https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_2_Beck_UCSD.pdf>\n",
    "supporting": [
      "sensmakr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}