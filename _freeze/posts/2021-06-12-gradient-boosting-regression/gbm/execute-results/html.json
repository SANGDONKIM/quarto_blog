{
  "hash": "3d0c579799bd322d9fdf9c43f6d940f8",
  "result": {
    "markdown": "---\ntitle: \"gradient boosting machine tutorial\"\ndescription: |\n  gradient boosting machine 소개\nauthor: \"Don Don\"\ndate: \"2021-01-06\"\ncategories: [R, gbm]\nimage: \"https://bradleyboehmke.github.io/HOML/images/boosted-trees-process.png\"\n---\n\n::: {.cell}\n\n:::\n\n\n# Gradient Boosting Regression Tree\n\n## Pseudo code\n\nInput : Data $\\{x_i, y_i\\}^n_{i=1}$, and a differentiable Loss function $L(y_i, F(x))$\\\nStep 1 : Initialized model with a constant value: $F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)$\\\nStep 2 : for $m = 1$ to M:\\\n(A) Compute $r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}$ for $i = 1,...n$\\\n(B) Fit a regression tree to the $r_{im}$ values and create terminal regions $R_{im}$, for $j = 1,...,J_m$\\\n(C) For $j = 1,...,j_m$ compute $r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)$\\\n(D) Update $F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})$\\\nStep 3 : Output $F_M(x)$\n\n## Details\n\n1.  Input : Data $\\{x_i, y_i\\}^n_{i=1}$, and a differentiable Loss function $L(y_i, F(x))$\n\n미분 가능한 loss function으로 GBM에서는 L2 norm을 선택한다. 이 때 $\\frac{1}{2}$는 계산상의 편의를 위해서 scaling constant이다.\\\nLoss function : $L(y_i, F(x)) = \\frac{1}{2} \\sum_{i=1}^n (y_i -F(x))^2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss <- function(y, yhat){\n                return(mean(1/2*(y-yhat)^2))\n}\n```\n:::\n\n\n2.  Step 1 : Initialized model with a constant value: $F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)$\\\n\n\n$$\n\\begin{align}\nF_0(x) &= \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma) \\newline\n&={d \\over d\\gamma} \\frac{1}{2} \\sum_{i=1}^n (y_i -\\gamma^2)\\newline\n&= -\\sum_{i=1}^n(y_i - \\gamma) \\newline\n&= 0 \\newline\n&\\Leftrightarrow \\hat{\\gamma} = \\bar{y}\n\\end{align} \n$$\n\n\n초기값은 y의 평균으로 계산한다.\n\n3.  \n\n    (A) Compute $r_{im} = -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}$ for $i = 1,...n$\n\n    $$\n    \\begin{align}\n    r_{im} &= -[{\\partial L(y_i, F(x_i)) \\over\\partial F_(x_i)}]_{F(x) = F_{m-1}(x)}, \\quad i = 1,...n, \\quad m = \\#tree \\newline\n    &\\Leftrightarrow r_{im} = y_i - F_{m-1}(x_i)\n    \\end{align}\n    $$\n\n    $r_{im}$은 negative gradient or pseudo residual이라고 한다. GBM은 residual을 기반으로 regression tree를 생성하는데 이 때 이용되는 residual 값이 $r_{im}$으로 계산된 pseudo residual이다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnegative_residual <- function(y, yhat) {\n                return(y - yhat)\n}\n```\n:::\n\n\n4.  \n\n    (B) Fit a regression tree to the $r_{im}$ values and create terminal regions $R_{im}$, for $j = 1,...,J_m$\n\n    tree의 깊이는 보통 8\\~32 정도로 구성된다. full tree가 아닌 weak learner or weak tree를 만들기 때문에 tree의 terminal regions $R_{im}$ 에는 여러 개의 값이 존재할 수 있다.\n\n5.  \n\n    (C) For $j = 1,...,j_m$ compute $r_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)$\n\n    이 때 tree의 terminal regions $R_{im}$ 에 존재하는 여러 개의 값은 $r_{jm}$ : terminal region의 평균으로 계산된다.\n\n\n$$\n\\begin{align}\nr_{jm} &= \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma) \\newline\n       &=\\arg\\min_{\\gamma} \\sum_{x_i \\in R_{ij}}\\frac{1}{2}(y_i - (F_{m-1}(x_i)+\\gamma)) \\newline \n       &\\Leftrightarrow -\\sum_{x_i \\in R_{ij}}(y_i - F_{m-1}(x_i)-\\gamma)) = 0 \\newline\n       &\\Leftrightarrow \\hat{\\gamma} := terminal\\; region의\\;평균 \n\\end{align}\n$$\n\n\n6.  \n\n    (D) Update $F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})$\n\n완성된 tree는 $\\sum_{j=1}^{J_m} \\gamma_m I(x \\in R_{jm})$ 로 표현되며 learning rate $\\nu$를 이용해서 예측값에 대한 개별 tree의 영향력을 조절한다. $\\nu$가 작으면 개별 tree의 영향력이 줄어들고, 계산량이 많아지지만 accuracy는 향상된다. $\\nu$가 크면 개별 tree의 영향력이 커지고, 계산량이 상대적으로 적으며, accuracy가 상대적으로 줄어든다.\n\n### Full code\n\n구글 서치 중에 OLS 기반으로 gradient boosting 수행하는 코드를 발견했는데 gradient boosting 알고리즘을 이해하는데 많은 도움이 되었다(실제 패키지에서는 regression tree 기반으로 계산되기 때문에 theta 값은 계산되지 않는다)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad_boost <- function(formula, data, nu = 0.01, stop, \n                       grad.fun, loss.fun, yhat.init = 0) {\n  \n  data <- as.data.frame(data)\n  formula <- terms.formula(formula)\n  X <- model.matrix(formula, data)\n  \n  y <- data[, as.character(formula)[2]] # as.character(formula)[2] : formula y~.에서 y에 해당하는 명칭\n\n  fit <- yhat.init\n  \n  u <- grad.fun(y = y, yhat = fit) # pseudo residual 계산 \n  \n  theta <- rep(0, ncol(X))\n  \n  loss <- c()\n \n  for (i in 1:stop) {\n    \n    # Design matrix를 이용한 regression, OLS 기반, Tree X \n    base_prod <- lm.fit(x = X, y = u) \n    theta_i <- coef(base_prod)\n    \n    # theta 값 업데이트 \n    theta <- theta + nu*as.vector(theta_i)\n    \n    # yhat 값 업데이트\n    fit <- fit + nu * fitted(base_prod)\n    \n    # pseudo residual 계산\n    u <- grad.fun(y = y, yhat = fit)\n    \n    # loss 값 업데이트 \n    loss <- append(loss, loss.fun(y = y, yhat = fit))\n  }  \n  names(theta) <- colnames(X)\n  return(list(theta = theta, u = u, fit = fit, loss = loss, \n              formula = formula, data = data))\n}\n\nfit <- grad_boost(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, data = iris, stop = 1000, grad.fun = negative_residual, loss.fun = loss)\nfit$theta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      (Intercept)       Sepal.Width      Petal.Length       Petal.Width \n        2.1711726         0.4958675         0.8292081        -0.3151416 \nSpeciesversicolor  Speciesvirginica \n       -0.7235307        -1.0234536 \n```\n:::\n:::\n\n\n### Using GBM package\n\n패키지는 Tree 기반으로 계산되므로 고정된 coefficient 결과를 산출하지 않는다.\\\n대신에 feature importance 값으로 변수별 상대적인 영향력을 볼 수 있다.\\\n\n-   n.trees : tree의 갯수(the number of gradient boosting iteration), pseudo code에서 $M$에 해당\n\n-   interaction.depth : tree당 최대 노드의 개수, 보통 8\\~32\n\n-   shringkage : learning rate($\\nu$)\n\n-   n.minobsinnode : terminal nodes의 최소 관찰값의 수\n\n-   bag.fraction (Subsampling fraction) : training set을 나눌 비율. 기본적으로 stochastic gradient boosting 전략 채택. default : 0.5\n\n-   train.fraction : 첫 train.fraction \\* nrows(data) 관찰값은 gbm fitting에 사용되고 나머지는 loss function에서의 out-of-sample 추정량을 계산하는데 사용됨. default = 1\n\n-   cv.folds : cross validation fold의 개수\n\n-   verbose : 모델 진행 상황을 모니터링할건지 유무\n\n-   distribution : 분류 문제일 경우 - bernoulli, multinomial, regression 문제일 경우 - gaussian or tdist\n\n**보통 bag.fraction, train.fraction은 따로 지정하지 않음.**\n\n#### R code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\nfit_pack <- gbm(formula = Sepal.Length~Sepal.Width + Petal.Length + Petal.Width + Species, \n                data = iris, \n                verbose = T, \n                shrinkage = 0.01, \n                distribution = 'gaussian')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.6726             nan     0.0100    0.0076\n     2        0.6650             nan     0.0100    0.0081\n     3        0.6573             nan     0.0100    0.0080\n     4        0.6493             nan     0.0100    0.0075\n     5        0.6414             nan     0.0100    0.0073\n     6        0.6326             nan     0.0100    0.0068\n     7        0.6248             nan     0.0100    0.0072\n     8        0.6171             nan     0.0100    0.0067\n     9        0.6102             nan     0.0100    0.0068\n    10        0.6028             nan     0.0100    0.0070\n    20        0.5403             nan     0.0100    0.0058\n    40        0.4424             nan     0.0100    0.0039\n    60        0.3720             nan     0.0100    0.0030\n    80        0.3182             nan     0.0100    0.0016\n   100        0.2757             nan     0.0100    0.0014\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_pack)\n```\n\n::: {.cell-output-display}\n![](gbm_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      var rel.inf\nPetal.Length Petal.Length 96.0517\nPetal.Width   Petal.Width  3.9483\nSepal.Width   Sepal.Width  0.0000\nSpecies           Species  0.0000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npretty.gbm.tree(fit_pack, i = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n0        1  4.5500000000        1         2           3       35.36091     75\n1       -1 -0.0054100775       -1        -1          -1        0.00000     43\n2       -1  0.0084729167       -1        -1          -1        0.00000     32\n3       -1  0.0005133333       -1        -1          -1        0.00000     75\n     Prediction\n0  0.0005133333\n1 -0.0054100775\n2  0.0084729167\n3  0.0005133333\n```\n:::\n:::\n\n\npretty.gbm.tree()를 이용하면 개별 tree를 적합할 때 진행상황을 모니터링할 수 있다. 여기서의 predict 값은 개별 tree에 대한 값이므로 pseudo residual 값에 해당한다(참고 3).\n\n### 참고 자료 \n\n참고 1 : <https://www.youtube.com/watch?v=2xudPOBz-vs&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6&index=2>\n\n참고 2 : [https://medium.com/\\@statworx_blog/coding-gradient-boosted-machines-in-100-lines-of-code-d06b1d7bc084](https://medium.com/@statworx_blog/coding-gradient-boosted-machines-in-100-lines-of-code-d06b1d7bc084){.uri}\n\n참고 3 : <https://stats.stackexchange.com/questions/237582/interpretation-of-gbm-single-tree-prediction-in-pretty-gbm-tree>\n\n\n",
    "supporting": [
      "gbm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}